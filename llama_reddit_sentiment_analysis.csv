Title,Text,Sentiment,Score,Upvotes,Comments,URL
"Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present ‚ÄúBoth Sides‚Äù","Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present ‚ÄúBoth Sides‚Äù",POSITIVE,0.9938325881958008,2326,434,https://www.404media.co/facebook-pushes-its-llama-4-ai-model-to-the-right-wants-to-present-both-sides/
"Winamp, your parents' favorite MP3 software, is back | It's not clear how many llamas have been affected.","Winamp, your parents' favorite MP3 software, is back | It's not clear how many llamas have been affected.",NEGATIVE,0.9995829463005066,2833,756,https://www.engadget.com/winamp-mp3-player-software-returns-191027227.html
Lawsuit says Mark Zuckerberg approved Meta's use of pirated materials to train Llama AI,Lawsuit says Mark Zuckerberg approved Meta's use of pirated materials to train Llama AI,NEGATIVE,0.986271858215332,2064,43,https://www.engadget.com/ai/lawsuit-says-mark-zuckerberg-approved-metas-use-of-pirated-materials-to-train-llama-ai-141548827.html
"Meta made its Llama 2 AI model open-source because 'Zuck has balls,' a former top Facebook engineer says","Meta made its Llama 2 AI model open-source because 'Zuck has balls,' a former top Facebook engineer says",NEGATIVE,0.9903869032859802,1415,178,https://www.businessinsider.com/meta-llama2-open-source-mark-zuckerberg-balls-replit-amjad-masad-2023-10
"Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present ‚ÄúBoth Sides‚Äù","Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present ‚ÄúBoth Sides‚Äù",POSITIVE,0.9938325881958008,180,184,https://www.404media.co/facebook-pushes-its-llama-4-ai-model-to-the-right-wants-to-present-both-sides/
[D] Llama-3 may have just killed proprietary AI models,"[D] Llama-3 may have just killed proprietary AI models [Full Blog Post ](https://www.kadoa.com/blog/llama3-killed-proprietary-models)

Meta released Llama-3 only three days  ago, and it already feels like the inflection point when open source models finally closed the gap with proprietary models. The initial benchmarks show that Llama-3 70B comes pretty close to GPT-4 in many tasks:

* The [official Meta page](https://llama.meta.com/llama3/) only shows that Llama-3 outperforms Gemini 1.5 and Claude Sonnet.
* [Artificial Analysis](https://artificialanalysis.ai/models/llama-3-instruct-70b) shows that Llama-3 is in-between Gemini-1.5 and Opus/GPT-4 for quality.
* On [LMSYS Chatbot Arena Leaderboard](https://arena.lmsys.org/), Llama-3 is ranked #5 while current GPT-4 models and Claude Opus are still tied at #1.

The even more powerful Llama-3 400B+ model is still in training and is likely to surpass GPT-4 and Opus once released.

## Meta vs OpenAI

Some speculate that Meta's goal from the start was to target OpenAI with a [""scorched earth""](https://en.wikipedia.org/wiki/Scorched_earth) approach by releasing powerful open models to disrupt the competitive landscape and avoid being left behind in the AI race.

Meta can likely outspend OpenAI on compute and talent:

* OpenAI makes an estimated revenue of $2B and is likely unprofitable. Meta generated a revenue of $134B and profits of $39B in 2023.
* Meta's compute resources likely outrank OpenAI by now.
* Open source likely attracts better talent and researchers.

One possible outcome could be the acquisition of OpenAI by Microsoft to catch up with Meta. Google is also making moves into the open model space and has similar capabilities to Meta. It will be interesting to see where they fit in.

## The Winners: Developers and AI Product Startups

I recently wrote about the [excitement of building an AI startup](https://www.kadoa.com/blog/why-building-an-ai-startup-feels-amazing) right now, as your product automatically improves with each major model advancement. With the release of Llama-3, the opportunities for developers are even greater:

* No more vendor lock-in.
* Instead of just wrapping proprietary API endpoints, developers can now integrate AI deeply into their products in a very cost-effective and performant way. There are already over 800 [llama-3 models variations on Hugging Face](https://huggingface.co/models?sort=modified&search=llama3), and it looks like everyone will be able to fine-tune for their us-cases, languages, or industry.
* Faster, cheaper hardware: Groq can now generate 800 llama-3 tokens per second at a small fraction of the GPT costs. Near-instant LLM responses at low prices are on the horizon.

Open source multimodal models for vision and video still have to catch up, but I expect this to happen very soon.

The release of Llama-3 marks a significant milestone in the democratization of AI, but it's probably too early to declare the death of proprietary models. Who knows, maybe GPT-5 will surprise us all and surpass our imaginations of what transformer models can do.

These are definitely super exciting times to build in the AI space!",NEGATIVE,0.9994801878929138,692,205,https://www.reddit.com/r/MachineLearning/comments/1cad7kk/d_llama3_may_have_just_killed_proprietary_ai/
"[R] ü§ñüåü Unlock the Power of Personal AI: Introducing ChatLLaMA, Your Custom Personal Assistant! üöÄüí¨","[R] ü§ñüåü Unlock the Power of Personal AI: Introducing ChatLLaMA, Your Custom Personal Assistant! üöÄüí¨ üöÄ Introducing ChatLLaMA: Your Personal AI Assistant Powered by LoRA! ü§ñ

&#x200B;

Hey AI enthusiasts! üåü We're excited to announce that you can now create custom personal assistants that run directly on your GPUs!

&#x200B;

ChatLLaMA utilizes LoRA, trained on Anthropic's HH dataset, to model seamless conversations between an AI assistant and users.

&#x200B;

Plus, the RLHF version of LoRA is coming soon! üî•

&#x200B;

üëâ Get it here: [https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)

&#x200B;

üìö Know any high-quality dialogue-style datasets? Share them with us, and we'll train ChatLLaMA on them!

&#x200B;

üåê ChatLLaMA is currently available for 30B and 13B models, and the 7B version.

&#x200B;

üîî Want to stay in the loop for new ChatLLaMA updates? Grab the FREE \[gumroad link\]([https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)) to sign up and access a collection of links, tutorials, and guides on running the model, merging weights, and more.  (Guides on running and training the model coming soon)

&#x200B;

ü§î Have questions or need help setting up ChatLLaMA? Drop a comment or DM us, and we'll be more than happy to help you out! üí¨

&#x200B;

Let's revolutionize AI-assisted conversations together! üåü

&#x200B;

\*Disclaimer: trained for research, no foundation model weights, and the post was ran through gpt4 to make it more coherent.

&#x200B;

üëâ Get it here: [https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)

&#x200B;

\*Edit: [https://github.com/serp-ai/LLaMA-8bit-LoRA](https://github.com/serp-ai/LLaMA-8bit-LoRA) <- training repo/instructions (If anything is unclear just let us know and we will try to help/fix the issue!)  (Sorry for spamming the link, don't really know how else to remind people lol)",POSITIVE,0.9980777502059937,727,247,https://www.reddit.com/r/MachineLearning/comments/11w03sy/r_unlock_the_power_of_personal_ai_introducing/
Spotify shuffle isn't shuffling? You're not alone,Spotify shuffle isn't shuffling? You're not alone,NEGATIVE,0.9895648956298828,8360,1028,https://www.androidauthority.com/spotify-shuffle-isnt-shuffling-3474262/
[R] Meta AI open sources new SOTA LLM called LLaMA. 65B version (trained on 1.4T tokens) is competitive with Chinchilla and Palm-540B. 13B version outperforms OPT and GPT-3 175B on most benchmarks.,"[R] Meta AI open sources new SOTA LLM called LLaMA. 65B version (trained on 1.4T tokens) is competitive with Chinchilla and Palm-540B. 13B version outperforms OPT and GPT-3 175B on most benchmarks. [https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19](https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19)

Paper here - [https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)",NEGATIVE,0.9774960279464722,622,214,https://www.reddit.com/r/MachineLearning/comments/11awp4n/r_meta_ai_open_sources_new_sota_llm_called_llama/
[D] Facebooks LLaMA leaks via torrent file in PR,"[D] Facebooks LLaMA leaks via torrent file in PR See here:
https://github.com/facebookresearch/llama/pull/73/files

Note that this PR *is not* made by a member of Facebook/Meta staff.    I have downloaded parts of the torrent and it does appear to be lots of weights, although I haven't confirmed it is trained as in the LLaMA paper, although it seems likely.


I wonder how much finetuning it would take to make this work like ChatGPT - finetuning tends to be much cheaper than the original training, so it might be something a community could do...",NEGATIVE,0.9990028738975525,527,183,https://www.reddit.com/r/MachineLearning/comments/11h3p2x/d_facebooks_llama_leaks_via_torrent_file_in_pr/
[N] Meta releases Llama 3,"[N] Meta releases Llama 3 [https://llama.meta.com/llama3/](https://llama.meta.com/llama3/) 

&#x200B;

&#x200B;

https://preview.redd.it/n3lwb4xfj9vc1.png?width=3840&format=png&auto=webp&s=b756d89c50c627955668d5ac16df82f7af01cdbc",NEGATIVE,0.9931105375289917,402,100,https://www.reddit.com/r/MachineLearning/comments/1c77f0m/n_meta_releases_llama_3/
"Mark Zuckerberg gave Meta's Llama team the OK to train on copyrighted works, filing claims","Mark Zuckerberg gave Meta's Llama team the OK to train on copyrighted works, filing claims",POSITIVE,0.990003228187561,404,36,https://techcrunch.com/2025/01/09/mark-zuckerberg-gave-metas-llama-team-the-ok-to-train-on-copyrighted-works-filing-claims/
"Winamp releases source code, asks for help modernizing the player","Winamp releases source code, asks for help modernizing the player",NEGATIVE,0.9676408171653748,5306,421,https://www.bleepingcomputer.com/news/software/winamp-releases-source-code-asks-for-help-modernizing-the-player/
[N] Llama 3.1 405B launches,"[N] Llama 3.1 405B launches https://llama.meta.com/

* Comparable to GPT-4o and Claude 3.5 Sonnet, according to the benchmarks
* The weights are publicly available
* 128K context",NEGATIVE,0.990256667137146,244,82,https://www.reddit.com/r/MachineLearning/comments/1eaaq05/n_llama_31_405b_launches/
Mark Zuckerberg's Meta releases early versions of Llama 3 AI model in bid to catch ChatGPT,Mark Zuckerberg's Meta releases early versions of Llama 3 AI model in bid to catch ChatGPT,NEGATIVE,0.996329128742218,273,89,https://nypost.com/2024/04/18/business/mark-zuckerbergs-meta-releases-early-versions-of-llama-3-ai-model-in-bid-to-catch-chatgpt/
President Biden meets with AI CEOs at the White House amid ethical criticism,President Biden meets with AI CEOs at the White House amid ethical criticism,NEGATIVE,0.9876700639724731,11225,866,https://arstechnica.com/information-technology/2023/05/critics-take-aim-at-bidens-ai-meeting-with-ceos-from-google-openai-microsoft/
[N] Llama 2 is here,"[N] Llama 2 is here Looks like a better model than llama according to the benchmarks they posted. But the biggest difference is that its free even for commercial usage. 


https://ai.meta.com/resources/models-and-libraries/llama/",NEGATIVE,0.9862648844718933,413,90,https://www.reddit.com/r/MachineLearning/comments/1533mj9/n_llama_2_is_here/
[N] OpenLLaMA: An Open Reproduction of LLaMA,"[N] OpenLLaMA: An Open Reproduction of LLaMA https://github.com/openlm-research/open_llama

> We train our models on the RedPajama dataset released by Together, which is a reproduction of the LLaMA training dataset containing over 1.2 trillion tokens. We follow the exactly same preprocessing steps and training hyperparameters as the original LLaMA paper, including model architecture, context length, training steps, learning rate schedule, and optimizer. The only difference between our setting and the original one is the dataset used: OpenLLaMA employs the RedPajama dataset rather than the one utilized by the original LLaMA.",NEGATIVE,0.9984388947486877,388,98,https://www.reddit.com/r/MachineLearning/comments/136exj2/n_openllama_an_open_reproduction_of_llama/
Epic Games sued over ‚Äòpredatory‚Äô Llama loot boxes,Epic Games sued over ‚Äòpredatory‚Äô Llama loot boxes,NEGATIVE,0.9956335425376892,698,145,https://www.theverge.com/2019/2/28/18245574/fortnite-epic-games-sued-lawsuit-predatory-llama-loot-boxes
"Meta's leak problem just cost 20 employees their jobs, with more firings expected","Meta's leak problem just cost 20 employees their jobs, with more firings expected",NEGATIVE,0.9997382760047913,1610,207,https://www.techspot.com/news/106965-meta-leak-problem-cost-20-employees-their-jobs.html
US restricts Switzerland's access to AI chips,US restricts Switzerland's access to AI chips,NEGATIVE,0.9979985356330872,1847,205,https://www.swissinfo.ch/eng/multinational-companies/us-restricts-switzerlands-access-to-ai-chips/88781270?utm_source=multiple&utm_medium=website&utm_campaign=ne
[Project] Alpaca-30B: Facebook's 30b parameter LLaMa fine-tuned on the Alpaca dataset,"[Project] Alpaca-30B: Facebook's 30b parameter LLaMa fine-tuned on the Alpaca dataset How to fine-tune Facebooks 30 billion parameter LLaMa on the Alpaca data set.

Blog post: [https://abuqader.substack.com/p/releasing-alpaca-30b](https://abuqader.substack.com/p/releasing-alpaca-30b)

Weights: [https://huggingface.co/baseten/alpaca-30b](https://huggingface.co/baseten/alpaca-30b)",NEGATIVE,0.9948744177818298,295,80,https://www.reddit.com/r/MachineLearning/comments/11wqmga/project_alpaca30b_facebooks_30b_parameter_llama/
"[P] Llama 3.2 1B-Based Conversational Assistant Fully On-Device (No Cloud, Works Offline)","[P] Llama 3.2 1B-Based Conversational Assistant Fully On-Device (No Cloud, Works Offline) I‚Äôm launching a privacy-first mobile assistant that runs a **Llama 3.2 1B Instruct model**, **Whisper Tiny ASR**, and **Kokoro TTS**, all **fully on-device**.

What makes it different:

* Entire pipeline (ASR ‚Üí LLM ‚Üí TTS) runs locally
* Works with no internet connection
* No user data ever touches the cloud
* Built on ONNX runtime and a custom on-device Python‚ÜíAST‚ÜíC++ execution layer SDK

We believe on-device AI assistants are the future ‚Äî especially as people look for alternatives to cloud-bound models and surveillance-heavy platforms.

",POSITIVE,0.7468648552894592,18,16,https://www.reddit.com/r/MachineLearning/comments/1kkw6cf/p_llama_32_1bbased_conversational_assistant_fully/
"[D] How to and Deploy LLaMA 3 Into Production, and Hardware Requirements
","[D] How to and Deploy LLaMA 3 Into Production, and Hardware Requirements
 Many are trying to install and deploy their own LLaMA 3 model, so here is a tutorial I just made showing how to deploy LLaMA 3 on an AWS EC2 instance:¬†[https://nlpcloud.com/how-to-install-and-deploy-llama-3-into-production.html](https://nlpcloud.com/how-to-install-and-deploy-llama-3-into-production.html?utm_source=reddit&utm_campaign=fqwerty13-6816-81ed-a26450242ac140019)

Deploying LLaMA 3 8B is fairly easy but LLaMA 3 70B is another beast. Given the amount of VRAM needed you might want to provision more than one GPU and use a dedicated inference server like vLLM in order to split your model on several GPUs.

LLaMA 3 8B requires around 16GB of disk space and 20GB of VRAM (GPU memory) in FP16. As for LLaMA 3 70B, it requires around 140GB of disk space and 160GB of VRAM in FP16.

I hope it is useful, and if you have questions please don't hesitate to ask!

Julien",NEGATIVE,0.9984323382377625,172,52,https://www.reddit.com/r/MachineLearning/comments/1cb3ge1/d_how_to_and_deploy_llama_3_into_production_and/
[N] Senators are sending letters to Meta over LLAMA leak,"[N] Senators are sending letters to Meta over LLAMA leak Two Senators a democrat and republican sent a letter questioning Meta about their LLAMA leak and expressed concerns about it. Personally I see it as the internet and there is already many efforts done to prevent misuse like disinformation campaigns. 

‚Äúpotential for its misuse in spam, fraud, malware, privacy violations, harassment, and other wrongdoing and harms‚Äù

I think the fact that from the reasons cited shows the law makers don‚Äôt know much about it and we make AI look like too much of a black box to other people. I disagree the dangers in AI are there because social media platforms and algorithms learned how to sift out spam and such things they are concerned about. The same problem with bots are similar issues that AI poses and we already have something to work off of easily. 

What do you all think?

Source: 

https://venturebeat.com/ai/senators-send-letter-questioning-mark-zuckerberg-over-metas-llama-leak/",NEGATIVE,0.9955007433891296,99,115,https://www.reddit.com/r/MachineLearning/comments/14346o5/n_senators_are_sending_letters_to_meta_over_llama/
[D] Why did DeepSeek open-source their work?,"[D] Why did DeepSeek open-source their work? If their training is 45x more efficient, they could have dominated the LLM market. Why do you think they chose to open-source their work? How is this a net gain for their company? Now the big labs in the US can say: ""we'll take their *excellent* ideas and we'll just combine them with our *secret* ideas, and we'll still be ahead""

---

*Edit:* `DeepSeek-R1` is now ranked #1 in the LLM Arena (with `StyleCtrl`). They share this rank with 3 other models: `Gemini-Exp-1206`, `4o-latest` and `o1-2024-12-17`.",POSITIVE,0.9986312985420227,952,331,https://www.reddit.com/r/MachineLearning/comments/1ib2vtx/d_why_did_deepseek_opensource_their_work/
[N] Llama 4 release,"[N] Llama 4 release [Llama4 ELO score vs cost](https://preview.redd.it/sttz9ok0h2te1.png?width=1525&format=png&auto=webp&s=c57ae7cf9e69afb461237e7b47a8526275223739)

  
[https://www.llama.com/](https://www.llama.com/)",NEGATIVE,0.9896752238273621,121,6,https://www.reddit.com/r/MachineLearning/comments/1jsbbuy/n_llama_4_release/
"[P] The first RedPajama models are here! The 3B and 7B models are now available under Apache 2.0, including instruction-tuned and chat versions. These models aim replicate LLaMA as closely as possible.","[P] The first RedPajama models are here! The 3B and 7B models are now available under Apache 2.0, including instruction-tuned and chat versions. These models aim replicate LLaMA as closely as possible.",NEGATIVE,0.9889623522758484,406,48,https://www.together.xyz/blog/redpajama-models-v1
Judge on Meta‚Äôs AI training: ‚ÄúI just don‚Äôt understand how that can be fair use‚Äù,Judge on Meta‚Äôs AI training: ‚ÄúI just don‚Äôt understand how that can be fair use‚Äù,NEGATIVE,0.9995031356811523,1164,142,https://arstechnica.com/tech-policy/2025/05/judge-on-metas-ai-training-i-just-dont-understand-how-that-can-be-fair-use/
Meta's new AI model Llama 4 has been released,Meta's new AI model Llama 4 has been released,POSITIVE,0.8965772390365601,0,19,https://ai.meta.com/blog/llama-4-multimodal-intelligence/
[P] Self-Hosting a 16B LLAMA 2 Model in the Banking Sector: What Could Go Wrong?,"[P] Self-Hosting a 16B LLAMA 2 Model in the Banking Sector: What Could Go Wrong? I've received a freelance job offer from a company in the banking sector that wants to host their own LLAMA 2 model in-house.

I'm hesitating to accept the gig. While I'll have access to the hardware (I've estimated that an A100 80GB will be required to host the 16B parameter version and process some fine-tuning & RAG), I'm not familiar with the challenges of self-hosting a model of this scale. I've always relied on managed services like Hugging Face or Replicate for model hosting.

For those of you who have experience in self-hosting such large models, what do you think will be the main challenges of this mission if I decide to take it on?

&#x200B;

**Edit: Some additional context information**

**Size of the company**: Very small \~ 60 employees

**Purpose**: This service will be combined with a vector store to search content such as Word, Excel and PowerPoint files stored on their servers. I'll implement the RAG pattern and do some prompt engineering with it. They also want me to use it for searching things on specific websites and APIs, such as stock exchanges, so I (probably) need to fine-tune the model based on the search results and the tasks I want the model to do after retrieving the data.",NEGATIVE,0.9962574243545532,35,101,https://www.reddit.com/r/MachineLearning/comments/165dd1z/p_selfhosting_a_16b_llama_2_model_in_the_banking/
[D] Is it possible to run Meta's LLaMA 65B model on consumer-grade hardware?,"[D] Is it possible to run Meta's LLaMA 65B model on consumer-grade hardware? I think it's clear that a single beefy rig could handle the 7B model, but what about the big one? What kind of hardware are we looking at? What's the price range?

I'd imagine something like this:

* high end motherboard with lots of PCIe slots
* 256GB of RAM (doable for a high end gaming rig)
* some beefy CPU like the latest Threadripper
* 2TB or more of SSD storage
* a robust power supply (would 1000W be enough?)
* 2 NVIDIA A100 80GB devices to total up to 160GB of vRAM
* a big case and maybe a water cooler

Am I on the right track here? What am I missing?

(note: I don't intend to buy this hardware and run this model, but I think it's a fascinating discussion)",NEGATIVE,0.9898573160171509,73,109,https://www.reddit.com/r/MachineLearning/comments/11i4olx/d_is_it_possible_to_run_metas_llama_65b_model_on/
Researchers removed Llama 3's safety guardrails in just 3 minutes,Researchers removed Llama 3's safety guardrails in just 3 minutes,NEGATIVE,0.9791184663772583,73,39,https://arxiv.org/abs/2407.01376
[P] Llama on Windows (WSL) fast and easy,"[P] Llama on Windows (WSL) fast and easy  In this video tutorial, you will learn how to install Llama - a powerful  generative text AI model - on your Windows PC using WSL (Windows  Subsystem for Linux). With Llama, you can generate high-quality text in a variety of styles, making it an essential tool for writers, marketers, and content creators. This tutorial will guide you through a very simple and fast process of installing Llama on your Windows PC using WSL, so you can start exploring Llama in no time.

Github: [https://github.com/Highlyhotgames/fast\_txtgen\_7B](https://github.com/Highlyhotgames/fast_txtgen_7B)

This project allows you to download other models from the 4-bit 128g (7B/13B/30B/65B)

https://github.com/Highlyhotgames/fast_txtgen

Follow the instructions on the webpage while u see the tutorial here:

Youtube: [https://www.youtube.com/watch?v=RcHIOVtYB7g](https://www.youtube.com/watch?v=RcHIOVtYB7g)

NEW: Installation script designed for Ubuntu 22.04 (NVIDIA only):

https://github.com/Highlyhotgames/fast_txtgen/blob/Linux/README.md",POSITIVE,0.9992828965187073,221,65,https://www.reddit.com/r/MachineLearning/comments/12fg7sc/p_llama_on_windows_wsl_fast_and_easy/
[P] Discord Chatbot for LLaMA 4-bit quantized that runs 13b in <9 GiB VRAM,[P] Discord Chatbot for LLaMA 4-bit quantized that runs 13b in <9 GiB VRAM,NEGATIVE,0.998764157295227,320,46,https://github.com/AmericanPresidentJimmyCarter/yal-discord-bot
Looking for some advice on choosing between Gemini and Llama for my AI project.,"Looking for some advice on choosing between Gemini and Llama for my AI project. Working on a conversational AI project that can dynamically switch between AI models. I have integrated ChatGPT and Claude so far but don't know which one to choose next between Gemini and Llama for the MVP.

**My evaluation criteria:**

* API reliability and documentation quality
* Unique strengths that complement my existing models
* Cost considerations
* Implementation complexity
* Performance on specialized tasks

For those who have worked with both, I'd appreciate insights on:

1. Which model offers more distinctive capabilities compared to what I already have?
2. Implementation challenges you encountered with either
3. Performance observations in production environments
4. If you were in my position, which would you prioritize and why?

Thanks in advance for sharing your expertise!",POSITIVE,0.9881604909896851,7,5,https://www.reddit.com/r/artificial/comments/1kd3jgk/looking_for_some_advice_on_choosing_between/
[R] LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,[R] LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,POSITIVE,0.9965981841087341,229,57,https://arxiv.org/abs/2303.16199
"[P] 80% faster, 50% less memory, 0% loss in accuracy Llama finetuning","[P] 80% faster, 50% less memory, 0% loss in accuracy Llama finetuning Hey [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)!

I  manually derived backpropagation steps, did some chained matrix  multiplication optims, wrote all kernels in OpenAI's Triton language and  did more maths and coding trickery to make QLoRA finetuning for Llama  5x faster on Unsloth: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)! Some highlights:

* **5x faster** (5 hours to 1 hour)
* Use **50% less memory**
* With **0% loss in accuracy**
* All **locally** on NVIDIA GPUs (Tesla T4, RTX 20/30/40, Ampere, Hopper) for **free**!
* QLoRA / LoRA is now 80% faster to train.

On  Slim Orca 518K examples on 2 Tesla T4 GPUs via DDP, Unsloth trains 4bit  QLoRA on all layers in 260 hours VS Huggingface's original  implementation of 1301 hours.

[Slim Orca 1301 hours to 260 hours](https://preview.redd.it/54qzkb66np3c1.png?width=1000&format=png&auto=webp&s=5f6fd95482263cbdca225415af91d49342bea10e)

You might (most likely not) remember me from Hyperlearn ([https://github.com/danielhanchen/hyperlearn](https://github.com/danielhanchen/hyperlearn)) which I launched a few years back to make ML algos 2000x faster via maths and coding tricks.

I wrote up a blog post about all the manual hand derived backprop via [https://unsloth.ai/introducing](https://unsloth.ai/introducing).

I wrote a Google Colab for T4 for Alpaca: [https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing](https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing) which finetunes Alpaca 2x faster on a single GPU.

On Kaggle via 2 Tesla T4s on DDP: [https://www.kaggle.com/danielhanchen/unsloth-laion-chip2-kaggle](https://www.kaggle.com/danielhanchen/unsloth-laion-chip2-kaggle), finetune LAION's OIG 5x faster and Slim Orca 5x faster.

You can install Unsloth all locally via:

    pip install ""unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git""
    
    pip install ""unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git""  

Currently we only support Pytorch 2.1 and Linux distros - more installation instructions via [https://github.com/unslothai/unsloth/blob/main/README.md](https://github.com/unslothai/unsloth/blob/main/README.md)

I hope to:

1. Support other LLMs other than Llama style models (Mistral etc)
2. Add sqrt gradient checkpointing to shave another 25% of memory usage.
3. And other tricks!

Thanks a bunch!!",NEGATIVE,0.9994078874588013,230,36,https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/
"Llama 3.3 better than 4o, o1","Llama 3.3 better than 4o, o1 I have started myself to shift from ChatGPT models because now they have started to go out of context for no reason.

You ask them to make a summary of what we have discussed on this chat so far and it fails to note the important points.

And if the chat is pretty lengthy, it just dismisses and gets bizarre info out of it.

I think I smell the commercialness of it spreading across the room.

I have had better output from the recent Microsoft's phi-4 model compared to 4o.",NEGATIVE,0.9997465014457703,72,15,https://www.reddit.com/r/artificial/comments/1hg3xsw/llama_33_better_than_4o_o1/
[D] Llama 3.2 Detailed Analysis,"[D] Llama 3.2 Detailed Analysis Hey folks! Meta released a new set of Llama 3.2 models for text (1B, 3B) and vision (11B, 90B). I took a deep dive of the models and hopefully it's insightful:

1. New 1B and 3B text only LLMs 9 trillion tokens
2. New 11B and 90B vision multimodal models
3. 128K context length
4. 1B and 3B used some distillation from 8B and 70B
5. VLM 6 billion img, text pairs
6. CLIP MLP GeLU + cross attention

Long analysis:
1. CLIP type MLP with GeLU activation used in vision encoder. Similar to GPT2's MLP. Different to Llama 3's MLP since SwiGLU is not used for the vision MLP.

2. Normal layernorm used for vision encoder - not RMS Layernorm. Also some ""gating"" parameter is used to multiply the hidden states.

3. Gating multiplier done to hidden states after attention and MLP - tanh used to move vector scaling to numbers from -1 to 1.

4. Evals look pretty good for small 1B and 3B LLMs and multimodal VLMs 11B and 90B. 1B 49.3 MMLU and 3B 63.4. VLM MMMU 50.7 and 90B 60.3

Thank you for reading and if you have any questions please let me know!",POSITIVE,0.9932818412780762,71,25,https://www.reddit.com/r/MachineLearning/comments/1fpckbb/d_llama_32_detailed_analysis/
[R][P] Byte-level LLaMA and Gemma via cross-tokenizer distillation (with open-source toolkit),"[R][P] Byte-level LLaMA and Gemma via cross-tokenizer distillation (with open-source toolkit) Hello r/MachineLearning !

I‚Äôve been experimenting with a method called ALM to distill language models *across tokenizers.* This enables, for example, transferring LLMs to a new tokenizer and distilling knowledge from a model with one tokenizer into a model with a different tokenizer (see [our paper](https://arxiv.org/abs/2503.20083) for details).

I‚Äôve released [tokenkit](https://github.com/bminixhofer/tokenkit), a library implementing ALM among other methods, to make this easy to use.

One neat application of ALM is distilling subword-based LLMs into byte-level models. I've applied this to two instruction-tuned models:

* Gemma2-2B-IT-Byte: [https://huggingface.co/benjamin/Gemma2-2B-IT-Byte](https://huggingface.co/benjamin/Gemma2-2B-IT-Byte)
* Llama3-2-3B-IT-Byte: [https://huggingface.co/benjamin/Llama3-2-3B-IT-Byte](https://huggingface.co/benjamin/Llama3-2-3B-IT-Byte)

Even though the distillation phase is very short (just 1.2B bytes ‚âà 330M subword tokens), the models perform competitively (for example **57.0% MMLU of the byte-level Llama** vs. 62.4% MMLU of the original Llama3-3B-Instruct).

This approach opens up an interesting direction: we can potentially keep subword tokenization for pretraining (to still squeeze as much text into the model in as little time as possible), but then change to a more user-friendly tokenization afterwards.

These models aren‚Äôt yet optimized for efficiency, but if you would add [self-speculative decoding](https://arxiv.org/abs/2309.08168) plus a [BLT/DTP-style hierarchical architecture](https://arxiv.org/abs/2211.09761) and/or [linearized attention](https://openreview.net/forum?id=G-uNfHKrj46), they might also be able to replace subword-based models when speed matters.

If you want to train your own models, [this guide on tokenizer transfer via tokenkit](https://github.com/bminixhofer/tokenkit/blob/main/docs/tokenizer_transfer.md) should make it easy. The model cards of the transfers above also contain the exact command used to train them. I‚Äôve been training on fairly limited hardware, so effective transfer is possible even in a (near) consumer-grade setup.

I'd love to get feedback on the method, the models, or tokenkit itself. Happy to discuss or answer questions!",NEGATIVE,0.9951440095901489,32,2,https://www.reddit.com/r/MachineLearning/comments/1k6oxvq/rp_bytelevel_llama_and_gemma_via_crosstokenizer/
[D] Small stupid question about Llama 4 implementation,"[D] Small stupid question about Llama 4 implementation So there used to be the No stupid question thread for a while, not anymore so here's one in a new thread:

In Llama 4 MOEs, my understanding, is that the implementation of the Expert mechanism works that way:

Calculating the weights the same way as traditional MOEs
Calculating expert output for every experts on every tokens
Weighted Sum of only the selected experts based on the routing logits
And a shared expert
My question then is this: Doesn't that need a lot more RAM than traditional MOE? Also, is there a more efficient way of doing this?

Like is there a way to have the best of both worlds : the parallelism of this method while having the smaller memory usage of the traditional one?",NEGATIVE,0.998501181602478,4,0,https://www.reddit.com/r/MachineLearning/comments/1kkcvuz/d_small_stupid_question_about_llama_4/
"[P] Serge, a self-hosted app for running LLaMa models (Alpaca) entirely locally, no remote API needed.","[P] Serge, a self-hosted app for running LLaMa models (Alpaca) entirely locally, no remote API needed. Hello there!

[Serge chat UI, with conversations on the left](https://preview.redd.it/rayrn7m4ncpa1.png?width=1922&format=png&auto=webp&s=d99a8d8751e4113eddde0cba92d57f20acb2cbb2)

I've recently been working on Serge, a self-hosted dockerized way of running LLaMa models with a decent UI & stored conversations. It currently supports Alpaca 7B, 13B and 30B and we're working on integrating it with LangChain and the ReAct chain agent.

I've tried my best at making the instructions dead easy, so it's all dockerized with a download manager for weights and it can be run with almost zero configuration required.

I think being able to run those models locally will be key to expanding their ability, and so I hope this can contribute to that.

Let me know if you have any feedback or suggestions on how to extend its capabilities!

&#x200B;

GitHub: [https://github.com/nsarrazin/serge](https://github.com/nsarrazin/serge)",NEGATIVE,0.9968172311782837,164,60,https://www.reddit.com/r/MachineLearning/comments/11yvbzc/p_serge_a_selfhosted_app_for_running_llama_models/
[P] Converting GPT to Llama step-by-step code guide,"[P] Converting GPT to Llama step-by-step code guide An often-asked question is how GPT compares to Llama. In my opinion, one of the best ways to understand the differences is to implement both architectures from scratch. Here's a [step-by-step Jupyter notebook guide](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb).

https://preview.redd.it/qowi1sf12krd1.jpg?width=4286&format=pjpg&auto=webp&s=b815e4e6df8d38c70816fb6f51ff1482b6cca80e

",NEGATIVE,0.9926038384437561,119,15,https://www.reddit.com/r/MachineLearning/comments/1frer4z/p_converting_gpt_to_llama_stepbystep_code_guide/
Llama 4 is here,Llama 4 is here,POSITIVE,0.9989137649536133,14,3,https://ai.meta.com/blog/llama-4-multimodal-intelligence/
"[P] fastLLaMa, A python wrapper to run llama.cpp","[P] fastLLaMa, A python wrapper to run llama.cpp Hi all, I have been working on fastLLaMa. It is a Python package that provides a Pythonic interface to a C++ library, llama.cpp. It allows you to use the functionality of the C++ library from within Python, without having to write C++ code or deal with low-level C++ APIs.

Using fastLLaMa, you can ingest the model with system prompts and then save the state of the model, Then later load the state, and start inferencing the model immediately.

No noticeable performance drop between lama.cpp and fastLLaMa.

Have a look at it if it is of interest and do let me know what you think :)

[Repo Link](https://github.com/PotatoSpudowski/fastLLaMa)

[Tweet](https://twitter.com/Bahushruth/status/1638231265320239106)  


Update 1:  
Hi good folks!  
I have added support to export Peft fine-tuned models and run them as well.   
Should be easy to quickly test out the capabilities of Peft fine-tuning.   
Happy hacking :)",NEGATIVE,0.8597700595855713,227,46,https://www.reddit.com/r/MachineLearning/comments/11y9qgg/p_fastllama_a_python_wrapper_to_run_llamacpp/
Millions of Llamas inconsolable after gmail blackouts,Millions of Llamas inconsolable after gmail blackouts,NEGATIVE,0.9980605244636536,585,99,http://twitpic.com/gal3w
"Meta has revenue sharing agreements with Llama AI model hosts, filing reveals | TechCrunch","Meta has revenue sharing agreements with Llama AI model hosts, filing reveals | TechCrunch",NEGATIVE,0.9821037650108337,24,1,https://techcrunch.com/2025/03/21/meta-has-revenue-sharing-agreements-with-llama-ai-model-hosts-filing-reveals/
"LIMA, a 65B-Param LLaMa fine-tuned with standard supervised loss on only 1,000 carefully curated prompts & responses, without any RLHF, demonstrates remarkably strong performance, learning to follow specific responses from only a handful of examples in the training data, including complex queries.","LIMA, a 65B-Param LLaMa fine-tuned with standard supervised loss on only 1,000 carefully curated prompts & responses, without any RLHF, demonstrates remarkably strong performance, learning to follow specific responses from only a handful of examples in the training data, including complex queries.",POSITIVE,0.9989965558052063,308,29,https://arxiv.org/abs/2305.11206
"Finland Dumps Handwriting In Favor Of Typing -- from autumn 2016 cursive handwriting will no longer be a compulsory part of the school curriculum. Instead the schools will teach keyboard skills and ""texting""","Finland Dumps Handwriting In Favor Of Typing -- from autumn 2016 cursive handwriting will no longer be a compulsory part of the school curriculum. Instead the schools will teach keyboard skills and ""texting""",NEGATIVE,0.9995052814483643,7530,1114,http://www.i-programmer.info/news/150-training-a-education/8005-finland-dumps-handwriting-in-favor-of-typing.html
[P] Run Llama 2 Locally in 7 Lines! (Apple Silicon Mac),"[P] Run Llama 2 Locally in 7 Lines! (Apple Silicon Mac) **Want to start playing with Meta‚Äôs Llama 2?**

It takes just 7 lines of shell script using llama.cpp to get you started!

https://preview.redd.it/vhuzhrj4h6db1.png?width=2030&format=png&auto=webp&s=d349dd796039f3af7e117423c4abdae7efde2fae

Copy Code Snippet:¬†[https://lastmileai.dev/workbooks/clkbifegg001jpheon6d2s4m8](https://lastmileai.dev/workbooks/clkbifegg001jpheon6d2s4m8) ",NEGATIVE,0.9981386661529541,118,49,https://www.reddit.com/r/MachineLearning/comments/1551wp6/p_run_llama_2_locally_in_7_lines_apple_silicon_mac/
Llama 3.3 has been released!,"Llama 3.3 has been released! Llama 3.3 has been released!
[https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)
The 70B model has been fine-tuned to the point where it occasionally outperforms the 405B model. There's a particularly significant improvement in math and coding tasks, where Llama has traditionally been weaker. This time, only the 70B model is being released‚Äîthere are no other sizes or VLM versions.",POSITIVE,0.9828326106071472,92,5,https://www.reddit.com/r/artificial/comments/1h8624m/llama_33_has_been_released/
[P] Higgsfield.AI ‚Äì Anyone can train Llama 70B or Mistral for free,"[P] Higgsfield.AI ‚Äì Anyone can train Llama 70B or Mistral for free [https://higgsfield.ai](https://higgsfield.ai)

We have developed our own infrastructure to train massive models.

There's how it works:

1. You upload the dataset with preconfigured format into HuggingFa—Åe \[1\].
2. Choose your LLM (e.g. LLaMa 70B, Mistral 7B)
3. Place your submission into the queue
4. Wait for it to get trained.
5. Then you get your trained model there on HuggingFace.

Essentially, why would we want to do it?

1. We already have an experience with training big LLMs.
2. We could achieve near-perfect infrastructure performance for training.
3. Sometimes GPUs have just nothing to train.

Thus we thought it would be cool if  could give back to Open Source community (already built an e2e distributed training framework \[2\]).

This is in an early stage, so you can expect some bugs.

Any thoughts, opinions, or ideas are quite welcome!

\[1\]: [https://github.com/higgsfield-ai/higgsfield/blob/main/tutori...](https://github.com/higgsfield-ai/higgsfield/blob/main/tutorials/README.md)

\[2\]: [https://github.com/higgsfield-ai/higgsfield](https://github.com/higgsfield-ai/higgsfield)",NEGATIVE,0.9959132075309753,180,29,https://www.reddit.com/r/MachineLearning/comments/17usssn/p_higgsfieldai_anyone_can_train_llama_70b_or/
"Winamp shutting down December 20, 2013","Winamp shutting down December 20, 2013",NEGATIVE,0.9989845156669617,4403,4909,http://www.winamp.com/media-player/en
[P] [D] Comparing Llama Models and GPT 4o Models on Multilingual Machine Translation with Backtranslation,"[P] [D] Comparing Llama Models and GPT 4o Models on Multilingual Machine Translation with Backtranslation Hey all,

In the spirit of practical real world tasks for LLMs, we wanted to see how well different models could automatically translate text from English to Spanish and the backtranslate to English on a Nike product catalog. We started with Llama 405B, Llama 70B, Llama 8B, GPT 4o-mini, and GPT 4o, but would love to test [more models](https://www.oxen.ai/explore/models).

  
\~ TLDR \~ Here are the results with all the data and code here:

[https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments](https://www.oxen.ai/datasets/Nike-Product-Translation-Experiments)

https://preview.redd.it/qken2vjfhc3e1.png?width=1150&format=png&auto=webp&s=739ef336dd7b89856a39d872ef12e03f806ce799

Although backtranslation may not be the most effective way to benchmark, we thought this would be an interesting experiment to see how well it correlates with model performance. It would be ideal to get native Spanish speakers to annotate the dataset with ground truth labels, so if anyone wants to contribute feel free to fork the repo and we can get some real labels.

  
We're trying to make some more real world datasets / benchmarks, so let us know if you want to help out.

If you‚Äôre new to the [Oxen.ai](https://www.oxen.ai/) project, we‚Äôre building a fast¬†[open source dataset collaboration tools](https://github.com/Oxen-AI/oxen-release)¬†as well as a ton of [helpful data exploration tools](https://docs.oxen.ai/features/web_hub) on top of it! If you are into data or ML/AI, we‚Äôd love your thoughts on the tool and project!",NEGATIVE,0.9726693630218506,15,13,https://www.reddit.com/r/MachineLearning/comments/1h0sehj/p_d_comparing_llama_models_and_gpt_4o_models_on/
[P] How much VRAM I need to train llama 3 8B?,"[P] How much VRAM I need to train llama 3 8B? Hello,

I assume very noob question, but can not find an answer.

I want to take llama 3 8b and enhance model with my custom data. 

I want to do both training and run model locally, on my Nvidia GPU.

I don't have GPU now, only mac m2 pro 16Gb,  and need to know what to purchase.

I wonder, what are the VRAM requirements? Would I be fine with 12 GB, or I need to get gpu with 16? Or only way is 24 GB 4090 like stuff?",NEGATIVE,0.9988083839416504,36,28,https://www.reddit.com/r/MachineLearning/comments/1dbp2sz/p_how_much_vram_i_need_to_train_llama_3_8b/
AT&T to pay $23M fine for bribing powerful lawmaker‚Äôs ally in exchange for vote,AT&T to pay $23M fine for bribing powerful lawmaker‚Äôs ally in exchange for vote,NEGATIVE,0.9946974515914917,3853,230,https://arstechnica.com/tech-policy/2022/10/att-to-pay-23m-fine-for-bribing-powerful-lawmakers-ally-in-exchange-for-vote/
[Project] Llama 3 8B is not doing well at text understanding: alternatives?,"[Project] Llama 3 8B is not doing well at text understanding: alternatives? Hey! I've been trying to use Llama-3-8B-Instruct to recognise and extract quantities, descriptions and prices of various products. Regex is not an option as the documents are not well structured enough. NER is not an option as I have no labeled dataset. Therefore I opted for a LLM, but Llama3 is not doing well. It cannot deal with variation very well. I've tried with few-shot and CoT, but same unsatisfactory results.

Apart from asking the company to pay a few hundreds of buck for GPT4 (which would do this really well), what are my other options? Any other models I can run locally that are more powerful than this version of Llama3? 

Thanks!",NEGATIVE,0.9997422099113464,5,19,https://www.reddit.com/r/MachineLearning/comments/1g0tg1m/project_llama_3_8b_is_not_doing_well_at_text/
[P] llama.ttf: A font which is also an LLM,[P] llama.ttf: A font which is also an LLM,NEGATIVE,0.9836834073066711,127,15,https://fuglede.github.io/llama.ttf/
[R] üöÄüß† Introducing 3 New LoRA Models Trained with LLaMA on the OASST Dataset at 2048 seq length! üìäüî•,"[R] üöÄüß† Introducing 3 New LoRA Models Trained with LLaMA on the OASST Dataset at 2048 seq length! üìäüî• We are super excited to announce the release of 3 brand new LoRA models trained using the LLaMA model! These state-of-the-art models have been trained on the full 2048 sequence length for 4 epochs, using the OASST dataset. üåêüí°

Shoutout to LAION and Open-Assistant for giving us early research access to the dataset  üéâ

Checkout this and more over on our [FREE gumroad](https://serp.ly/@serpai/chat-llama) if you want to sign up for future releases and guides as well.

Checkout out our website for a post with more info:  [https://serp.ai/chat-llama/](https://serp.ai/chat-llama/)

\- [LoRA-7B](https://huggingface.co/serpdotai/llama-oasst-lora-7B) üöÄ

\- [LoRA-13B](https://huggingface.co/serpdotai/llama-oasst-lora-13B) üí•

\- [LoRA-30B](https://huggingface.co/serpdotai/llama-oasst-lora-30B) üåå

We can't wait to see what amazing things you'll be able to accomplish with these new models! üåü So, feel free to share your experiences, ask questions, or discuss the potential applications for these models. üß™üî¨

Happy experimenting, and let's revolutionize the world of machine learning together! üíªüåç

[Checkout our github](https://github.com/serp-ai) for LLaMA LoRA training repos, inferencing guis, chat plugins (that you can also use with llama), and more.

Cheers! üçª",POSITIVE,0.8551564812660217,303,24,https://www.reddit.com/r/MachineLearning/comments/12rds2h/r_introducing_3_new_lora_models_trained_with/
[P] Implementing the Llama 3.2 1B and 3B Architectures from Scratch (A Standalone Jupyter Notebook),[P] Implementing the Llama 3.2 1B and 3B Architectures from Scratch (A Standalone Jupyter Notebook),NEGATIVE,0.996910035610199,120,5,https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/standalone-llama32.ipynb
"PIN AI launches mobile app letting you make your own personalized, private DeepSeek or Llama-powered AI model on your phone | VentureBeat","PIN AI launches mobile app letting you make your own personalized, private DeepSeek or Llama-powered AI model on your phone | VentureBeat",NEGATIVE,0.9630746245384216,0,3,https://venturebeat.com/ai/pin-ai-launches-mobile-app-letting-you-make-your-own-personalized-private-deepseek-or-llama-powered-ai-model-on-your-phone/
"ChatGPT, DeepSeek, Or Llama? Meta‚Äôs LeCun Says Open-Source Is The Key","ChatGPT, DeepSeek, Or Llama? Meta‚Äôs LeCun Says Open-Source Is The Key",NEGATIVE,0.5224936008453369,0,5,https://www.forbes.com/sites/luisromero/2025/01/27/chatgpt-deepseek-or-llama-metas-lecun-says-open-source-is-the-key/
[R] Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis,"[R] Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis By adding a speech tokenizer and special speech tokens, Llama can be turned into a competent STT and TTS system capable of high accuracy zero shot voice cloning.

The models have been out for a few weeks and are impressive, now the paper is out.

https://arxiv.org/pdf/2502.04128",POSITIVE,0.9189138412475586,18,1,https://www.reddit.com/r/MachineLearning/comments/1ikq3bz/r_llasa_scaling_traintime_and_inferencetime/
Meta is pushing for the government to use its AI | CEO Mark Zuckerberg says the company is working to get Llama used ‚Äòacross the US government.‚Äô,Meta is pushing for the government to use its AI | CEO Mark Zuckerberg says the company is working to get Llama used ‚Äòacross the US government.‚Äô,NEGATIVE,0.9925374388694763,0,14,https://www.theverge.com/2024/10/30/24283985/meta-government-llama-ai-q3-earnings
[D] Meta's new LLama model,"[D] Meta's new LLama model So meta just dropped a new, more efficient Llama model, Llama 3.3 70B, that basically promises to cut compute costs for large AI models. Has anyone here had a chance to test it out? Curious to see how it performs compared to previous versions, in terms of speed, resource usage, and accuracy",NEGATIVE,0.9991045594215393,23,7,https://www.reddit.com/r/MachineLearning/comments/1han33i/d_metas_new_llama_model/
Performance drops after fine tuning Llama 3 [D],Performance drops after fine tuning Llama 3 [D] I fine tuned a Llama3 model with a Lora adapter for a classification task. I have about 9000 samples and I trained the model for about 5 epochs. But the recall of the fine tuned model is worse than the base model. Any reason reasons why this might happen? ,NEGATIVE,0.9996119141578674,0,15,https://www.reddit.com/r/MachineLearning/comments/1g39y0b/performance_drops_after_fine_tuning_llama_3_d/
[R] Llama-3.2-3B-Instruct-uncensored,"[R] Llama-3.2-3B-Instruct-uncensored This is an uncensored version of the original¬†[Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct), created using¬†[mlabonne](https://huggingface.co/mlabonne)'s¬†[script](https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing), which builds on¬†[FailSpy's notebook](https://huggingface.co/failspy/llama-3-70B-Instruct-abliterated/blob/main/ortho_cookbook.ipynb)¬†and the original work from¬†[Andy Arditi et al.](https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw?usp=sharing). The method is discussed in details in this¬†[blog](https://huggingface.co/blog/mlabonne/abliteration)¬†and this¬†[paper](https://arxiv.org/abs/2406.11717).

You can find the uncensored model¬†[here](https://huggingface.co/chuanli11/Llama-3.2-3B-Instruct-uncensored) and play with it in this ü§ó¬†[space](https://huggingface.co/spaces/chuanli11/Chat-Llama-3.2-3B-Instruct-uncensored).",NEGATIVE,0.9986786246299744,58,9,https://www.reddit.com/r/MachineLearning/comments/1fqqzuh/r_llama323binstructuncensored/
[Research] Which is more effective for NER? Training a spaCy model or using Meta‚Äôs LLaMA?,"[Research] Which is more effective for NER? Training a spaCy model or using Meta‚Äôs LLaMA? **Hey everyone,**

I‚Äôm working on a Named Entity Recognition (NER) pipeline for a project involving YouTube comments about German political discourse. My goal is to extract and standardize entities like politicians, political parties, and institutions.

A key part of this is transforming **politicians' names into their abbreviations** (e.g., *Ricarda Lang ‚Üí RL, Friedrich Merz ‚Üí FM*), ensuring consistency for downstream text analysis.

Right now, I‚Äôm debating between two approaches:

1. **Training a spaCy model for NER**
   * I‚Äôd fine-tune a **pretrained spaCy transformer model** on \~1,000 manually annotated examples.
   * My dataset includes noisy, informal text (e.g., abbreviations, misspellings, and sarcasm).
   * I already have a structured **gazetteer** with entity mappings to ensure proper abbreviation.
2. **Using Meta‚Äôs LLaMA API for zero-shot or few-shot NER**
   * LLaMA isn‚Äôt specifically trained for NER, but I could use structured **prompting** to extract named entities and return them in an **abbreviated format**.
   * Example prompt: *""Extract all named entities (people, political parties, and institutions) from this text and return them as JSON in their standardized abbreviation format.""*
   * This avoids training overhead, but I worry about **consistency and accuracy**.

**Main questions:**

* Has anyone successfully used **LLaMA** (or other LLM APIs) for **NER**? How reliable is it compared to a fine-tuned model?
* Would **fine-tuning spaCy** (even with a small dataset) be more robust for messy social media data?
* Any other recommendations for handling **political entity recognition and transformation in German**?

I‚Äôd love to hear your thoughts, especially if you‚Äôve tried similar approaches! Thanks in advance! üöÄ",NEGATIVE,0.8858128786087036,3,2,https://www.reddit.com/r/MachineLearning/comments/1icoi78/research_which_is_more_effective_for_ner_training/
Chinese researchers build military AI using Meta‚Äôs open-source Llama model ‚Äî ChatBIT allegedly performs at around 90% of the performance of OpenAI GPT-4 LLM,Chinese researchers build military AI using Meta‚Äôs open-source Llama model ‚Äî ChatBIT allegedly performs at around 90% of the performance of OpenAI GPT-4 LLM,NEGATIVE,0.997680127620697,25,9,https://www.tomshardware.com/tech-industry/artificial-intelligence/chinese-researchers-build-military-ai-using-metas-open-source-llama-model-chatbit-allegedly-performs-at-around-90-percent-of-the-performance-of-openai-gpt-4-llm
[D] Llama 3 Monstrosities,"[D] Llama 3 Monstrosities I just noticed some guy created a 120B Instruct variant of Llama 3 by merging it with itself (end result duplication of 60 / 80 layers). He seems to specialize in these Frankenstein models. For the life of me, I really don't understand this trend. These are easy breezy to create with mergekit, and I wonder about their commercial utility in the wild. Bud even concedes its not better than say, GPT-4. So what's the point? Oh wait, he gets to the end of his post and mentions he submitted it to Open LLM Leaderboard... there we go. The gamification of LLM leaderboard climbing is tiring.",NEGATIVE,0.97453373670578,49,23,https://www.reddit.com/r/MachineLearning/comments/1cljvpa/d_llama_3_monstrosities/
[D] ROPE frequency calculation for llama ,"[D] ROPE frequency calculation for llama  This is based on the code for ROPE frequency computation, used by the transformers library as found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_rope_utils.py#L310). The function `_compute_llama3_parameters` takes the default rope frequency values and does some transformers on the top. What I understood is that the low and high frequency values are scaled differently and the intermediate values are adjusted to have a smooth transition.

I am looking for a source (paper or even a blog post) where I can find more information on this particular approach used by llama type models. Is the calculation correct? What is the motivation (like low frequencies scaled down to allow a longer periodicity)?

  
Code for reference (copied from the link above):

        inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len, **rope_kwargs)
    
        factor = config.rope_scaling[""factor""]  # `8` in the original implementation
        low_freq_factor = config.rope_scaling[""low_freq_factor""]  # `1` in the original implementation
        high_freq_factor = config.rope_scaling[""high_freq_factor""]  # `4` in the original implementation
        old_context_len = config.rope_scaling[""original_max_position_embeddings""]  # `8192` in the original implementation
    
        low_freq_wavelen = old_context_len / low_freq_factor
        high_freq_wavelen = old_context_len / high_freq_factor
    
        wavelen = 2 * math.pi / inv_freq
        # wavelen < high_freq_wavelen: do nothing
        # wavelen > low_freq_wavelen: divide by factor
        inv_freq_llama = torch.where(wavelen > low_freq_wavelen, inv_freq / factor, inv_freq)
        # otherwise: interpolate between the two, using a smooth factor
        smooth_factor = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)
        smoothed_inv_freq = (1 - smooth_factor) * inv_freq_llama / factor + smooth_factor * inv_freq_llama
        is_medium_freq = ~(wavelen < high_freq_wavelen) * ~(wavelen > low_freq_wavelen)
        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)
    
        return inv_freq_llama, attention_factor",NEGATIVE,0.9975513815879822,2,5,https://www.reddit.com/r/MachineLearning/comments/1hovvmm/d_rope_frequency_calculation_for_llama/
That's getting interesting - LLaMA,That's getting interesting - LLaMA,POSITIVE,0.9994271993637085,199,32,https://i.redd.it/riesfstch8ka1.jpg
Meta Being the Good Guys in AI? ü§® About Llama 3 Open Source Free LLM,"Meta Being the Good Guys in AI? ü§® About Llama 3 Open Source Free LLM Hey Reddit, 

I'm reading up on some of the latest tech developments -- notably, Llama 3 is impressive - and it's completely free. 

Now, they seem to have announced that they'll spend $40 billion on NVIDIA H100 GPUs to scale their data centers to the next level in order to train Llama 3 and other models.

I know they want to reach AGI. 

But why are they releasing the Llama models for free to the community? Why not follow a more closed-source approach? How do they plan to monetize vs other players? What is Zuck's secret plan?

If you spend $40B on hardware, you should see some kind of ROI very soon as a good capital allocator. 

Speculation allowed!",NEGATIVE,0.9640722870826721,0,29,https://www.reddit.com/r/artificial/comments/1cn1uja/meta_being_the_good_guys_in_ai_about_llama_3_open/
"[D] Llama-3 based OpenBioLLM-70B & 8B: Outperforms GPT-4, Gemini, Meditron-70B, Med-PaLM-1 & Med-PaLM-2 in Medical-domain","[D] Llama-3 based OpenBioLLM-70B & 8B: Outperforms GPT-4, Gemini, Meditron-70B, Med-PaLM-1 & Med-PaLM-2 in Medical-domain **Open Source Strikes Again**, We are thrilled to announce the release of OpenBioLLM-Llama3-70B & 8B. These models outperform industry giants like **Openai‚Äôs GPT-4, Google‚Äôs Gemini, Meditron-70B, Google‚Äôs Med-PaLM-1, and Med-PaLM-2** in the biomedical domain, setting a new state-of-the-art for models of their size. **The most capable openly available Medical-domain LLMs to date!** ü©∫üíäüß¨



https://preview.redd.it/2h4ebhftf0xc1.png?width=2514&format=png&auto=webp&s=bbc3a583d45fb37b87a6fbbabe2d9e0f23c75d8b

üî• OpenBioLLM-70B delivers SOTA performance, while the OpenBioLLM-8B model even surpasses GPT-3.5 and Meditron-70B!

The models underwent a rigorous two-phase fine-tuning process using the LLama-3 70B & 8B models as the base and leveraging Direct Preference Optimization (DPO) for optimal performance. üß†



https://preview.redd.it/w41pv7mwf0xc1.png?width=5760&format=png&auto=webp&s=f3143919ef8472961f329bb8eb98937d8f8e41e0

**Results are available at Open Medical-L**LM Leaderboard:¬†[https://huggingface.co/spaces/openlifescienceai/open\_medical\_llm\_leaderboard](https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard)

Over \~4 months, we meticulously curated a diverse custom dataset, collaborating with medical experts to ensure the highest quality. The dataset spans 3k healthcare topics and 10+ medical subjects. üìö¬†OpenBioLLM-70B's remarkable performance is evident across 9 diverse biomedical datasets, achieving an impressive average score of 86.06% despite its smaller parameter count compared to GPT-4 & Med-PaLM. üìà



https://preview.redd.it/5ff2k9szf0xc1.png?width=5040&format=png&auto=webp&s=15dc4aa948f2608717f68ddf2cb27a6a2de03496

You can download the models directly from Huggingface today.

- 70B : [https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B](https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B)  
- 8B : [https://huggingface.co/aaditya/OpenBioLLM-Llama3-8B](https://huggingface.co/aaditya/OpenBioLLM-Llama3-8B)

This release is just the beginning! In the coming months, we'll introduce

* Expanded medical domain coverage,
* Longer context windows,
* Better benchmarks, and
* Multimodal capabilities.

More details can be found here:¬†[https://twitter.com/aadityaura/status/1783662626901528803](https://twitter.com/aadityaura/status/1783662626901528803) Over the next few months, Multimodal will be made available for various medical and legal benchmarks.

I hope it's useful in your research üî¨ Have a wonderful weekend, everyone! üòä",POSITIVE,0.9974964261054993,147,11,https://www.reddit.com/r/MachineLearning/comments/1cecpvk/d_llama3_based_openbiollm70b_8b_outperforms_gpt4/
"Open-source AI must reveal its training data, per new OSI definition | Meta‚Äôs Llama does not fit OSI‚Äôs new definition","Open-source AI must reveal its training data, per new OSI definition | Meta‚Äôs Llama does not fit OSI‚Äôs new definition",NEGATIVE,0.999658465385437,70,3,https://www.theverge.com/2024/10/28/24281820/open-source-initiative-definition-artificial-intelligence-meta-llama
[D] Discussion: Llama model weights torrent files,"[D] Discussion: Llama model weights torrent files Back in 2023, one of Metas large language models, one of the Llama variants, were leaked over 4chan via torrent files.
Does anyone know where I could find these files?
I have tried requesting access directly from meta but they are not responding to me hence in the spirit of open source I would like to download the torrent files instead.
If anyone could point me in the right direction I would greatly appreciate it.",NEGATIVE,0.9982763528823853,0,1,https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/
[D] New Llama scaling laws?,"[D] New Llama scaling laws? ""We made several new observations on scaling behavior during the development of Llama 3. For example, while the Chinchilla-optimal amount of training compute for an 8B parameter model corresponds to \~200B tokens, we found that model performance continues to improve even after the model is trained on two orders of magnitude more data. Both our 8B and 70B parameter models continued to improve log-linearly after we trained them on up to 15T tokens. Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.""

That's an excerpt from [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/)

Do you think Llama just introduced new scaling laws? Why didn't Chinchilla find that out before? Is there any new number on the proportion of model size vs amount of tokens?",POSITIVE,0.9685305953025818,51,12,https://www.reddit.com/r/MachineLearning/comments/1eq95ga/d_new_llama_scaling_laws/
Llama 3.1 8B CPU inference on any PC with a browser,"Llama 3.1 8B CPU inference on any PC with a browser In May of this year, a team at Yandex Research, in collaboration with ISTA and KAUST, published a new SOTA quantization method called PV-tuning.

This project from one of the authors runs models like Llama 3.1 8B inside any modern browser using PV-tuning compression.

[Demo](https://galqiwi.github.io/aqlm-rs/about.html)

[Code](https://github.com/galqiwi/demo-aqlm-rs)",NEGATIVE,0.9962363839149475,15,2,https://www.reddit.com/r/artificial/comments/1ho37cf/llama_31_8b_cpu_inference_on_any_pc_with_a_browser/
"[N] Llama 3.1 70B, Llama 3.1 70B Instruct compressed by 6.4 times","[N] Llama 3.1 70B, Llama 3.1 70B Instruct compressed by 6.4 times Our latest work with the Llama 3.1 70B and Llama 3.1 70B Instruct models achieved a compression ratio of 6.4 times, with most of the MMLU quality preserved. If you have a 3090 GPU, you can run the compressed models at home right now.  
  
Here are the results and the compressed models:  
[https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-AQLM-PV-2Bit-1x16](https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-AQLM-PV-2Bit-1x16)   
[https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16/tree/main](https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16/tree/main)",NEGATIVE,0.9956693649291992,102,3,https://www.reddit.com/r/MachineLearning/comments/1fivdkg/n_llama_31_70b_llama_31_70b_instruct_compressed/
Where to find Llama 3 initialisation [R] ,"Where to find Llama 3 initialisation [R]  Title basically says it all, I want a good transformer baseline and I imagine that the initialisation can matter quite a bit. I can find the llama 3 model, but I can find how they init parameters. Does anyone know where I can find this?",NEGATIVE,0.9989089965820312,0,5,https://www.reddit.com/r/MachineLearning/comments/1hbudg3/where_to_find_llama_3_initialisation_r/
Meta does everything OpenAI should be [D],"Meta does everything OpenAI should be [D]  I'm surprised (or maybe not) to say this, but Meta (or Facebook) democratises AI/ML much more than OpenAI, which was originally founded and primarily funded for this purpose. OpenAI has largely become a commercial project for profit only. Although as far as Llama models go, they don't yet reach GPT4 capabilities for me, but I believe it's only a matter of time. What do you guys think about this?",NEGATIVE,0.9984934329986572,990,256,https://www.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/
"[D] Local LLaMA based LLM for Technical Document Search | Help!
","[D] Local LLaMA based LLM for Technical Document Search | Help!
 I wanted to make an LLM that could search through around 60k technical documents (about 50000 characters each) and could retrieve information from them semantically. The final model I envisioned would know those technical documents and I could just prompt the model to find me something similar to the information it already knew or something exact.

* My initial approach was to fine tune the LLM with these documents and then query it. But after researching I got to know that it is very resource heavy and the model often hallucinates a lot.
* I came across RAG and Sematic RAG recently and I'm currently reading about it. Could it work for my use case? Or anything else that you can suggest? One Issue in my mind for RAG was that let's say I ask the model something vague and the vector database returned top k Nearest Neighbor Vectors and I pass that onto my LLM with the Original Prompt. What if the information was not completely there in the Top K Nearest Neighbors or if the Context Window for the LLM is not big enough?
* Another issue was that with RAG wouldn't inference with the LLM become a lot more resource heavy due to a large input token count.

Could you guys comment on anything in it?

PS: I know this is a large question. I'm a bit new to ML and NLP and learning about it. Also sorry about my English, I'm not a native speaker.",NEGATIVE,0.9950448274612427,8,8,https://www.reddit.com/r/MachineLearning/comments/1gftg6l/d_local_llama_based_llm_for_technical_document/
"[N] Upstage AI's 30M Llama 1 Outshines 70B Llama2, Dominates #1 Spot in OpenLLM Leaderboard!","[N] Upstage AI's 30M Llama 1 Outshines 70B Llama2, Dominates #1 Spot in OpenLLM Leaderboard! # Title Fix: Upstage AI's 30B Llama 1 Outshines 70B Llama2, Dominates #1 Spot in OpenLLM Leaderboard!

We are thrilled to share an extraordinary achievement with you today. Our team at Upstage AI has reached a significant milestone. Our fine-tuned 30B model, Llama 1, has ascended to the coveted #1 position on the prestigious global OpenLLM Leaderboard. In a thrilling turn of events, our fine-tuned 30B Llama 1 has outperformed the 70B model of Llama2.

Please check out the leaderboard and download/use our model at [https://huggingface.co/spaces/HuggingFaceH4/open\_llm\_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

Once again, we are happy to bring this news to all of you. Stay tuned for more exciting updates from Upstage AI!

https://preview.redd.it/m7xzlzrpyxcb1.png?width=2310&format=png&auto=webp&s=23429478474d23071837fe9c2e85e6ddea10039c",POSITIVE,0.9996851682662964,63,37,https://www.reddit.com/r/MachineLearning/comments/153yfry/n_upstage_ais_30m_llama_1_outshines_70b_llama2/
[D] Tutorial: Run LLaMA on 8gb vram on windows (thanks to bitsandbytes 8bit quantization),"[D] Tutorial: Run LLaMA on 8gb vram on windows (thanks to bitsandbytes 8bit quantization) ~~facebookresearch/LLaMA-7b-8bit using less than 10GB vram, or LLaMA-13b on less than 24GB~~.

facebookresearch/LLaMA-7b-4bit using less than 6GB vram, or LLaMA-13b-4bit on less than 10GB.


**Udpate:**

Developments are fast, the guide below is already outdated. You can now get LLaMA 4bit models, which are smaller than original model weights, and better than 8bit models and need even less vram. Follow the new guide for Windows and Linux: 

https://github.com/underlines/awesome-marketing-datascience/blob/master/llama.md

---

Efforts are being made to  get the larger LLaMA 30b onto <24GB vram with 4bit quantization by implementing the technique from the paper [GPTQ quantization](https://github.com/oobabooga/text-generation-webui/issues/177) 

Since bitsandbytes doesn't officially have windows binaries, the following trick using an older unofficially compiled cuda compatible bitsandbytes binary works for windows.

1. install miniconda, start the miniconda console
1. create a new dir, for example *C:\textgen\* and cd into it
1. git clone *github.com/oobabooga/text-generation-webui*
1. follow the installation instructions of text-generation-webui for conda, create the env with the name textgen
1. Download not the original LLaMA weights, but the [HuggingFace converted](https://rentry.org/llama-tard-v2) weights. The torrent link is on top of this linked article.
1. copy the llama-7b or -13b folder (or whatever size you want to run) into *C:\textgen\text-generation-webui\models*. The folder should contain the config.json, generation_config.json, pytorch_model.bin, index.json, special_tokens_map.json, tokenizer.model, tokenizer_config.json as well as all the 33 pytorch_model-000xx-of-00033.bin files
1. put [libbitsandbytes_cuda116.dll](https://github.com/DeXtmL/bitsandbytes-win-prebuilt) in *C:\Users\xxx\miniconda3\envs\textgen\lib\site-packages\bitsandbytes\*
1. edit *\bitsandbytes\cuda_setup\main.py*:
  
  search for:
  
  *if not torch.cuda.is_available(): return 'libsbitsandbytes_cpu.so', None, None, None, None*
  
  replace with:
  
  *if torch.cuda.is_available(): return 'libbitsandbytes_cuda116.dll', None, None, None, None*

  search for this twice:
  
  *self.lib = ct.cdll.LoadLibrary(binary_path)*
  
  replace with:
  
  *self.lib = ct.cdll.LoadLibrary(str(binary_path))*

1. Start text-generation-webui by typing: *python server.py --model LLaMA-7B --load-in-8bit*",NEGATIVE,0.998465895652771,118,39,https://www.reddit.com/r/MachineLearning/comments/11kwdu9/d_tutorial_run_llama_on_8gb_vram_on_windows/
"[D] Is there limited quantization in all LLM models? For example you can take a standard model like meta-llama/Llama-3.2-1B and run it at half, but there are also models specifically made for 4bit quantization (i.e. meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8)","[D] Is there limited quantization in all LLM models? For example you can take a standard model like meta-llama/Llama-3.2-1B and run it at half, but there are also models specifically made for 4bit quantization (i.e. meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8) I'm just trying to understand how quantization is setup in all the models.

Standard models like meta-llama/Llama-3.2-1B can be run without quantization (bfloat16 or float32?), or they can be told to run at half (float16?) with an inferencing app (like vLLM). So does that mean there is some quantization build into all models? Instead of telling it to run at half quantization, can I instead say int8? Or does that only work if the model was built for it?

And then there are models that are specifically built for int4 (i.e. meta-llama/Llama-3.2-1B-Instruct-SpinQuant\_INT4\_EO8). Does that mean that when you run this model with vLLM, you have to explicitly say you are running it at int4, or you just leave that at default and it will automatically run at int4? Can that be overridden to int8? Or is it just hardcoded for int4?

Just been trying to wrap my head around this for the past 2 days.",NEGATIVE,0.9983822107315063,4,7,https://www.reddit.com/r/MachineLearning/comments/1gjhjza/d_is_there_limited_quantization_in_all_llm_models/
Exclusive: Chinese researchers develop AI model for military use on back of Meta's Llama,Exclusive: Chinese researchers develop AI model for military use on back of Meta's Llama,POSITIVE,0.9479820728302002,16,5,https://www.reuters.com/technology/artificial-intelligence/chinese-researchers-develop-ai-model-military-use-back-metas-llama-2024-11-01/
Police taking 'excessive' data from mobile phones,Police taking 'excessive' data from mobile phones,NEGATIVE,0.9977978467941284,5738,239,https://www.bbc.co.uk/news/technology-53092175
[D] What are the problems with using Llama in a commercial app?,"[D] What are the problems with using Llama in a commercial app? I searched and saw a thread saying Llama shouldn't be used for commercial purposes, but I can't tell why. I looked at the Meta license for Llama and it says you don't need a license until you have 700M monthly users, a number which there is no way the application I have in mind would ever hit.

What am I missing? If I use Llama in a commercial application with far fewer users (maybe 1M per month at the very highest), is there going to be a problem?
",NEGATIVE,0.9990490078926086,1,16,https://www.reddit.com/r/MachineLearning/comments/1e9lfu3/d_what_are_the_problems_with_using_llama_in_a/
Meta wants its Llama AI in Britain‚Äôs public healthcare system,Meta wants its Llama AI in Britain‚Äôs public healthcare system,NEGATIVE,0.9867938160896301,0,4,https://www.engadget.com/ai/meta-wants-its-llama-ai-in-britains-public-healthcare-system-174119281.html
[P] Fullstack LlamaIndex App to Build and Query Document Collections with LLMs (MIT Licensed),"[P] Fullstack LlamaIndex App to Build and Query Document Collections with LLMs (MIT Licensed) Wanted to share an MIT-licensed, open source starter project called [Delphic](https://github.com/JSv4/Delphic) I released to help people build apps to LlamaIndex to search through documents and use LLMs to interact with the text. Here's a super quick demo of uploading a word doc and then asking some questions:

https://reddit.com/link/12tn34b/video/cr9ts2wcb5va1/player

The backend and frontend communicate with websockets for low-latency, and there's a redis-backed asynchronous task queue to ensure that you can process multiple document collections simultaneously while remaining responsive to users. Thought it might be helpful to have a more production-grade starter project out there for people to start playing around with using LLMs on their own document collections without needing to use the command line. 

If you're curious about the architecture, there's a [full walkthrough](https://medium.com/@scrudato/introducing-delphic-a-production-grade-starter-app-to-use-llms-to-query-your-own-documents-5c2462357b84) up on Medium.",NEGATIVE,0.9981880784034729,85,38,https://www.reddit.com/r/MachineLearning/comments/12tn34b/p_fullstack_llamaindex_app_to_build_and_query/
Zoom's CEO says an AI clone will one day do a ton of your job while you enjoy life,Zoom's CEO says an AI clone will one day do a ton of your job while you enjoy life,POSITIVE,0.9976327419281006,553,397,https://qz.com/zoom-ceo-eric-yuan-ai-avatar-jobs-1851518757
[Discussion] Beat GPT-4o at Python by searching with 100 dumb LLaMAs,"[Discussion] Beat GPT-4o at Python by searching with 100 dumb LLaMAs > One thing that should be learned from the bitter lesson is the great
> power of general purpose methods, of methods that continue to scale
> with increased computation even as the available computation becomes
> very great. The two methods that seem to scale arbitrarily in this way
> are _search_ and _learning_.
>
> Richard Sutton, _The Bitter Lesson_

The eponymously distasteful take-away of Richard Sutton‚Äôs [essay](http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=blog.heim.xyz) has often been misconstrued: because scale is all you need, they say, smaller models are doomed to irrelevance. The rapid increase in model size above one trillion parameters and the technological limitations of GPU memory together seemed to foreclose on economical frontier intelligence anywhere except at an oligopoly of intelligence-as-a-service providers. Open models and self-serve inference were in retreat.

But as the quote above indicates, there are in fact two arrows in the scaling quiver: learning and search. Learning, as we do it now with neural networks, scales with _memory_ at inference time ‚Äî larger models perform better, ceteris paribus, because they can extract more data from their training set into more [circuits](https://transformer-circuits.pub/) and more [templates](https://arxiv.org/abs/2305.18654). Search scales smoothly with _compute_ at inference time ‚Äî compute that can be spent on either producing higher quality candidates or on producing more candidates. In the ideal case, the scaling behavior can be predicted via so-called scaling laws.

Recent papers indicate that generative models like LLMs can be scaled up with search. The [Large Language Monkeys](https://arxiv.org/abs/2407.21787) paper, published on arXiv by Brown, Juravsky, and co-authors last week, includes several results in this vein and indicates that frontier-level intelligence in certain domains can be elicited from smaller models that can run on a single, past-generation GPU.
Further, they observed smooth, predictable improvement of performance with scale.

Put more simply: where before, it seemed frontier capabilities required [one horse-sized duck](https://knowyourmeme.com/memes/horse-sized-duck), it is clear we can now alternatively get them with one hundred duck-sized horses (or, rather, LLaMAs).

This weekend, we set out to replicate this finding.

### Scaling LLaMA 3.1 8B HumanEval on Modal

Running all of our experiments, including configuration and testing, cost well under $50.

You can find our code [here](https://gist.github.com/charlesfrye/27f25188dbbcfdf20a83c0230020fe05). You can run it yourself without exceeding the $30/month in credits included in [Modal‚Äôs free tier](/pricing).

### Metrics and data: HumanEval and pass@k

[**Continued in original post...**](https://modal.com/blog/llama-human-eval)",NEGATIVE,0.9632741212844849,30,9,https://www.reddit.com/r/MachineLearning/comments/1elo2d1/discussion_beat_gpt4o_at_python_by_searching_with/
llama-lite: a proof of concept fast sentence embeddings service based on llama.cpp (~1ms per token on CPU) [P],llama-lite: a proof of concept fast sentence embeddings service based on llama.cpp (~1ms per token on CPU) [P],NEGATIVE,0.8813703656196594,101,34,https://github.com/skeskinen/llama-lite
"Meta launches open source Llama 3.3, shrinking powerful bigger model into smaller size","Meta launches open source Llama 3.3, shrinking powerful bigger model into smaller size",NEGATIVE,0.9858992099761963,1,0,https://venturebeat.com/ai/meta-launches-open-source-llama-3-3-shrinking-powerful-bigger-model-into-smaller-size/
[D] Llama 3 vs llama 3.1 in Medical Domain: Llama 3 Outperforms 3.1 in Base Category,"[D] Llama 3 vs llama 3.1 in Medical Domain: Llama 3 Outperforms 3.1 in Base Category Just analyzed Llama 3 and 3.1 models in medical tasks. Here are the key findings:

1. ü•á Meta-Llama-3.1-70B-Instruct: Overall champion ü•à Meta-Llama-3-70B-Instruct: Close runner-up
2. But here's the shocker: In base models (both 70B and 8B), Llama 3 often outperforms Llama 3.1! ü§Ø
3. Llama 3.1 70B instruct beats GPT-4 in a few tasks and is almost equal to GPT-4 in Open Medical-LLM Leaderboard

**70B Models:**

Instruct:

* Llama 3.1 generally outperforms Llama 3
* 3.1 excels in college biology, 3 in college medicine
* Both strong in medical genetics

Base:

* Surprisingly, Llama 3 outperforms 3.1 overall
* 3 dominates in college anatomy
* 3.1 superior in medical genetics and professional medicine

**8B Models:**

Instruct:

* Llama 3.1 leads in most categories
* 3.1 shines in college biology
* 3 maintains edge in medical genetics

Base:

* Llama 3 slightly outperforms 3.1 overall
* 3 better in anatomy
* 3.1 excels in medical genetics and PubMedQA

  
for a detailed comparison check out   
[https://x.com/aadityaura/status/1815836602607768041](https://x.com/aadityaura/status/1815836602607768041)

For the latest on AI models, datasets, and research in life sciences, check out **Open Life Science AI**.    
Don't miss any Model or dataset in the **AI x Healthcare domain**  [https://x.com/openlifesciai](https://x.com/openlifesciai)

https://preview.redd.it/5nzv5cd4zbed1.jpg?width=2168&format=pjpg&auto=webp&s=52a9abc2fb152393c9378e216e77a29df8e45ec4",NEGATIVE,0.9263173937797546,26,8,https://www.reddit.com/r/MachineLearning/comments/1eainqi/d_llama_3_vs_llama_31_in_medical_domain_llama_3/
[R] Llama 3.2 Interpretability with Sparse Autoencoders,[R] Llama 3.2 Interpretability with Sparse Autoencoders,NEGATIVE,0.9883204698562622,3,0,https://github.com/PaulPauls/llama3_interpretability_sae
"[P] New LLM Pre-training and Post-training Paradigms: Comparing Qwen 2, Llama 3.1, Gemma 2, and Apple's FMs","[P] New LLM Pre-training and Post-training Paradigms: Comparing Qwen 2, Llama 3.1, Gemma 2, and Apple's FMs",NEGATIVE,0.6555680632591248,27,6,https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training
I made a python program that gives LLMs running locally the power to search the internet for LLMs running via Llama.cpp!,I made a python program that gives LLMs running locally the power to search the internet for LLMs running via Llama.cpp!,NEGATIVE,0.9684258103370667,12,5,https://github.com/TheBlewish/Web-LLM-Assistant-Llama-cpp
"[P] Data science & ML on sensitive data with local code interpreter, with GPT-4 or Llama 2 (open-source project, link in comments)","[P] Data science & ML on sensitive data with local code interpreter, with GPT-4 or Llama 2 (open-source project, link in comments)",NEGATIVE,0.9466560482978821,77,26,https://v.redd.it/bflfx1jcv1jb1
SimCity Developers' Reddit AMA Swiftly Turns Into WTF With The Online-Only DRM?,SimCity Developers' Reddit AMA Swiftly Turns Into WTF With The Online-Only DRM?,NEGATIVE,0.9983876943588257,2721,1557,http://www.techdirt.com/articles/20121214/16262621391/simcity-developers-reddit-ama-swiftly-turns-into-wtf-with-online-only-drm.shtml
"[P] Hey all! I'm excited to launch GPTCall, a platform that enables real-time voice conversations with Llama 2 and other open-source models! It supports both desktop and mobile browsers. See comments for details.","[P] Hey all! I'm excited to launch GPTCall, a platform that enables real-time voice conversations with Llama 2 and other open-source models! It supports both desktop and mobile browsers. See comments for details.",POSITIVE,0.9992813467979431,107,20,https://gptcall.net/
[D] Creating a DPO Dataset using Llama: Best Practices?,"[D] Creating a DPO Dataset using Llama: Best Practices? I was to create a synthetic DPO dataset to fine-tune Llama-3-8b. Can I just use Llama-3-70 responses as ""accepted"" and Llama-3-8b responses as ""rejected""? Or the better approach is to sample two responses from Llama-3-8b and pick one as accepted and the other as rejected.",NEGATIVE,0.9989151954650879,8,10,https://www.reddit.com/r/MachineLearning/comments/1dxh210/d_creating_a_dpo_dataset_using_llama_best/
Meta‚Äôs Next Llama AI Models Are Training on a GPU Cluster ‚ÄòBigger Than Anything‚Äô Else,Meta‚Äôs Next Llama AI Models Are Training on a GPU Cluster ‚ÄòBigger Than Anything‚Äô Else,NEGATIVE,0.9629343152046204,0,1,https://www.wired.com/story/meta-llama-ai-gpu-training/
"Llama 3.1 70B models compressed by 6.4x using state-of-the-art algorithm, now released 
","Llama 3.1 70B models compressed by 6.4x using state-of-the-art algorithm, now released 
",NEGATIVE,0.9968462586402893,14,3,https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16/tree/main
"[R] CodeCapybara: Another open source model for code generation based on instruction tuning, outperformed Llama and CodeAlpaca","[R] CodeCapybara: Another open source model for code generation based on instruction tuning, outperformed Llama and CodeAlpaca We are the first that attempt to reproduce results of Llama on code generation benchmark, such as HumanEval and MBPP.

We also try to evaluate existing trending models, such as CodeAlpaca, on such benchmarks.

All of the source code and scripts for evaluation will be made available for the research community.

Our code can be accessed here: [https://github.com/FSoft-AI4Code/CodeCapybara](https://github.com/FSoft-AI4Code/CodeCapybara)

Model weights will be released very soon.

&#x200B;",NEGATIVE,0.9837146401405334,138,22,https://www.reddit.com/r/MachineLearning/comments/12xgzbx/r_codecapybara_another_open_source_model_for_code/
[P] Run Llama 2 locally on GPU or CPU from anywhere (Linux/Windows/Mac) ‚û°Ô∏èhttps://github.com/liltom-eth/llama2-webui,"[P] Run Llama 2 locally on GPU or CPU from anywhere (Linux/Windows/Mac) ‚û°Ô∏èhttps://github.com/liltom-eth/llama2-webui  Running Llama 2 locally with gradio UI on GPU or CPU from anywhere (Linux/Windows/Mac). Supporting Llama-2-7B/13B/70B with 8-bit, 4-bit. Supporting GPU inference (6 GB VRAM) and CPU inference. ‚û°Ô∏è[https://github.com/liltom-eth/llama2-webui](https://github.com/liltom-eth/llama2-webui)

Successfully running #Llama2 on my Apple Silicon MacBook Air:

[demo](https://twitter.com/liltom_eth/status/1682791729207070720?s=20)",NEGATIVE,0.9796323180198669,106,21,https://www.reddit.com/r/MachineLearning/comments/157nj6u/p_run_llama_2_locally_on_gpu_or_cpu_from_anywhere/
[P] Using ChatGPT plugins with LLaMA,[P] Using ChatGPT plugins with LLaMA,NEGATIVE,0.9811074733734131,132,24,https://blog.lastmileai.dev/using-openais-retrieval-plugin-with-llama-d2e0b6732f14
"[D] Llama-3.2 vs llama-3.1 in Medical Domain: Llama-3.1 70B Outperforms Llama-3.2 90B
","[D] Llama-3.2 vs llama-3.1 in Medical Domain: Llama-3.1 70B Outperforms Llama-3.2 90B
 [Large LLama Models in Medical Domain \(90B, 70B, 11B\)](https://preview.redd.it/v9di045iq3rd1.png?width=2084&format=png&auto=webp&s=e7bd3de9dcc4f7025f0cd5e9ad2a6c2e88ced26e)

Exploring how LLama 3.2 Large models and Llama 3.1 models perform in the medical field. (Without Fine-tuning)

ü•á Meta-Llama-3.1-70B-Instruct: Overall champion with 84% average score

* Excels in MMLU College Biology (95.14%)
* Strong performance in MMLU Professional Medicine (91.91%)

ü•à Meta-Llama-3.2-90B-Vision (Instruct and Base): Tied for second place with 83.95% average

* Consistent performance across Instruct and Base versions
* Best in MMLU College Biology (93.06%) and MMLU Professional Medicine (91.18%)

ü•â Meta-Llama-3-70B-Instruct: Third place with 82.24% average

* Strongest in MMLU Medical Genetics (93%)
* Solid performance in MMLU College Biology (90.28%)

[Small LLama & Phi Models in Medical Domain \(3B, 1B\)](https://preview.redd.it/r7r2n03jz3rd1.png?width=2078&format=png&auto=webp&s=32de7cdf48f21f5997972ec3b279ba1b698822df)

I also analyzed Smaller models and compared them with phi-3 to explore how small models perform in the medical field. (Without Fine-tuning)

ü•á Phi-3-4k: Top performer with 68.93% average score

* Excels in MMLU College Biology (84.72%)
* Strong performance in MMLU Clinical Knowledge (75.85%)

ü•à Meta-Llama-3.2-3B-Instruct: Second place with 64.15% average

* Best in MMLU College Biology (70.83%)
* Solid performance in PubMedQA (70.6%)

ü•â Meta-Llama-3.2-3B: Third place with 60.36% average

* Strongest in MMLU College Biology (63.89%)
* Good performance in PubMedQA (72.8%)

Additional Observations:

[Eval Results](https://preview.redd.it/zm361cl8h4rd1.png?width=2744&format=png&auto=webp&s=ff617d221d238aa4f7e8b1b59d5de1c10b34389b)

1. **Identical Performance in Vision Models**:
   * Meta-Llama-3.2-90B-Vision Instruct and Base versions show identical performance (83.95% average) across all metrics and all 9 datasets, down to the decimal point.
   * Similarly, Meta-Llama-3.2-11B-Vision Instruct and Base versions also demonstrate identical scores (72.8% average) in all categories. (evaluated twice)
2. Unusual Consistency:
   * This perfect alignment between the Instruct and Base versions is a little atypical, as Instruct and base variants usually show slight performance differences..
   * I am guessing is it due to vision instruct tuning? Could vision models' capabilities be less dependent on specific instruction tuning for medical tasks?
   * Results are in JSON, available [here on Github](https://github.com/monk1337/Medical_LLM_Evals/blob/main/llama_3_results.json)

Will be evaluating more models soon for the Medical Domain Here - [Source Post](https://x.com/aadityaura/status/1839233111927750830)",POSITIVE,0.9784324169158936,20,1,https://www.reddit.com/r/MachineLearning/comments/1fps53b/d_llama32_vs_llama31_in_medical_domain_llama31/
[P] How to Fine-tune Llama 3.1 on Lightning.ai with Torchtune,[P] How to Fine-tune Llama 3.1 on Lightning.ai with Torchtune,POSITIVE,0.798175573348999,1,2,https://zackproser.com/blog/how-to-fine-tune-llama-3-1-on-lightning-ai-with-torchtune
"[P] Dive into Transformers and LLM World ‚Äì Llama 3.1 in Go, Step by Step","[P] Dive into Transformers and LLM World ‚Äì Llama 3.1 in Go, Step by Step I‚Äôm so excited to show the updated version of my latest open-source project here: Llama Nuts and Bolts. The previous version was built for Llama 2 and was now updated to support Llama 3.1 8B-Instruct model.

Code and documentation:¬†[https://github.com/adalkiran/llama-nuts-and-bolts](https://github.com/adalkiran/llama-nuts-and-bolts)

And now, the documentation is also available on Github Pages:¬†[https://adalkiran.github.io/llama-nuts-and-bolts](https://adalkiran.github.io/llama-nuts-and-bolts)

If you are curious like me about how the LLMs (Large Language Models) and transformers work and have delved into conceptual explanations and schematic drawings in the sources but hunger for deeper understanding, then this project is perfect for you too!

You will not only find the details of the Llama architecture but will find explanations of a wide variety of related concepts in the documentation directory. From reading a Pickle, a PyTorch model, a Tiktoken tokenizer model files at byte-by-byte level, to the internals of BFloat16 data type, implementation from scratch of a Tensor structure and mathematical operations including linear algebraic computations.

This project was initially started to learn what an LLM does behind by running and debugging it and was made for experimental and educational purposes only, not for production use.

The goal is to make an experimental project that can perform inference on the Llama 3.1 8B-Instruct model completely outside of the Python ecosystem (using the Go language). Throughout this journey, the aim is to acquire knowledge and shed light on the abstracted internal layers of this technology.

This journey is an intentional journey of literally reinventing the wheel. While reading my journey in the documentation, you will see the details of how Large Language Models work, through the example of the Llama model.

I will be happy if you check out it and comments are welcome!",POSITIVE,0.9738513231277466,24,3,https://www.reddit.com/r/MachineLearning/comments/1ew2l3i/p_dive_into_transformers_and_llm_world_llama_31/
[P] Llama model easy & fast installer,[P] Llama model easy & fast installer,NEGATIVE,0.6878379583358765,10,39,https://github.com/Highlyhotgames/fast_txtgen
"[D] Experimenting with Llama-3 codebase and Google NotebookLM ‚Äì Mind-Blowing Results!
","[D] Experimenting with Llama-3 codebase and Google NotebookLM ‚Äì Mind-Blowing Results!
 Inspired by karpathy's recent tweet about the NotebookLM project, I provided the codebase of the Llama-3 architecture to NLM and used Rag, along with SERP APIs, to find the perfect images and sync them with the generated audio (few images I added myself)

The result exceeded my expectations. Google's NotebookLM is truly amazing! :)



[LLAMA-3 Explained by Google NotebookLM](https://preview.redd.it/5wgwp0o2jwrd1.jpg?width=1920&format=pjpg&auto=webp&s=01beb36fd27e642b4b819a71c3804068693c5dc5)

Here is the Youtube link as well : [https://www.youtube.com/watch?v=4Ns6aFYLWEQ](https://www.youtube.com/watch?v=4Ns6aFYLWEQ)",POSITIVE,0.9987819790840149,4,1,https://www.reddit.com/r/MachineLearning/comments/1fsq2vp/d_experimenting_with_llama3_codebase_and_google/
Meta won‚Äôt release its multimodal Llama AI model in the EU,Meta won‚Äôt release its multimodal Llama AI model in the EU,NEGATIVE,0.997255265712738,4,7,https://www.theverge.com/2024/7/18/24201041/meta-multimodal-llama-ai-model-launch-eu-regulations
[R][P] AI Agents LlamaIndex,"[R][P] AI Agents LlamaIndex AI Agents LlamaIndex Crash Course

It covers:

- Function Calling  
- Function Calling Agents + Agent Runner  
- Agentic RAG  
- REAcT Agent: Build your own Search Assistant Agent

[https://youtu.be/bHn4dLJYIqE](https://youtu.be/bHn4dLJYIqE)",NEGATIVE,0.9989317059516907,0,0,https://www.reddit.com/r/MachineLearning/comments/1fy2y5d/rp_ai_agents_llamaindex/
"[R] Efficient Streaming Language Models with Attention Sinks - Meta AI 2023 - StreamingLLM enables Llama-2, Falcon and Pythia to have an infinite context length without any fine-tuning! Allows streaming use of LLMs!","[R] Efficient Streaming Language Models with Attention Sinks - Meta AI 2023 - StreamingLLM enables Llama-2, Falcon and Pythia to have an infinite context length without any fine-tuning! Allows streaming use of LLMs! Paper: [https://arxiv.org/abs/2309.17453](https://arxiv.org/abs/2309.17453)

Github: [https://github.com/mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm)

Abstract:

>Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe  an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \`\`sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an **efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning.** We show that **StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more**. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further  improve streaming deployment. In streaming settings, **StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup**. 

https://preview.redd.it/elatp0917urb1.jpg?width=875&format=pjpg&auto=webp&s=b1652bf5bbd0606e148b935c192093732d55827f

https://preview.redd.it/j947mz817urb1.jpg?width=1502&format=pjpg&auto=webp&s=d8aa6092f3c4cdd8c2b6d19a614f7a28ac37cb59

https://preview.redd.it/vrjlj3917urb1.jpg?width=1504&format=pjpg&auto=webp&s=c5ebbc04bf9c97e55c6ba3201c0e3f17a71f9389

https://preview.redd.it/l3x4x0917urb1.jpg?width=1654&format=pjpg&auto=webp&s=48fa1fe61a0e97b06f4835fe2cba0c6d58c718a0

&#x200B;",NEGATIVE,0.9876531362533569,59,19,https://www.reddit.com/r/MachineLearning/comments/16y5bk2/r_efficient_streaming_language_models_with/
[P] How to use Memory in Chatbot LlamaIndex,"[P] How to use Memory in Chatbot LlamaIndex While building a chatbot using the RAG pipeline, Memory is the most important component in the entire pipeline.

We will integrate Memory in LlamaIndex and enable Hybrid Search Using the Qdrant Vector Store.

Implementation:¬†[https://www.youtube.com/watch?v=T9NWrQ8OFfI](https://www.youtube.com/watch?v=T9NWrQ8OFfI)",NEGATIVE,0.9614133238792419,0,1,https://www.reddit.com/r/MachineLearning/comments/1fn2hye/p_how_to_use_memory_in_chatbot_llamaindex/
[P] llama-3-70b on Groq with code interpreting,[P] llama-3-70b on Groq with code interpreting,NEGATIVE,0.9893612265586853,38,7,https://github.com/e2b-dev/e2b-cookbook/blob/main/examples/llama-3-code-interpreter/llama_3_code_interpreter_groq.ipynb
[D] First glance at LLaMA,"[D] First glance at LLaMA [https://medium.com/@enryu9000/mini-post-first-look-at-llama-4403517d41a1](https://medium.com/@enryu9000/mini-post-first-look-at-llama-4403517d41a1)  


I'm kind of surprised - I expected it to be much better than ChatGPT, but results are all over the place (e.g. it is better for few-shot classification, but worse for SQL generation).  


I wonder what makes ChatGPT so decent; given that OpenAI can afford to serve it, it is probably an order of magnitude smaller than LLaMA, yet it is competitive; can RLHF get the model that far?",NEGATIVE,0.9863157868385315,68,27,https://www.reddit.com/r/MachineLearning/comments/11ibm1j/d_first_glance_at_llama/
[N] Llama based open source model claims to beat ChatGPT 3.5,"[N] Llama based open source model claims to beat ChatGPT 3.5 Link: [https://huggingface.co/openchat/openchat](https://huggingface.co/openchat/openchat)

Not only that, they do it with only 6k conversations, i.e LIMA

However evaluation does not looks very through, so call me a skeptic",NEGATIVE,0.9975548386573792,73,19,https://www.reddit.com/r/MachineLearning/comments/14o4tgn/n_llama_based_open_source_model_claims_to_beat/
[D] Llama-3 (7B and 70B) on a medical domain benchmark,"[D] Llama-3 (7B and 70B) on a medical domain benchmark Llama-3 is making waves in the AI community. I was curious how it will perform in the medical domain, Here are the evaluation results for Llama-3 (7B and 70B) on a medical domain benchmark consisting of 9 diverse datasets



https://preview.redd.it/sdwx5tglxbvc1.png?width=1464&format=png&auto=webp&s=d32585a69244d44c83e2b1e8a85301a7a8676ea2

I'll be fine-tuning, evaluating & releasing Llama-3 & different LLMs over the next few days on different Medical and Legal benchmarks. Follow the updates here: [https://twitter.com/aadityaura](https://twitter.com/aadityaura)



https://preview.redd.it/9egbcayv9avc1.png?width=1344&format=png&auto=webp&s=436a972421d5568e1a544962b8cfd1c7b14efe04



",POSITIVE,0.96230149269104,39,6,https://www.reddit.com/r/MachineLearning/comments/1c7b35q/d_llama3_7b_and_70b_on_a_medical_domain_benchmark/
[N] Microsoft partners with Meta for Llama 2 release. But why?,"[N] Microsoft partners with Meta for Llama 2 release. But why? Staying on top of all changes, tools, and best practices with AI is getting increasingly hard. Each week I find **just 1 piece of information** that is most interesting across research, products, business news, and many more. No fluff guaranteed.

Sharing the top research from this week's edition:

https://preview.redd.it/fa3b1u39nlgb1.png?width=591&format=png&auto=webp&s=1ccd78136e3578396878fd9641605845f0309865

***Summary:*** Meta released their latest open-source model, Llama 2, in partnership with Microsoft‚Äôs Azure platform. But Microsoft also offers OpenAI models and is a major investor in the company (they paid $14B for 49%). So, confused Matt asks, why would Microsoft partner with Meta, when it might undermine their investment in OpenAI?

üí° **Answering the question:**

* **Spreading the risk:** OpenAI may have the first mover advantages, but this does not always last (e.g. Blackberry, Myspace, Yahoo). Microsoft is betting on AI but keeps the chips diversified on multiple players.
* **It‚Äôs beside the point:** regardless of who Microsoft supports, their game is to attract all AI utilization on Azure. It's not about the tools but about the CPU/GPU cycles they can charge for. *smart!*
* **The real AI gangsta:** Microsoft is sitting on the holy trinity of AI now.

1. Exclusive partnerships with top LLMs (OpenAI, Meta)
2. Priority access to Nvidia GPUs
3. And strategic assets like GitHub and Azure

[View tweet](https://twitter.com/mreflow/status/1681346606564769793?s=20&utm_source=tomorrownow.beehiiv.com&utm_medium=referral&utm_campaign=microsoft-partner-with-meta-ai-cover-letter-generator-stackoverflow-brings-ai-to-play)

If you'd like weekly recaps like this sent to your inbox, consider subscribing to the [**Tomorrow Now**](https://www.tomorrownow.tech/) newsletter. üòÑ",NEGATIVE,0.9177119135856628,39,21,https://www.reddit.com/r/MachineLearning/comments/15k7x0j/n_microsoft_partners_with_meta_for_llama_2/
"Llama Army: LLM's will proceed to radiate into millions of small specialized LLM's joined by central-LLM-front-pages, kindof like the googles and Bing's AI conversation results, folk will become rich by managing and happy branding of central LLM to distributed LLM services.","Llama Army: LLM's will proceed to radiate into millions of small specialized LLM's joined by central-LLM-front-pages, kindof like the googles and Bing's AI conversation results, folk will become rich by managing and happy branding of central LLM to distributed LLM services. Given that ChatGPT's can be specialized into every subject so that 100'ds of experts will fit on a 1tb SSD and run on local PC's, millions of specialized expert LLM's will appear on the web for subjects like electronics, chemistry, movies, python code, arts, medecine, every topic of encyclopedias, and the millions of experts will have to be centralized into search engines which take the first question and send it to the LLM's. The way those central front ends are branded and user friendly and efficient will make them hyper popular pages and the lead ones will have as many page views as wikipedia and that kind of website. 

The only limit to that happening is law, if all the world's governments manage to outlaw and firewall the LLM movements, and data training difficulty. I think that those limits are not going to stop central searches of millions of expert LLM's happening by 2029. 

Am I totally confused and misjudging it?",POSITIVE,0.9994277358055115,4,8,https://www.reddit.com/r/artificial/comments/1ccmqh6/llama_army_llms_will_proceed_to_radiate_into/
[P] Running Llama 2 locally in <10 min,"[P] Running Llama 2 locally in <10 min I wanted to play with Llama 2 right after its release yesterday, but it took me \~4 hours to download all 331GB of the 6 models. If you don‚Äôt have 4 hours or 331GB to spare, I brought all the models into XetHub, where it‚Äôs now available for you to use: [https://xethub.com/XetHub/Llama2](https://xethub.com/XetHub/Llama2).

I used xet mount to get started in seconds, and within a few minutes, I had the model generating text without needing to download everything or make an inference API call.  


`# From a g4dn.8xlarge instance in us-west-2:`

`Mount complete in 8.629213s`

`# install model requirements, and then ...`

`(venv-test) ubuntu@ip-10-0-30-1:~/Llama2/code$ torchrun --nproc_per_node 1 example_chat_completion.py \`

`--ckpt_dir ../models/llama-2-7b-chat/ \`

`--tokenizer_path ../models/tokenizer.model \`

`--max_seq_len 512 --max_batch_size 4`

`> initializing model parallel with size 1`

`> initializing ddp with size 1`

`> initializing pipeline with size 1`

`Loaded in 306.17 seconds`

`User: what is the recipe of mayonnaise?`

`> Assistant:  Thank you for asking! Mayonnaise is a popular condiment made from a mixture of egg yolks, oil, vinegar or lemon juice, and seasonings. Here is a basic recipe for homemade mayonnaise:`

`...`

  
Detailed instructions here: [https://xethub.com/XetHub/Llama2](https://xethub.com/XetHub/Llama2).

I‚Äôll add the -GGML variants next for the folks using llama.cpp.¬† Don‚Äôt forget to register with Meta to accept the license and acceptable use policy for these models!",NEGATIVE,0.9871413707733154,84,15,https://www.reddit.com/r/MachineLearning/comments/1547wcv/p_running_llama_2_locally_in_10_min/
[D] What are the differences between Gemma and Llama?,"[D] What are the differences between Gemma and Llama? What are the architectural differences between Gemma and Llama, and can we use this knowledge to infer anything about how Gemini works? What has Google figured out?",NEGATIVE,0.9979602098464966,8,11,https://www.reddit.com/r/MachineLearning/comments/1b0a5gu/d_what_are_the_differences_between_gemma_and_llama/
Meta inches toward open source AI with new LLaMA 3.1,Meta inches toward open source AI with new LLaMA 3.1,NEGATIVE,0.9793841242790222,7,1,https://www.zdnet.com/article/meta-inches-toward-open-source-ai-with-new-llama-3-1/
"[P] New collection of Llama, Mistral, Phi, Qwen, and Gemma models for function/tool calling","[P] New collection of Llama, Mistral, Phi, Qwen, and Gemma models for function/tool calling Introducing Rubra v0.1: a Collection of Open-Weight, Tool-Calling LLMs

Try it out [here](https://huggingface.co/spaces/sanjay920/rubra-v0.1-function-calling) in Hugging Face Spaces for free!

We also extended vLLM and llama.cpp so you can get started really easily. Check out our docs: [Rubra Documentation](https://docs.rubra.ai/)

| Model                                                     | Function Calling | MMLU (5-shot) | GPQA (0-shot) | GSM-8K (8-shot, CoT) | MATH (4-shot, CoT) | MT-bench |
|-----------------------------------------------------------|------------------|---------------|---------------|----------------------|--------------------|----------|
| [**Rubra Llama-3 70B Instruct**](https://huggingface.co/rubra-ai/Meta-Llama-3-70B-Instruct)       | 97.85%           | 75.90         | 33.93         | 82.26                | 34.24              | 8.36     |
| [**Rubra Llama-3 8B Instruct**](https://huggingface.co/rubra-ai/Meta-Llama-3-8B-Instruct)        | 89.28%           | 64.39         | 31.70         | 68.99                | 23.76              | 8.03     |
| [**Rubra Qwen2 7B Instruct**](https://huggingface.co/rubra-ai/Qwen2-7B-Instruct)                 | 85.71%           | 68.88         | 30.36         | 75.82                | 28.72              | 8.08     |
| [**Rubra Mistral 7B Instruct v0.3**](https://huggingface.co/rubra-ai/Mistral-7B-Instruct-v0.3)   | 73.57%           | 59.12         | 29.91         | 43.29                | 11.14              | 7.69     |
| [**Rubra Phi-3 Mini 128k Instruct**](https://huggingface.co/rubra-ai/Phi-3-mini-128k-instruct)   | 65.71%           | 66.66         | 29.24         | 74.09                | 26.84              | 7.45     |
| [**Rubra Mistral 7B Instruct v0.2**](https://huggingface.co/rubra-ai/Mistral-7B-Instruct-v0.2)   | 69.28%           | 58.90         | 29.91         | 34.12                | 8.36               | 7.36     |
| [**Rubra Gemma-1.1 2B Instruct**](https://huggingface.co/rubra-ai/gemma-1.1-2b-it)               | 45.00%           | 38.85         | 24.55         | 6.14                 | 2.38               | 5.75     |


### Why We Created These Models

Though the gap in capabilities has been closing between proprietary and open-source models, we saw function/tool calling still lagged behind in open source.

Until now, there have been limited options to get LLMs to output reliable function calls the same way you can get OpenAI and Anthropic to do so. Prompt engineering, output parsing, and JSON grammar is a hacky option. The other option has been models that do function calling, such as Berkeley Gorilla, NexusRaven, Hermes, Command-R+, but all of them are pinned to a model and some are not realistic in agentic use cases where you need long context and the ability to chat on top of function calling. Most recently, Mistral v0.3 has tool calling available in it, but in our tests, it doesn't meet expectations.

We also knew with our experience with [gptscript](https://github.com/gptscript-ai/gptscript), autogen, and other agent frameworks, that you may want a smaller or larger model depending on the use case. We didn't want to be pinned to one model, so we decided to further post-train all the ones we liked.

---

A couple of side notes:
- The Rubra Qwen2 model is capable of function calling in Chinese! It has limited function calling capability in the 28 other languages that Qwen2 supports.
- The [GGUF models](https://huggingface.co/collections/rubra-ai/rubra-v01-gguf-667f52cef892a8cb95bac7c8) have received ~100k downloads in the last 48 hours!
- We have already started to train a new Rubra Phi3 based on the June 2024 Phi-3-mini update that came out today. Stay tuned!",NEGATIVE,0.6165325045585632,32,0,https://www.reddit.com/r/MachineLearning/comments/1du3b1e/p_new_collection_of_llama_mistral_phi_qwen_and/
Meta AI is lying to your face,Meta AI is lying to your face,NEGATIVE,0.9943163990974426,310,119,https://www.reddit.com/gallery/1js6k41
[D] Disappointing Llama 2 Coding Performance: Are others getting similar results? Are there any other open-source models that approach ChatGPT 3.5's performance?,"[D] Disappointing Llama 2 Coding Performance: Are others getting similar results? Are there any other open-source models that approach ChatGPT 3.5's performance? I've been excitedly reading the news and discussions about Llama 2 the past couple of days, and got a chance to try it this morning.

I was underwhelmed by the coding performance (running the 70B model on https://llama2.ai/).  It has consistently failed most of the very-easy prompts that I made up this morning.  I checked each prompt with ChatGPT 3.5, and 3.5 got 100% (which means these prompts are quite easy).  This result was surprising to me based on the discussion and articles I've read.  However, digging into the paper (https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), the authors are transparent that the coding performance is lacking.

Are my observations consistent with the results others are getting?

I haven't had time to keep up with all the open-source LLMs being worked on by the community; are there any other models that approach even ChatGPT 3.5's coding performance? (Much less GPT 4's performance, which is the real goal.)",NEGATIVE,0.9997413754463196,16,23,https://www.reddit.com/r/MachineLearning/comments/154vlnr/d_disappointing_llama_2_coding_performance_are/
"[D] RTX 3060 12 GB for stable diffusion, BERT and LLama","[D] RTX 3060 12 GB for stable diffusion, BERT and LLama 

Do you think it's worth buying rtx 3060 12 gb  to train stable diffusion, llama (the small one) and Bert ? 

I d like to create a serve where I can use DL models. What do you think? 

EDIT: I also would like to compete in Kaggle for NLP problems. I thought that could be a good workflow if the dataset is too large:
- Train locally for small dataset
- Train in the cloud, maybe grid.ai (or another option that you like, please tell below)",NEGATIVE,0.9913046360015869,10,29,https://www.reddit.com/r/MachineLearning/comments/12oge7w/d_rtx_3060_12_gb_for_stable_diffusion_bert_and/
Chinese Premier warned there is ‚Äúa serious lack of self-sufficiency‚Äù in Chinese AI development because most Chinese LLMs are built on Meta's Llama,Chinese Premier warned there is ‚Äúa serious lack of self-sufficiency‚Äù in Chinese AI development because most Chinese LLMs are built on Meta's Llama,NEGATIVE,0.9981364011764526,25,1,https://www.scmp.com/tech/big-tech/article/3255545/china-said-fall-short-matching-us-advances-ai-owing-many-challenges-theory-and-technologies
Generate PowerPoints using Llama-3‚Ää‚Äî‚ÄäA first step in automating slide decks,Generate PowerPoints using Llama-3‚Ää‚Äî‚ÄäA first step in automating slide decks,POSITIVE,0.9949051141738892,2,6,https://medium.com/firebird-technologies/generate-powerpoints-using-llama-3-a-first-step-in-automating-slide-decks-536f5fcb6e0e
[R] Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B,[R] Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B,NEGATIVE,0.997156023979187,21,1,https://arxiv.org/abs/2406.07394
[P] FastLoRAChat Instruct-tune LLaMA on consumer hardware with shareGPT data,"[P] FastLoRAChat Instruct-tune LLaMA on consumer hardware with shareGPT data Announcing [FastLoRAChat](https://github.com/bupticybee/FastLoRAChat) , training chatGPT without A100.

&#x200B;

Releasing model:  [https://huggingface.co/icybee/fast\_lora\_chat\_v1\_sunlight](https://huggingface.co/icybee/fast_lora_chat_v1_sunlight)

and training data:  [https://huggingface.co/datasets/icybee/share\_gpt\_90k\_v1](https://huggingface.co/datasets/icybee/share_gpt_90k_v1)

&#x200B;

The purpose of this project is to produce similar result to the Fastchat model, but in much cheaper hardware (especially in non-Ampere GPUs).

This repository combined features of [alpaca-lora](https://github.com/tloen/alpaca-lora) and [Fastchat](https://github.com/lm-sys/FastChat):

1. Like Fastchat, support multilanguage and multi round chat.
2. Like alpaca-lora, support training and inference on low-end graphic cards (using LORA).
3. Opensource everything, include dataset, training code, export model code, and more.

Give it a try!",NEGATIVE,0.9976962208747864,105,14,https://www.reddit.com/r/MachineLearning/comments/12qf60j/p_fastlorachat_instructtune_llama_on_consumer/
[P] Local Llama 3.1 and Marqo Retrieval Augmented Generation,"[P] Local Llama 3.1 and Marqo Retrieval Augmented Generation I built a simple starter demo of a Knowledge Question and Answering System using Llama 3.1 (8B GGUF) and Marqo. Feel free to experiment and build on top of this yourselves! 

GitHub: [https://github.com/ellie-sleightholm/marqo-llama3\_1](https://github.com/ellie-sleightholm/marqo-llama3_1) ",NEGATIVE,0.9798081517219543,3,0,https://www.reddit.com/r/MachineLearning/comments/1ebzm8e/p_local_llama_31_and_marqo_retrieval_augmented/
"Apple Music is a major mess, and it won't beat Spotify","Apple Music is a major mess, and it won't beat Spotify",NEGATIVE,0.9997105002403259,2046,699,http://mashable.com/2015/06/09/apple-music-mess/
[P] Paddler (stateful load balancer custom-tailored for llama.cpp),"[P] Paddler (stateful load balancer custom-tailored for llama.cpp) I have started this project recently. It allows us to self-host llama.cpp and use it with open-source models. 

It started to gain some traction recently, and it is production-ready. 

It allows scaling from zero instances, so if you are using cloud providers to prototype your ideas with open-source LLMs, you will only pay for what you actually use. If there is a period of inactivity, you can use it to shut down expensive GPU instances and only leave some cheap CPU instances with the balancer itself running.

It is deployable on any cloud or in a Kubernetes cluster. It has some AWS helper utilities to make it easy to deploy there, but those are optional.

Paddler does not force you to configure llama.cpp in a specific way. You can configure your llama.cpp instances in any way, it plugs into its HTTP API.

https://github.com/distantmagic/paddler",NEGATIVE,0.5956836938858032,10,1,https://www.reddit.com/r/MachineLearning/comments/1dql325/p_paddler_stateful_load_balancer_customtailored/
"Meta‚Äôs battle with ChatGPT begins now. Meta‚Äôs AI assistant is being put everywhere across Instagram, WhatsApp, and Facebook. Meanwhile, the company‚Äôs next major AI model, Llama 3, has arrived.","Meta‚Äôs battle with ChatGPT begins now. Meta‚Äôs AI assistant is being put everywhere across Instagram, WhatsApp, and Facebook. Meanwhile, the company‚Äôs next major AI model, Llama 3, has arrived.",POSITIVE,0.6134534478187561,23,4,https://www.theverge.com/2024/4/18/24133808/meta-ai-assistant-llama-3-chatgpt-openai-rival
[R] Classifier-Free Guidance can be applied to LLMs too. It generally gives results of a model twice the size you apply it to. New SotA on LAMBADA with LLaMA-7B over PaLM-540B and plenty other experimental results.,[R] Classifier-Free Guidance can be applied to LLMs too. It generally gives results of a model twice the size you apply it to. New SotA on LAMBADA with LLaMA-7B over PaLM-540B and plenty other experimental results.,POSITIVE,0.9790135622024536,85,13,https://arxiv.org/abs/2306.17806
[D] What‚Äôs the best practice in choosing which quantized Llama 2 model to use?,"[D] What‚Äôs the best practice in choosing which quantized Llama 2 model to use? I am reading these 3 articles below and it is still not clear to me what‚Äôs the best practice to follow to guide me in choosing which quantized Llama 2 model to use.

https://huggingface.co/blog/gptq-integration

https://huggingface.co/blog/overview-quantization-transformers

https://towardsai.net/p/machine-learning/gptq-quantization-on-a-llama-2-7b-fine-tuned-model-with-huggingface?amp=1

Questions:
1) I understand there are currently 4 quantized Llama 2 models (8, 4, 3, and 2-bit precision) to choose from. Is this right?
2) with the default Llama 2 model, how many bit precision is it?
3) are there any best practice guide to choose which quantized Llama 2 model to use?

Would really appreciate any input on the above, even if you only know the answer to 1 or 2 of the questions above. Many thanks!",NEGATIVE,0.9931572079658508,10,18,https://www.reddit.com/r/MachineLearning/comments/16lvj4b/d_whats_the_best_practice_in_choosing_which/
"[D] Mistral-7B-v0.3 instruct vs Llama-3 8B Instruct eval in the Medical domain 
","[D] Mistral-7B-v0.3 instruct vs Llama-3 8B Instruct eval in the Medical domain 
 https://preview.redd.it/wzkhd6k1v02d1.png?width=571&format=png&auto=webp&s=0733a90b13450d48721f6eced9e70dfe2028bf93

",NEGATIVE,0.9906883239746094,9,3,https://www.reddit.com/r/MachineLearning/comments/1cy7no2/d_mistral7bv03_instruct_vs_llama3_8b_instruct/
"Meta says Llama 3 beats most other models, including Gemini","Meta says Llama 3 beats most other models, including Gemini",POSITIVE,0.9972078204154968,0,6,https://www.theverge.com/2024/4/18/24134103/llama-3-benchmark-testing-ai-gemma-gemini-mistral
[R] Code Llama: Open Foundation Models for Code - Meta Ai 2023,"[R] Code Llama: Open Foundation Models for Code - Meta Ai 2023 Paper: [https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) 

Github: [https://github.com/facebookresearch/codellama](https://github.com/facebookresearch/codellama) 

Models: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) 

Blog: [https://ai.meta.com/blog/code-llama-large-language-model-coding/](https://ai.meta.com/blog/code-llama-large-language-model-coding/) 

Abstract:

>We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance  among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.

https://preview.redd.it/grzcrnx4p3kb1.jpg?width=915&format=pjpg&auto=webp&s=ae41c02d892bfb8275723dbfede7ac3165717357

https://preview.redd.it/4qpazkx4p3kb1.jpg?width=641&format=pjpg&auto=webp&s=31aaf9ecafbd70fbf2c1cd4e92ccf594c09b3861

https://preview.redd.it/hlrp4x05p3kb1.jpg?width=711&format=pjpg&auto=webp&s=3651f519dc9b23b432656416749c3f7e113b4ce7

&#x200B;

&#x200B;",NEGATIVE,0.9915920495986938,39,15,https://www.reddit.com/r/MachineLearning/comments/1609z3u/r_code_llama_open_foundation_models_for_code_meta/
[P] OS Mass Document Analytics with LlamaIndex Index,"[P] OS Mass Document Analytics with LlamaIndex Index Hi, folks, sharing my [latest](https://github.com/JSv4/OpenContracts) open source project to do mass data extraction and question answering across a corpus of documents. You can define target data schemas as pydantic models or python primitives. Layouts elements and human annotations are automatically embedded and accessible as a LlamaIndex VectorStore. If you write custom LlamaIndex question answering pipelines, they show up in the frontend and can be applied to a corpus.

I've worked on [OpenContracts](https://github.com/JSv4/OpenContracts) for a number of years now. While it started out as a tool to label and annotate documents, thanks to the recent advances in LLMs and vector databases, I've released a new version with a bunch of cool features to use LLMs, vector search and AI Agents. It's based on Django, and it keeps amazing me how Django keeps getting more and more capable with age!

# Key Features:

1. Manage Documents - Manage document collections

2. Layout Parser - Automatically extracts layout features from PDFs

3. Automatic Vector Embeddings - generated for uploaded PDFs and extracted layout blocks

4. Pluggable microservice analyzer architecture - to let you analyze documents and automatically annotate them

5. Human Annotation Interface - to manually annotated documents, including multi-page annotations.

6. LlamaIndex Integration - Use our vector stores (powered by pgvector) and any manual or automatically annotated features to let an LLM intelligently answer questions.

7. Data Extract - ask multiple questions across hundreds of documents using complex LLM-powered querying behavior. Our sample implementation uses LlamaIndex + Marvin.

8. Custom Data Extract - Custom data extract pipelines can be used on the frontend to query documents in bulk.

Checkout the [repo](https://github.com/JSv4/OpenContracts) or the docs!
",NEGATIVE,0.9899942278862,7,0,https://www.reddit.com/gallery/1do4ujr
[P] Llama-2 4bit fine-tune with dolly-15k on Colab (Free),"[P] Llama-2 4bit fine-tune with dolly-15k on Colab (Free) Simple walkthrough of fine-tuning [llama-2 instruct fine-tuned on guanaco](https://huggingface.co/guardrail/llama-2-7b-guanaco-instruct-sharded) model with 4bit QLoRA on a free Google Colab instance.

**Colab**: [https://colab.research.google.com/drive/134o\_cXcMe\_lsvl15ZE\_4Y75Kstepsntu?usp=sharing](https://colab.research.google.com/drive/134o_cXcMe_lsvl15ZE_4Y75Kstepsntu?usp=sharing)  
**GitHub**: [https://github.com/kw2828/guardrail-ml](https://github.com/kw2828/guardrail-ml)  
**YouTube Overview**: [https://www.youtube.com/watch?v=o5bU1H-6TqM&ab\_channel=GenerativeAIEntrepreneurs](https://www.youtube.com/watch?v=o5bU1H-6TqM&ab_channel=GenerativeAIEntrepreneurs)

Bonus colab in repo on generating your own JSON Q&A dataset from PDF in the repo above.",NEGATIVE,0.9967768788337708,39,16,https://www.reddit.com/r/MachineLearning/comments/156k8xz/p_llama2_4bit_finetune_with_dolly15k_on_colab_free/
"[P] A look at the latest major open LLM releases: Mixtral, Llama 3, Phi-3, and OpenELM","[P] A look at the latest major open LLM releases: Mixtral, Llama 3, Phi-3, and OpenELM",POSITIVE,0.9956881403923035,26,1,https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms
Chat with your SQL Database using Llama 3,Chat with your SQL Database using Llama 3,NEGATIVE,0.9695965647697449,11,3,https://medium.com/@arslanshahid-1997/chat-with-your-sql-database-using-llama-3-4d4c496e12e8
[P] Llama 3 70B powered coding copilot extension,"[P] Llama 3 70B powered coding copilot extension Was super excited to read the news from Meta this morning, particularly around the HumanEval scores the 70B model got.

Thought it'd be useful to make the new Llama 3 70B available to anyone that wants to try it, so I added it to my VS Code coding copilot extension [double.bot](https://www.double.bot). 

Also making it free for the first 50 messages so everyone gets a chance to try it while we wait for the quantized versions to run locally",NEGATIVE,0.7555140256881714,2,4,https://www.reddit.com/r/MachineLearning/comments/1c7c6e2/p_llama_3_70b_powered_coding_copilot_extension/
[D] [RAG] [llama-index] How to execute multiple SQL queries with SQLTableRetrieverQueryEngine in NL2SQL project?,"[D] [RAG] [llama-index] How to execute multiple SQL queries with SQLTableRetrieverQueryEngine in NL2SQL project? I am working on a project where user will ask natural language queries and this llama-index based engine will convert that natural language to sql query and execute it on my database and give answer in natural language to the user. Problem is it is only able to execute one query per question so comparison quetions are not possible to answer and also if a question does not require querying the database it will still query the database. How can I solve this. Please help me with your suggesting.   
Thanks in advance. ",NEGATIVE,0.999277651309967,7,9,https://www.reddit.com/r/MachineLearning/comments/193usfg/d_rag_llamaindex_how_to_execute_multiple_sql/
Introducing Meta Llama 3,Introducing Meta Llama 3,POSITIVE,0.9982501864433289,17,2,https://ai.meta.com/blog/meta-llama-3/
[D] LLaMA release is a joke,"[D] LLaMA release is a joke I have tried to get access to the weights of LLaMA for a long time now. I filled out the google form couple of days after they released it and have been waiting patiently but no luck. I finally received the link a week ago, but now I am hit with the ""403 Forbidden"" error (. I can't even download the 7B weights and the link is supposed to expire today. I have emailed the authors and the support email without any luck. What I find most frustrating is that some researchers have a huge head start while others are scrambling to even get started. The GitHub issue is full of people with the same issue as me. I know that there are alternatives to LLaMA, but I am worried that they may not be as good as LLaMA and the paper might not be as strong.",NEGATIVE,0.9995638728141785,3,23,https://www.reddit.com/r/MachineLearning/comments/12ype1k/d_llama_release_is_a_joke/
"[D] Deploy Mixtral 8x7B, LLaMA 2, and Mistral, on AWS EC2 with vLLM","[D] Deploy Mixtral 8x7B, LLaMA 2, and Mistral, on AWS EC2 with vLLM Hi everyone,

In 2023, many sophisticated open-source LLMs have become available. However, integrating these AI models into a production environment continues to be a complex task. I made an article that will guide you through deploying some of the top LLMs, namely LLaMA 2 70B, Mistral 7B, and Mixtral 8x7B, on AWS EC2. I employ an inference engine capable of batch processing and distributed inference: vLLM.

vLLM will greatly aid in the implementation of LLaMA 2 and Mixtral because it allows us to use AWS EC2 instances equipped with multiple smaller GPUs (such as the NVIDIA A10) rather than relying on a single large GPU (like the NVIDIA A100 or H100).

See the detailed how-to here: [https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html](https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html?utm_source=reddit&utm_campaign=bqwerty12-3816-81ed-a26450242ac140019)

In this tutorial I used AWS EC2 but I could have used other vendors of course. The main challenge is the cost of the GPUs and their availability.

Please don't hesitate to share feedbacks about this article, it will be very much appreciated!

Julien",POSITIVE,0.9893401265144348,14,5,https://www.reddit.com/r/MachineLearning/comments/1apxg42/d_deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/
GPU out of memory error message on Colab with Llama 3 [D],"GPU out of memory error message on Colab with Llama 3 [D] Hi guys,

I just tried to run Llama 3 on my Colab(free version) and seems that I ran out of the memory: 

 OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 9.06 MiB is free. Process 8863 has 14.74 GiB memory in use. Of the allocated memory 14.60 GiB is allocated by PyTorch, and 22.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH\_CUDA\_ALLOC\_CONF=expandable\_segments:True to avoid fragmentation.  See documentation for Memory Management  ([https://pytorch.org/docs/stable/notes/cuda.html#environment-variables](https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))  
 

Anyone have the same experience? Has anyone managed to run Llama 3 on free version of Colab (or similar platform)? 

Thanks! ",NEGATIVE,0.9997598528862,0,2,https://www.reddit.com/r/MachineLearning/comments/1cdooci/gpu_out_of_memory_error_message_on_colab_with/
[P] Don't have enough GPU to train Mixtral? Why not try LLaMA-MoE~,"[P] Don't have enough GPU to train Mixtral? Why not try LLaMA-MoE~  LLaMA-MoE is a series of open-sourced Mixture-of-Expert (MoE) models based on LLaMA and SlimPajama. We build LLaMA-MoE with the following two steps:

1. Partition LLaMA's FFNs into sparse experts and insert top-K gate for each layer of experts.
2. Continually pre-train the initialized MoE model with an optimized data sampling weights from Sheared LLaMA and filtered datasets from SlimPajama.

If you don't have plenty of computing resources to train Mixtral, you may want to try LLaMA-MoE for downstream researches.

Check it out: [pjlab-sys4nlp/llama-moe](https://github.com/pjlab-sys4nlp/llama-moe)",NEGATIVE,0.9988226294517517,30,6,https://www.reddit.com/r/MachineLearning/comments/18qh2ch/p_dont_have_enough_gpu_to_train_mixtral_why_not/
"[D] Working in Python code, will Llama 2 return a table in JSON format when prompted?","[D] Working in Python code, will Llama 2 return a table in JSON format when prompted? Hi everyone, I wanted to confirm this question below before really jumping into Llama 2.

If I prompted Llama to provide answers in JSON format, for eg. something like this prompt:

    sequences = pipeline(""Provide your answer in the JSON format with the feature names as the keys."")

Then following that in my Python code I have:

    for seq in sequences:
       # Convert output string to dictionary object
       dict = json.loads(seq['generated_text'])

Will Llama 2 be able to output the answers in JSON format in order for me to convert it into a dictionary in the next step?

Would appreciate any input. Many thanks!",NEGATIVE,0.9982922673225403,0,14,https://www.reddit.com/r/MachineLearning/comments/16j1e9j/d_working_in_python_code_will_llama_2_return_a/
[D] Training OpenLlama 3Bv2 on a TPU v3-8 VM,"[D] Training OpenLlama 3Bv2 on a TPU v3-8 VM Hi! Apologize if this isn't the correct place to post this, but I figured why not give it a shot.  


I'm trying to fine-tune OpenLlama 3Bv2 for SequenceClassification but I've got very little experience working with TPUs. Here's my current code:  


    import torch
    import os
    import pickle
    from torch.utils.data import DataLoader, Dataset
    from transformers import LlamaForSequenceClassification, get_linear_schedule_with_warmup
    from sklearn.metrics import accuracy_score
    from tqdm.auto import tqdm
    
    import torch_xla
    import torch_xla.core.xla_model as xm
    import torch_xla.distributed.parallel_loader as pl
    import torch_xla.distributed.xla_multiprocessing as xmp
    
    # Paths and model selection
    path = ""trained_paragraph_1/""
    mini = False
    pref = ""tokenized_llama/mini/"" if mini else ""tokenized_llama/""
    model_path = 'tokenized_llama/model'
    
    # Training parameters
    batch_size = 32
    val_batch_size = 64
    epochs = 1
    steps_per_eval = 10000
    
    # Optimization settings
    learning_rate = 1e-6
    weight_decay = 0.05
    warmup_ratio = 0.1
    
    class PreTokenizedTextDataset(Dataset):
        def __init__(self, encodings, labels):
            self.encodings = encodings
            self.labels = labels
    
        def __len__(self):
            return len(self.labels)
    
        def __getitem__(self, idx):
            item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
            return item
        
    # Define the map function for multiprocessing
    def _mp_fn(index, flags):
        # Load the pre-tokenized training and validation datasets
        with open(f'{pref}train_tokenized.pkl', 'rb') as f:
            train_encodings, train_labels = pickle.load(f)
        train_dataset = PreTokenizedTextDataset(train_encodings, train_labels)
    
        with open(f'{pref}val_tokenized.pkl', 'rb') as f:
            val_encodings, val_labels = pickle.load(f)
        val_dataset = PreTokenizedTextDataset(val_encodings, val_labels)
    
        # Initialize the TPU device
        device = xm.xla_device()
    
        # Define the DataLoader for training and validation
        train_sampler = torch.utils.data.distributed.DistributedSampler(
            train_dataset,
            num_replicas=xm.xrt_world_size(),
            rank=xm.get_ordinal(),
            shuffle=True
        )
        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, pin_memory=True)
    
        val_sampler = torch.utils.data.distributed.DistributedSampler(
            val_dataset,
            num_replicas=xm.xrt_world_size(),
            rank=xm.get_ordinal(),
            shuffle=False
        )
        val_loader = DataLoader(val_dataset, batch_size=val_batch_size, sampler=val_sampler, pin_memory=True)
    
        # Initialize the tokenizer and model
        model = LlamaForSequenceClassification.from_pretrained(model_path, num_labels=2)
    
        # Move the model to the TPU
        model.to(device)
        
        # Define the optimizer
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
        # Define the learning rate scheduler with warmup
        num_training_steps = len(train_loader) * epochs
        num_warmup_steps = int(warmup_ratio * num_training_steps)
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)
    
        # Training loop with checkpointing and evaluation
        model.train()
        global_step = 0
        best_accuracy = 0.0
        for epoch in range(epochs):
            para_loader = pl.ParallelLoader(train_loader, [device])
            train_iterator = para_loader.per_device_loader(device)
            if xm.is_master_ordinal():
                train_iterator = tqdm(train_iterator, desc=f""Epoch {epoch+1}"", unit=""batch"")
            for batch in train_iterator:
                optimizer.zero_grad()
    
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
    
                loss.backward()
                xm.optimizer_step(optimizer)
    
                scheduler.step()
    
                global_step += 1
                if xm.is_master_ordinal():
                    train_iterator.set_postfix(loss=loss.item())
    
                if global_step % steps_per_eval == 0 and xm.is_master_ordinal():
                    # Checkpointing
                    xm.save(model.state_dict(), os.path.join(path, f'checkpoint-{global_step}.pt'))
                    # Evaluation
                    model.eval()
                    total_eval_accuracy = 0
                    total_eval_loss = 0
                    para_loader = pl.ParallelLoader(val_loader, [device])
                    eval_iterator = para_loader.per_device_loader(device)
                    for batch in eval_iterator:
                        with torch.no_grad():
                            input_ids = batch['input_ids'].to(device)
                            attention_mask = batch['attention_mask'].to(device)
                            labels = batch['labels'].to(device)
                            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                            logits = outputs.logits
                            loss = outputs.loss
                            total_eval_loss += loss.item()
                            predictions = torch.argmax(logits, dim=-1)
                            total_eval_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())
    
                    avg_val_accuracy = total_eval_accuracy / len(val_loader)
                    avg_val_loss = total_eval_loss / len(val_loader)
                    xm.master_print(f""Step {global_step}, Validation Loss: {avg_val_loss}, Validation Accuracy: {avg_val_accuracy}"")
    
                    # Save the best model
                    if avg_val_accuracy > best_accuracy:
                        best_accuracy = avg_val_accuracy
                        xm.save(model.state_dict(), os.path.join(path, 'best_model.pt'))
    
                    model.train()
    
        # Load the best model
        if xm.is_master_ordinal():
            model.load_state_dict(torch.load(os.path.join(path, 'best_model.pt')))
    
        # Save the fine-tuned model
        if xm.is_master_ordinal():
            model.save_pretrained(f'{path}fine_tuned_model')
    
    # Start training using xmp.spawn
    FLAGS = {}
    xmp.spawn(_mp_fn, args=(FLAGS,), start_method='fork')

However, it doesn't seem to be performing correctly. The first 2 batches run perfectly fine but I then receive this error:  
""BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.""  


If anyone has any ideas on what may be causing this as well as how to improve performance it would be greatly appreciated. Thanks!

&#x200B;",NEGATIVE,0.9993374943733215,0,8,https://www.reddit.com/r/MachineLearning/comments/18tvuv7/d_training_openllama_3bv2_on_a_tpu_v38_vm/
[P] LLaMA Nuts and Bolts: A holistic way of understanding how LLaMA and Large Language Models run,"[P] LLaMA Nuts and Bolts: A holistic way of understanding how LLaMA and Large Language Models run I'm so excited to announce that my LLaMA Nuts and Bolts open-source project developed using Go is now publicly out!

You can find it on my Github repo: [https://github.com/adalkiran/llama-nuts-and-bolts](https://github.com/adalkiran/llama-nuts-and-bolts)

A holistic way of understanding how LLaMA and its components run in practice, with code and detailed documentation. ""The nuts and bolts"" (practical side instead of theoretical facts, pure implementation details) of required components, infrastructure, and mathematical operations without using external dependencies or libraries.

The goal is to make an experimental project that can perform inference on the LLaMa 2 7B-chat model completely outside of the Python ecosystem (using Go language). Throughout this journey, the aim is to acquire knowledge and shed light on the abstracted internal layers of this technology.

This journey is an intentional journey of literally reinventing the wheel. While reading my journey in the documentation, you will see the details of how Large Language Models work, through the example of the LLaMa model.

If you are curious like me about how the LLMs (Large Language Models) and transformers work and have delved into conceptual explanations and schematic drawings in the sources but hunger for deeper understanding, then this project is perfect for you too!

You will not only find the details of the LLaMa architecture but will find explanations of a wide variety of related concepts in the documentation directory. From reading a Pickle, a PyTorch model, a Protobuf, and a SentencePiece tokenizer model files at byte-by-byte level, to internals of BFloat16 data type, implementation from scratch of a Tensor structure and mathematical operations including linear algebraic computations.  
This project was initially started to learn what an LLM does behind by running and debugging it and was made for experimental and educational purposes only, not for production use.

I will be happy if you check out it and comments are welcome!",POSITIVE,0.9958339929580688,37,0,https://www.reddit.com/r/MachineLearning/comments/1bg61qi/p_llama_nuts_and_bolts_a_holistic_way_of/
[P] Discover AstraQuasar-4B: a NEW LLaMA-based arch | First training implementation of the self layer calling (Duplicate Trick),"[P] Discover AstraQuasar-4B: a NEW LLaMA-based arch | First training implementation of the self layer calling (Duplicate Trick)  

Hey r/MachineLearning,

I'm reaching out to this incredible community because we've got something unique on our hands, and it's a bit of a diamond in the rough. Meet [AstraQuasar-4B](https://huggingface.co/AstraMindAI/AstraQuasar-4B), a fresh take on language models with a twist ‚Äì it's ambitiously undertrained but holds a secret sauce called the duplicate trick (also rocking in backprop!).

AstraQuasar-4B is built on the robust Phi-2 architecture, yet it's not your run-of-the-mill model. The duplicate trick is its standout feature, significantly reducing loss with a promise of untapped stability and performance enhancements. But here's the catch ‚Äì it's undertrained. We're currently training it at a decent scale but we're in uncharted territory, where the usual benchmarks haven't been met because, frankly, we're still figuring it out.

It's fully compatible with Hugging Face pipelines, so there's no need to worry about switching to other trainers.

We believe the true value of AstraQuasar-4B isn't just in what it is now but what it could become with your input. This is a call to arms for testers, tinkerers, and thinkers alike.

Let's start a conversation. Share your thoughts, your skepticism, your ideas. How would you approach training? What experiments would you run? How can we collectively push AstraQuasar-4B beyond its current limits?

(Note: This is a genuine call for collaboration and idea-sharing. No sponsorships, just pure, unadulterated curiosity and a belief in the power of community.)",POSITIVE,0.9973013997077942,6,4,https://www.reddit.com/r/MachineLearning/comments/1as2l18/p_discover_astraquasar4b_a_new_llamabased_arch/
"AI Explained: ‚ÄòHer‚Äô AI, Almost Here? Llama 3, Vasa-1, and Altman ‚ÄòPlugging Into Everything You Want To Do‚Äô","AI Explained: ‚ÄòHer‚Äô AI, Almost Here? Llama 3, Vasa-1, and Altman ‚ÄòPlugging Into Everything You Want To Do‚Äô",POSITIVE,0.9285874962806702,6,0,https://www.youtube.com/watch?v=pal-dMJFU6Q
[P] Public API for open LLMs like llama.cpp with pay-per-use ?,"[P] Public API for open LLMs like llama.cpp with pay-per-use ? Are there such service already ?  
If no would it be useful given:

* The need for setup
* The required computing power

?

Big cloud providers like AWS provide a lot of AI services but AFAIK I can't see such thing for open LLMs.

*LLM curated Google search did not tell me that already exists*",NEGATIVE,0.9990548491477966,8,18,https://www.reddit.com/r/MachineLearning/comments/139mnyd/p_public_api_for_open_llms_like_llamacpp_with/
[D]Suggestions on keeping Llama index cost down,"[D]Suggestions on keeping Llama index cost down Step 1 in my efforts to have a robot do my job for me :P has led to a successful implementation of Llama Index. I used ""GPTSimpleVectorIndex"" to read in a folder of 140 procedures (1 million tokens) into a single json which I can then query with ""index.query"". It works flawlessly giving me excellent responses. However, it costs quite a bit - anywhere from 0 to 30c per query. I think this comes down to it using Davinci 3 rather than GPT3.5 Turbo which does not appear to be implemented with Llama yet. It appears to always use the full whack of 4096 tokens too.

Just wondering if there is a way of keeping the price down without imposing a smaller max token limit? I was thinking of maybe using  some form of lemmatization or POS to condense down the context as much as possible but not sure if this would harm the accuracy. Any suggestions appreciated!

Update: thanks to @supreethrao, GPT3.5-Turbo is in fact implemented in Llama-index. Price per request instantly cut to one tenth of the cost. Just use these lines in python when building your index:  
from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor  
from langchain.llms import OpenAIChat  
data = SimpleDirectoryReader('database').load_data() #'database' is the folder that contains your documents  
llm_predictor = LLMPredictor(llm=OpenAIChat(temperature=0.7, model_name=""gpt-3.5-turbo"")) #set the model parameters  
index = GPTSimpleVectorIndex(data, llm_predictor=llm_predictor) # create the index  
response = index.query(""How to create an engineering drawing?"") #query the index  
print(response)    
Update2: After using the robot for a while, I've found that the responses from GPT3.5-Turbo have been very basic and unhelpful. It often says ""yes the context contains the information you are asking about"". Other times it just says ""the context does not have the information to answer that question"", which is untrue as I have the program print the context to the console and it is always contains very apt information to answer the query. Not sure if it's just not getting enough tokens to answer my query or if there is something more serious in GPT3.5's architecture that is just not very well suited to this task.   Will have to do a bit more trial and error to figure it out.",POSITIVE,0.9956936836242676,13,19,https://www.reddit.com/r/MachineLearning/comments/123j77g/dsuggestions_on_keeping_llama_index_cost_down/
[P] Fine-Tuning and Evaluating a Falcon 7B/LLAMA 7B Model for HTML Code Generation,"[P] Fine-Tuning and Evaluating a Falcon 7B/LLAMA 7B Model for HTML Code Generation I was given this assignment but however i have access to my own pc only. I dont think it has any gpu. I'm new to LLMs. Please guide me how to proceed.

it is also mentioned that i can use any other model as well from Hugging Face.. thanks in advance",NEGATIVE,0.7466484308242798,4,6,https://www.reddit.com/r/MachineLearning/comments/18okus7/p_finetuning_and_evaluating_a_falcon_7bllama_7b/
"[D] LLaMA 2 not working, could module named fire not found be the issue?","[D] LLaMA 2 not working, could module named fire not found be the issue? hi, I'm fairly new to elms and wanted to try out the raw llama without any webuis or other stuff, I did everything that was written in metaai's repo and when launching the model with

torchrun --nproc\_per\_node 2 example\_chat\_completion.py \\

\--ckpt\_dir llama-2-13b-chat/ \\

\--tokenizer\_path tokenizer.model \\

\--max\_seq\_len 512 --max\_batch\_size 6

I got hit with ""NOTE: Redirects are currently not supported in Windows or MacOs.

WARNING:torch.distributed.run:

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

Setting OMP\_NUM\_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

Traceback (most recent call last):

File ""/Users/filipbartczak/llama/example\_chat\_completion.py"", line 6, in <module>

import fire

ModuleNotFoundError: No module named 'fire'

Traceback (most recent call last):

File ""/Users/filipbartczak/llama/example\_chat\_completion.py"", line 6, in <module>

import fire

ModuleNotFoundError: No module named 'fire'

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local\_rank: 0 (pid: 6267) of binary: /opt/homebrew/opt/python@3.10/bin/python3.10

Traceback (most recent call last):

File ""/opt/homebrew/bin/torchrun"", line 8, in <module>

sys.exit(main())

File ""/opt/homebrew/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/\_\_init\_\_.py"", line 346, in wrapper

return f(\*args, \*\*kwargs)

File ""/opt/homebrew/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main

run(args)

File ""/opt/homebrew/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run

elastic\_launch(

File ""/opt/homebrew/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in \_\_call\_\_

return launch\_agent(self.\_config, self.\_entrypoint, list(args))

File ""/opt/homebrew/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch\_agent

raise ChildFailedError(

torch.distributed.elastic.multiprocessing.errors.ChildFailedError:

============================================================

example\_chat\_completion.py FAILED

\------------------------------------------------------------

Failures:

\[1\]:

time : 2023-12-28\_06:18:46

host : mbp-filip.home

rank : 1 (local\_rank: 1)

exitcode : 1 (pid: 6268)

error\_file: <N/A>

traceback : To enable traceback see: [https://pytorch.org/docs/stable/elastic/errors.html](https://pytorch.org/docs/stable/elastic/errors.html)

\------------------------------------------------------------

Root Cause (first observed failure):

\[0\]:

time : 2023-12-28\_06:18:46

host : mbp-filip.home

rank : 0 (local\_rank: 0)

exitcode : 1 (pid: 6267)

error\_file: <N/A>

traceback : To enable traceback see: [https://pytorch.org/docs/stable/elastic/errors.html](https://pytorch.org/docs/stable/elastic/errors.html)

============================================================""

help would be greatly appreciated!",NEGATIVE,0.999695897102356,0,6,https://www.reddit.com/r/MachineLearning/comments/18sv3cp/d_llama_2_not_working_could_module_named_fire_not/
"[d] Code Llama, a state-of-the-art large language model for coding","[d] Code Llama, a state-of-the-art large language model for coding [https://ai.meta.com/blog/code-llama-large-language-model-coding/](https://ai.meta.com/blog/code-llama-large-language-model-coding/)",NEGATIVE,0.7378607392311096,10,3,https://www.reddit.com/r/MachineLearning/comments/1ae0lsj/d_code_llama_a_stateoftheart_large_language_model/
"[P] Finetune 387% faster TinyLlama, 188% faster DPO, 2x faster LLM inference","[P] Finetune 387% faster TinyLlama, 188% faster DPO, 2x faster LLM inference Hey r/MachineLearning!!  Happy New Year! (Ok probably not since it's 25 days now lol) You might have heard of Unsloth - my OSS package makes LoRA / QLoRA finetuning of **Mistral 7b 200% faster** and use 60% less VRAM! It's Apache 2 and free! [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth). Released our January 2024 release a few days ago, and just wanted to share :)

https://preview.redd.it/7llah04qjeec1.png?width=990&format=png&auto=webp&s=9484021ccf687dbe2b75c4a3bbc6c45b04abf5d6

1. Finetune using QLoRA Tiny Llama **387% faster** \+ use **74% less memory** on 1 epoch of Alpaca's 52K dataset in **84 minutes** on a free Google Colab instance with **packing support**! We also extend the context window from 2048 to 4096 tokens automatically via u/kaiokendev's RoPE Scaling method! Colab [Notebook Link](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)
2. **DPO is 188% faster!** We have a [notebook replication](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) of Zephyr 7b.
3. With **packing** support through ü§óHugging Face, Tiny Llama is not 387% faster but a whopping **6,700% faster** than non packing!! Shocking! What's packing? Given 3 samples of different lengths, all must be padded to the maximum length in the batch, then one masks out the pad tokens during the CE Loss. But this wastes computation. Instead, take all 3 short sequences and ""pack"" them together, saving computation.

https://preview.redd.it/cxjku0biheec1.png?width=741&format=png&auto=webp&s=0a09e9fda0bafac8de9c287783f265cea89464fa

1. We pre-quantized Llama-7b, Mistral-7b, Codellama-34b etc to make **downloading 4x faster** \+ **reduce 500MB - 1GB in VRAM use** by reducing fragmentation. No more OOMs! Free [Notebook Link](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) for Mistral 7b. Have a whole list on our HF page of pre-quantized models: [https://huggingface.co/unsloth](https://huggingface.co/unsloth)
2. Just added a prelim update to make i**nference 2x faster than HF** for LoRA! It's nowhere as fast as SOTA, but I just started working on it!
3. You can now save to GGUF / 4bit to 16bit conversions in 5 minutes instead of >= 30 minutes in a free Google Colab!! So **600% faster GGUF conversion**! Scroll down the [free Llama 7b notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing%22) to see how we do it. Use it with:

&#x200B;

    model.save_pretrained_merged("""", save_method = ""merged_16bit"")
    model.save_pretrained_gguf(""dir"", tokenizer, quantization_method = ""f16"")
    model.save_pretrained_gguf(""dir"", tokenizer, quantization_method = ""q4_k_m"")
    
    ### Or pushing to hub:
    model.push_to_hub_merged(""hf/me"", save_method = ""merged_16bit"")
    model.push_to_hub_merged(""hf/me"", save_method = ""merged_4bit"")
    model.push_to_hub_gguf(""hf/me"", tokenizer, quantization_method = ""q4_k_m"")

* As highly requested, all **Llama/Mistral models, including Yi, Deepseek, Starling, and Qwen**,  are now supported. Just try your favorite model out! We'll error out if  it doesn't work :) In fact, just try your model out and we'll error out  if it doesn't work!

&#x200B;

    from unsloth import FastLanguageModel model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = ""ANY_MODEL!!"",
    )

DPO now has streaming support for stats - HF only provided it for the validation set, but now it's enabled even during training!

https://preview.redd.it/bsh1h72iieec1.jpg?width=1237&format=pjpg&auto=webp&s=f3c037ed7f93221a101ffd48ce5b5539481f77c6

We updated all our **free Colab notebooks**:

* Finetune **Mistral 7b 200% faster**, use 60% less VRAM: [https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg\_?usp=sharing](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)
* Finetune **Llama 7b 200% faster**: [https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing%22](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing%22)
* **DPO 188% faster**: [https://colab.research.google.com/drive/15vttTpzzVXv\_tJwEk-hIcQ0S9FcEWvwP?usp=sharing](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)
* **Tiny Llama 387% faste**r: [https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)

We also did a **blog post with  ü§ó Hugging Face**! [https://huggingface.co/blog/unsloth-trl](https://huggingface.co/blog/unsloth-trl) And we're in [HF docs](https://huggingface.co/docs/trl/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!

https://preview.redd.it/mgpng1slieec1.jpg?width=936&format=pjpg&auto=webp&s=ca7540329ab2c07cd2a34deb3a8d2cd7876a8a93

To upgrade Unsloth with no dependency updates:

    pip install --upgrade https://github.com/unslothai/unsloth.git 

Also we have Kofi - so if you can support our work that'll be much appreciated! [https://ko-fi.com/unsloth](https://ko-fi.com/unsloth) And whenever Llama-3 pops - we'll add it in quickly!! Thanks! Our blog post on all the stuff we added: [https://unsloth.ai/tinyllama-gguf](https://unsloth.ai/tinyllama-gguf)

Also if you wanna help out and love making LLM training faster and more efficient, I'm looking for collaborators - we're currently working to add Phi-2, Mixtral support, and working on following Mosaic's optimizations to not just do hardware and software tricks, but also methodology tricks!",NEGATIVE,0.9948939085006714,23,2,https://www.reddit.com/r/MachineLearning/comments/19eiwe8/p_finetune_387_faster_tinyllama_188_faster_dpo_2x/
[D] fine-tuning Llama model for summarization,"[D] fine-tuning Llama model for summarization Has anyone attempted to fine-tune the Llama model for text summarization? I've been working on a code implementation using the Llama model and incorporating Lora, but I'm encountering an issue where I'm getting a Rouge score of 99 for both the train and test sets, even in the first epoch. I know that something is amiss, but I'm having difficulty understanding the root of the problem. Furthermore, when I examined the predictions on the test set, it appears that the model is simply duplicating the input instead of generating a summary. Could anyone offer suggestions or insights into what might be causing this issue?",NEGATIVE,0.9992951154708862,6,17,https://www.reddit.com/r/MachineLearning/comments/132qh4g/d_finetuning_llama_model_for_summarization/
[D] Can someone explain the discrepancy between the findings of LLaMA and Chinchilla?,[D] Can someone explain the discrepancy between the findings of LLaMA and Chinchilla? Chinchilla states that the model size/dataset ratio should be 1 to 20 and they show it experimentally. LLaMA states their 7B model continued to improve even after 1T tokens. That's 1 to 142. Has anyone figured it out?,NEGATIVE,0.9641525149345398,14,17,https://www.reddit.com/r/MachineLearning/comments/11l3as6/d_can_someone_explain_the_discrepancy_between_the/
[D] Grouped Query Attention in LLaMA 70B v2,"[D] Grouped Query Attention in LLaMA 70B v2 Hey guys, after thousands of experiments with bigger LLaMA fine-tunes I'm somewhat sure the GQA mechanism might be your enemy and generate wrong answers, especially for math and such complex areas.  
I'd like to use MHA (Multi Head Attention) if possbile. I'm just not sure - do I need to retrain model completely or is it possible to just increase heads count and KV size and proceed with the stock model AS IS?",NEGATIVE,0.999312162399292,3,8,https://www.reddit.com/r/MachineLearning/comments/17gvphb/d_grouped_query_attention_in_llama_70b_v2/
[D] fine tuning llama 70B ,"[D] fine tuning llama 70B   I‚Äôm trying to find a simple way to finetune llama 70b on 8gpu node. Could anyone point me to tutorials ? I have it working on a single gpu with 7b . I came across acceletate , but not sure how to wrap hugfingface trainer in accelerate . Thanks ! ",NEGATIVE,0.996009349822998,4,0,https://www.reddit.com/r/MachineLearning/comments/1bm3tk4/d_fine_tuning_llama_70b/
[D] Free Inference for code LLAMA 70B,[D] Free Inference for code LLAMA 70B Is there a provider who gives free inference for code llama 70B? I want to do some testing before I download it's lamma.cpp version into my local.,NEGATIVE,0.999495267868042,1,3,https://www.reddit.com/r/MachineLearning/comments/1ah510a/d_free_inference_for_code_llama_70b/
[D] Data Engineering for Scaling Language Models to 128K Context - MIT 2024 - New open LLaMA-2 7B and 13B with 128k context!,"[D] Data Engineering for Scaling Language Models to 128K Context - MIT 2024 - New open LLaMA-2 7B and 13B with 128k context! Paper: [https://arxiv.org/abs/2402.10171](https://arxiv.org/abs/2402.10171)

**Github:** [**https://github.com/FranxYao/Long-Context-Data-Engineering**](https://github.com/FranxYao/Long-Context-Data-Engineering) **New models with 128k context inside!**

Abstract:

>We study the continual pretraining recipe for scaling language models‚Äô context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular the ability to utilize information at arbitrary input locations, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training (e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. **We investigate the quantity and quality of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context**; (2) for quality, our results equally emphasize domain balance and **length upsampling**. Concretely, we find that nively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an **effective and affordable strategy for scaling the context length of language models to 128K.** Our recipe outperforms strong open-source longcontext models and closes the gap to frontier models like GPT-4 128K.

https://preview.redd.it/bedg1gsgixjc1.jpg?width=1447&format=pjpg&auto=webp&s=cdf15e90c375988b169fd24ffd5d4505da002593

https://preview.redd.it/2qy3dhsgixjc1.jpg?width=1837&format=pjpg&auto=webp&s=2ced604b9e1360ee8d170773a1a0600523288516

https://preview.redd.it/pebawhsgixjc1.jpg?width=1446&format=pjpg&auto=webp&s=4a57b8bb6685d6122d51a67e4fa9645555c51d5a

https://preview.redd.it/o8v3kisgixjc1.jpg?width=577&format=pjpg&auto=webp&s=6d39b7736dc9221ed69e1c61ca36f303e8ef131e",NEGATIVE,0.9847283363342285,10,1,https://www.reddit.com/r/MachineLearning/comments/1awagj3/d_data_engineering_for_scaling_language_models_to/
[D] LLaMA or Alpaca Weights,"[D] LLaMA or Alpaca Weights Was anyone able to download the LLaMA or Alpaca weights for the 7B, 13B and or 30B models? If yes please share, not looking for HF weights",NEGATIVE,0.9973868727684021,18,15,https://www.reddit.com/r/MachineLearning/comments/11zcog6/d_llama_or_alpaca_weights/
[P] vanilla-llama an hackable plain-pytorch implementation of LLaMA that can be run on any system (if you have enough resources),"[P] vanilla-llama an hackable plain-pytorch implementation of LLaMA that can be run on any system (if you have enough resources) I put together this plain pytorch implementation of LLaMA (i just substituted the fairscale layers with the native ones and converted the weights accordingly) that can be more easily run in different environments. 

The big problem with the official implementation is that in order to run the 65B version you need 8 GPUs no matter what, and to run the 30B version you need 4 and so on. In reality you can easily fit the 65B version in 2 A100 with 100G of VRAM.

vanilla-llama solves this problem. You just need to have enough memory and the model will be load in all the available GPUs.

&#x200B;

[https://github.com/galatolofederico/vanilla-llama](https://github.com/galatolofederico/vanilla-llama)",NEGATIVE,0.9993615746498108,84,8,https://www.reddit.com/r/MachineLearning/comments/11ozl85/p_vanillallama_an_hackable_plainpytorch/
Apache Airflow vs. LangChain and LlamaHub for LLM data pipeline [D],"Apache Airflow vs. LangChain and LlamaHub for LLM data pipeline [D] I‚Äôm looking for recommendations, suggestions, and/or good documentation that outlines which data pipeline would be best to ingest my private data (which will then be split into chunks/nodes for vector embeddings and so forth). Thank you in advance!",POSITIVE,0.9667227268218994,0,10,https://www.reddit.com/r/MachineLearning/comments/160lexg/apache_airflow_vs_langchain_and_llamahub_for_llm/
LlamaEdge 0.2.9 is released! More LLMs supported. Shell script now work with any of the 3000+ GGUF repos on Hugging Face.,LlamaEdge 0.2.9 is released! More LLMs supported. Shell script now work with any of the 3000+ GGUF repos on Hugging Face.,NEGATIVE,0.9735011458396912,9,1,https://x.com/realwasmedge/status/1752034261907947652?s=20
[D] Multimodal using Gemini and LlamaIndex,"[D] Multimodal using Gemini and LlamaIndex  With  Gemini and LlamaIndex, the possibilities for AI-driven applications are  truly limitless. In this article, we will implement a Multimodal use  case basic example using Gemini Pro Vision and LlamaIndex

**Introduction to Gemini and LlamaIndex**

Artificial  intelligence (AI) and large language models (LLMs) are at the forefront  of innovation in today's rapidly evolving world. As the demand for  smarter machines continues to grow, so does the need for AI models that  can understand and interact with various types of information.

Enter  Gemini, one of the latest breakthroughs at Google DeepMind. Gemini is a  cutting-edge AI model that seamlessly processes different data types,  including text, code, audio, images, and video. This multimodal  capability represents a significant step forward in creating intuitive  and useful AI, like an expert assistant.

LlamaIndex  ‚Äî a powerful orchestration framework that simplifies the integration of  diverse data sources. LlamaIndex enables enterprises to augment their  private data, incorporate it into LLMs for knowledge generation, and  perform complex operations with ease.

Photo by Mojahid Mottakin on Unsplash

Code Implementation: Gemini Pro Vision and LlamaIndex

Installation:

    !pip install llama-index 'google-generativeai>=0.3.0' 

>Get Free Gemini Pro API Key

To begin, the first step is to obtain a Gemini API key. You can do this by visiting [ai.google.dev](https://ai.google.dev/) and creating a new API key from the studio section. After obtaining the key, make sure to save it as an environment variable.

    import os from getpass import getpass  google_api_key = getpass() os.environ['GOOGLE_API_KEY'] = google_api_key 

Approach-1: Using Image URLS

In  our initial approach, we‚Äôll start by taking an image URL and converting  it into documents to be passed into the Gemini-Pro-Vision model. To  accomplish this, we‚Äôll utilize the LlamaIndex utils and leverage the  multimodal functionalities available.

    IMAGE_URL = ""https://p.imgci.com/db/PICTURES/CMS/371200/371206.4.jpg"" 

Let's also display the image, before we proceed.

    from PIL import Image import matplotlib.pyplot as plt  import requests from io import BytesIO  response = requests.get(IMAGE_URL) img = Image.open(BytesIO(response.content)) plt.imshow(img) plt.axis(""off"") 

Time to create the  document for the above image and pass it to the model. This is where  LlamaIndex stands out as a data framework. Once we define this, the user  can ask any prompt related to the image and run the complete function  from GeminiMultiModal.

&#x200B;

https://preview.redd.it/fjsvgbxyofhc1.png?width=418&format=png&auto=webp&s=9ccaccc35aaa91c6c8635592f88e21ad72d811c8

    from llama_index.multi_modal_llms.generic_utils import load_image_urls from llama_index.multi_modal_llms.gemini import GeminiMultiModal  image_document1 = load_image_urls([IMAGE_URL]) gemini_pro = GeminiMultiModal(model_name=""models/gemini-pro-vision"")  response = gemini_pro.complete(     prompt = ""name both the team names and also describe the image"",     image_documents = image_document1 ) print(response) 

Output

>The  image shows a cricket match between New Zealand and England. The New  Zealand player is the wicket-keeper and the England player is the  batsman. The wicket-keeper is taking the bails off the stumps to appeal  for a wicket. The batsman is out of his crease and is trying to get back  in time. The umpire is signaling that the batsman is out.

Approach-2: Using Local Files

For  local files, we can directly use Simple Directory Reader. To use this,  we need to create a directory and store all the images within it. Once  we load the Simple Directory Reader, it automatically generates the  document for the image.

&#x200B;

https://preview.redd.it/1fu8t0b0pfhc1.png?width=573&format=png&auto=webp&s=eab3b2b03d0d3a2abb7050ba254f0237f9b9c550

In  my case, I have created a folder by the name data, inside that I have  added Naruto and Luffy pic (popular Anime Main characters).

    from llama_index import SimpleDirectoryReader  image_document2 = SimpleDirectoryReader(""./data"").load_data()  response2 = gemini_pro.complete(     prompt = ""who are the characters and whats so special about them"",     image_documents=image_document2 

Output:

>Monkey  D. Luffy is the main protagonist of the anime series One Piece. He is a  cheerful, optimistic young man who dreams of becoming the Pirate King.  He is highly loyal to his friends and crew and is always willing to  fight for what he believes in.

Naruto  Uzumaki is the main protagonist of the anime series Naruto. He is a  loud, hyperactive ninja who dreams of becoming the Hokage, the leader of  his village. He is a loyal friend and teammate and is always willing to  help those in need.

Conclusion

*By  leveraging multimodal functionalities and innovative techniques such as  data augmentation and vectorization, these tools empower developers to  create intelligent systems that can understand, interact with, and  derive insights from complex data sources. With the potential to  revolutionize various industries, from healthcare to finance, Gemini and  LlamaIndex represent a promising step towards realizing the vision of  responsibly empowered AI for the betterment of humanity.*",POSITIVE,0.9273443818092346,2,1,https://www.reddit.com/r/MachineLearning/comments/1am7kda/d_multimodal_using_gemini_and_llamaindex/
[D] llama 7b vs 65b ?,"[D] llama 7b vs 65b ? Hello

what are we talking in term of diminishing returns between the 2 models ?

do the 65b really improve a lot ?

bonus question: how to train the 7b model to learn specific field on my computer ? (makin it tailored to my needs)",NEGATIVE,0.998816728591919,8,15,https://www.reddit.com/r/MachineLearning/comments/125q87z/d_llama_7b_vs_65b/
[P] MergeLlama-7b - A fine tune of CodeLlama for resolving merge conflicts,"[P] MergeLlama-7b - A fine tune of CodeLlama for resolving merge conflicts Merge conflicts are something that give developers hours of headaches and I figured I would try and give my take on a solution. I followed a paper from IEEE engineers in 2022 who trained CodeBert on merge conflicts as a classification task, and they published their dataset for public use.

Input formatted as ‚Äú<<<<<<< A ======= B >>>>>>>‚Äù will output the attempted conflict resolution. I am still trying to find out how to do evaluations on this model as the loss applies to all sections not just the resolution, and the TRL Trainer with a data collator gives NaN as a loss.

The model and dataset are on HuggingFace under codys12/MergeLlama and codys12/MergeLlama-7b.

Any feedback is appreciated!",NEGATIVE,0.9983844757080078,15,4,https://www.reddit.com/r/MachineLearning/comments/1795yej/p_mergellama7b_a_fine_tune_of_codellama_for/
[Project] godot-dodo - Finetuning LLaMA on single-language comment:code data pairs,"[Project] godot-dodo - Finetuning LLaMA on single-language comment:code data pairs [GitHub Repository (godot-dodo)](https://github.com/minosvasilias/godot-dodo)

This repository presents finetuned LLaMA models that try to address the limited ability of existing language models when it comes to generating code for less popular programming languages.  


`gpt-3.5-turbo` and `gpt-4` have proven to be excellent coders, but fall off sharply when asked to generate code for languages other than `Python`/`Javascript` etc.   
The `godot-dodo` approach to address this: Finetune smaller models on a single one of these languages, using human-created code scraped from MIT-licensed GitHub repositories, with existing GPT models generating instructions for each code snippet. 

This differs from the dataset generation approach used by projects such as `stanford-alpaca` or `gpt4all`, in that the output values of the training set remain high quality, human data, while following the same instruction-following behavior. This will likely prove more effective the more obscure the language. In this case, `GDScript` was used, which is the scripting language for the popular open-source game-engine Godot. The same approach however can be applied to any other language.  


Performance is promising, with the 7 billion parameter finetune outperforming GPT models in producing syntax that compiles on first try, while being somewhat less capable at following complex instructions.  


A comprehensive evaluation comparing all models can be found here:  
[https://github.com/minosvasilias/godot-dodo/tree/main/models](https://github.com/minosvasilias/godot-dodo/tree/main/models)",NEGATIVE,0.9982951283454895,33,11,https://www.reddit.com/r/MachineLearning/comments/12wpig9/project_godotdodo_finetuning_llama_on/
Meta‚Äôs Code Llama AI coding tool just got a big performance boost,Meta‚Äôs Code Llama AI coding tool just got a big performance boost,POSITIVE,0.9982097148895264,0,1,https://www.itpro.com/technology/artificial-intelligence/metas-code-llama-ai-coding-tool-just-got-a-big-performance-boost
[D] transformers vs llama.cpp vs GPTQ vs GGML vs GGUF,"[D] transformers vs llama.cpp vs GPTQ vs GGML vs GGUF i am a little puzzled,

1. i know that transformers is the HF framework/library to load infere and train models easily
2. and that llama.cpp is another framework/library that does the more of the same but specialized in models that runs on CPU and quanitized and run much faster
3. i understand that GGML is a file format for saving model parameters in a single file, that its an old problematic format, and GGUF is the new kid on the block, and GPTQ is the same quanitized file format for models that runs on GPU

&#x200B;

so here is what i can't understand (assuming i got all the rest correct): 

1. does HF Transformers support loading GGUF or GGML models ? 
2. and does GGUF needs a tokenizer json or does the data comes from within the gguf file itself
3. and is safetensors (another file format) supported by both Transformers and Llama.cpp

&#x200B;

since i cannot find python examples for these combination i assume all the answers are - No

&#x200B;

can anyone shed some light ?",NEGATIVE,0.9983634352684021,21,4,https://www.reddit.com/r/MachineLearning/comments/1785hht/d_transformers_vs_llamacpp_vs_gptq_vs_ggml_vs_gguf/
[P] Tutorial: 2MB inference app to Run any Hugging Face LLMs with LlamaEdge on any Devices,"[P] Tutorial: 2MB inference app to Run any Hugging Face LLMs with LlamaEdge on any Devices a step-by-step guide to run a newly open sourced model as long as it is in GGUF format across devices.

[https://www.secondstate.io/articles/selfhost-huggingface-llms/](https://www.secondstate.io/articles/selfhost-huggingface-llms/)",NEGATIVE,0.9953505992889404,4,0,https://www.reddit.com/r/MachineLearning/comments/1ak7ywb/p_tutorial_2mb_inference_app_to_run_any_hugging/
[R] Benchmarking g5.12xlarge (4xA10) vs 1xA100 inference performance running upstage_Llama-2-70b-instruct-v2 (4-bit & 8-bit),"[R] Benchmarking g5.12xlarge (4xA10) vs 1xA100 inference performance running upstage_Llama-2-70b-instruct-v2 (4-bit & 8-bit) Hi Reddit folks, I wanted to share some benchmarking data I recently compiled running [upstage\_Llama-2-70b-instruct-v2](https://huggingface.co/upstage/Llama-2-70b-instruct-v2) on two different hardware setups. If you'd like to see the spreadsheet with the raw data you can [check out this link](https://docs.google.com/spreadsheets/d/16t-j5BBbABqd-_FYD3NpA5mNnvivk_KhgvETeOhQado/edit?usp=sharing).  
**Hardware Config #1**: AWS g5.12xlarge - 4 x A10 w/ 96GB VRAM  
**Hardware Config #2**: Vultr - 1 x A100 w/ 80GB VRAM  
**A few questions I wanted to answer:**

1. How does the inference speed (tokens/s) between these two configurations compare?
2. How does the number of input tokens impact inference speed?
3. How many input tokens can these machines handle before they start to hit OOM?
4. How does 4-bit vs 8-bit quantization affect all of the above?

  
**Why this model?**  
I chose [upstage\_Llama-2-70b-instruct-v2](https://huggingface.co/upstage/Llama-2-70b-instruct-v2) because it's the current #1 performing OS model on [HuggingFace's LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). Also, according to the documentation the model is able to support 10K+ tokens using RoPE which is allowed me to push memory on the machines to the point of OOM.  
**Why this hardware?**  
I have some projects I'm working on that will require high performance LLMs and these are the two most common configurations that we're considering. We do most of our cloud work on AWS so the g5.12xlarge is the ""go to"" option for inference with a model of this size. However, I have been very interested in understanding if there are compelling reasons to go with a 1xA100 setup which AWS doesn't offer.

**Text Generation Performance (t/s) vs Input Tokens (t)**

This chart shows how Text Generation Performance (t/s) responds to the number of input tokens (t) sent to the model. As expected, more input tokens results in slower generation speed.

https://preview.redd.it/rdd82awxsbhb1.png?width=1202&format=png&auto=webp&s=432a539084296e9d72b31464ed72fa1e3c856148

  
**GPU Load Performance (MM:SS)**  
This is a measure of how long it took to load the model into memory. I averaged this across 5 load attempts for each configuration.

|*Hardware*|*8-Bit GPU Load Time*|*4-Bit GPU Load Time*|
|:-|:-|:-|
|**g5.12xlarge (4xA10)**|0:59|1:00|
|**1xA100**|2:47|2:54|

  
**Average Text Generation Performance (tokens/second)**  
Note that these numbers are an average across all text generation attempts for each configuration.

|*Hardware*|*8-Bit Avg Text Generation Performance (tokens/second)*|*4-Bit Avg Text Generation Performance (tokens/second)*|
|:-|:-|:-|
|**g5.12xlarge (4xA10)**|2.07 t/s|4.08 t/s|
|**1xA100**|2.28 t/s|4.54 t/s|

  
**Maximum Context (tokens)**  
This was a measure of how many input tokens I could pass into the model before getting an OOM exception for each configuration.

|*Hardware*|*8-Bit Maximum Context (tokens)*|*4-Bit Maximum Context (tokens)*|
|:-|:-|:-|
|**g5.12xlarge (4xA10)**|2500 tokens|5500 tokens|
|**1xA100**|3000 tokens|8000 tokens|

  
**Summary**  
On text generation performance the A100 config outperforms the A10 config by \~11%. I was surprised to see that the A100 config, which has less VRAM (80GB vs 96GB), was able to handle a larger context size before hitting OOM errors. Additionally, it was interesting to see the A10 hardware was much faster at loading the model. I would presume this is because it can parallelize the load across the 4 separate GPUs. Unsurprisingly, 4-bit quantized models were much faster than 8-bit quantized models (almost 2x) and they were able to handle much larger context sizes before OOM.",NEGATIVE,0.9955419898033142,30,6,https://www.reddit.com/r/MachineLearning/comments/15nkdq2/r_benchmarking_g512xlarge_4xa10_vs_1xa100/
[P] Fine-Tuning LLama,"[P] Fine-Tuning LLama Hi all. I have a 16GB Macbook M2 and I would like to fine-tune LLaMa on some synthetic data that requires adding new special tokens to the model. The task at hand is not too complicated so I am hoping one of the small models should be fine.

1. Do you think it is possible that I run and fine-tune locally in a reasonable amount of time? I suspect to train on 10-100k tokens.
2. If it is not possible to do this locally, how do you suggest I proceed? Something that is easy to setup would be great as time is the most important concern here.

Thanks in advance.",NEGATIVE,0.9711911082267761,1,0,https://www.reddit.com/r/MachineLearning/comments/1anwrt3/p_finetuning_llama/
[N] Beating GPT-4 on HumanEval with a Fine-Tuned CodeLlama-34B,"[N] Beating GPT-4 on HumanEval with a Fine-Tuned CodeLlama-34B Blog: [https://www.phind.com/blog/code-llama-beats-gpt4](https://www.phind.com/blog/code-llama-beats-gpt4)

Models:

[https://huggingface.co/Phind/Phind-CodeLlama-34B-Python-v1](https://huggingface.co/Phind/Phind-CodeLlama-34B-Python-v1)

[https://huggingface.co/Phind/Phind-CodeLlama-34B-v1](https://huggingface.co/Phind/Phind-CodeLlama-34B-v1)",POSITIVE,0.6536309719085693,20,6,https://www.reddit.com/r/MachineLearning/comments/161uiz8/n_beating_gpt4_on_humaneval_with_a_finetuned/
‚ÄòMeta has stolen books‚Äô: authors to protest in London against AI trained using ‚Äòshadow library‚Äô,‚ÄòMeta has stolen books‚Äô: authors to protest in London against AI trained using ‚Äòshadow library‚Äô,NEGATIVE,0.9934775829315186,751,31,https://www.theguardian.com/books/2025/apr/03/meta-has-stolen-books-authors-to-protest-in-london-against-ai-trained-using-shadow-library
"[P] OnnxStream running TinyLlama and Mistral 7B, with CUDA support","[P] OnnxStream running TinyLlama and Mistral 7B, with CUDA support hi,  
I'm the author.  
I'm interested in opinions on this possible development of OnnxStream.  
URL: https://github.com/vitoplantamura/OnnxStream/blob/master/assets/LLM.md  
Thanks, Vito",POSITIVE,0.992620050907135,1,0,https://www.reddit.com/r/MachineLearning/comments/196mhx1/p_onnxstream_running_tinyllama_and_mistral_7b/
OpenAI's new model leaped 30 IQ points to 120 IQ - higher than¬†9¬†in¬†10¬†humans,OpenAI's new model leaped 30 IQ points to 120 IQ - higher than¬†9¬†in¬†10¬†humans,NEGATIVE,0.8809946179389954,317,161,https://i.redd.it/v3k09fj4e0pd1.jpeg
[D] LLaMA training vs. GPU time: smaller models seem better for a given budget,[D] LLaMA training vs. GPU time: smaller models seem better for a given budget,NEGATIVE,0.9984216690063477,12,8,https://espadrine.github.io/blog/posts/chinchilla-s-death.html#Can_Chinchillas_picture_a_Llama_s_sights_
Meta's Llama is not open source,Meta's Llama is not open source,NEGATIVE,0.9994000196456909,4,8,https://www.theregister.com/2023/07/21/llama_is_not_open_source/
[P] ChatLLaMA - A ChatGPT style chatbot for Facebook's LLaMA,"[P] ChatLLaMA - A ChatGPT style chatbot for Facebook's LLaMA üëã  Hey all, we just launched [ChatLLaMA](https://chatllama.baseten.co/). An experimental chatbot interface for interacting with variants of Facebook's LLaMa. Currently, we support the 7 billion parameter variant that was fine-tuned on the Alpaca dataset. This early version isn't as conversational as we'd like, but over the next week or so, we're planning on adding support for the 30 billion parameter variant, another variant fine-tuned on LAION's OpenAssistant dataset and more as we explore what this model is capable of.

If you want deploy your own instance is the model powering the chatbot and build something similar we've open sourced the Truss here: [https://github.com/basetenlabs/alpaca-7b-truss](https://github.com/basetenlabs/alpaca-7b-truss)

We'd love to hear any feedback you have!

[Check it out here](https://chatllama.baseten.co/)",NEGATIVE,0.8228779435157776,28,11,https://www.reddit.com/r/MachineLearning/comments/11yof4h/p_chatllama_a_chatgpt_style_chatbot_for_facebooks/
[D] LLaMa-2 and BERTScore,"[D] LLaMa-2 and BERTScore I have a couple of questions:

1. Why wasn't BERTScore one of the metrics used to evaluate Llama-2's performance on free-form response based tasks?
2. Does anyone think it's worth trying to produce those results?",NEGATIVE,0.9988846182823181,15,7,https://www.reddit.com/r/MachineLearning/comments/15gongf/d_llama2_and_bertscore/
What is Llama 2? Meta‚Äôs large language model explained,What is Llama 2? Meta‚Äôs large language model explained,NEGATIVE,0.9713431596755981,3,5,https://www.infoworld.com/article/3706470/what-is-llama-2-metas-large-language-model-explained.html
[D] LLaMA Model Parallelization and Server Configuration,"[D] LLaMA Model Parallelization and Server Configuration Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!",NEGATIVE,0.999401330947876,17,11,https://www.reddit.com/r/MachineLearning/comments/11iul6f/d_llama_model_parallelization_and_server/
[D] Is it possible to train LLaMa?,"[D] Is it possible to train LLaMa? Most AI is impossible to train(like chat GPT)

Dose LLaMa can be trained? 

Although the dataset is very hard to get, It would be nice if LLaMa can be trained.

When searching for reddit, this topic cannot be searched, so I hope it becomes a discuss about HW or availability.  
Thank you.",NEGATIVE,0.9863957166671753,0,14,https://www.reddit.com/r/MachineLearning/comments/11nhl03/d_is_it_possible_to_train_llama/
[P] llama.cpp GGUF inference with a single LLM pipeline,"[P] llama.cpp GGUF inference with a single LLM pipeline &#x200B;

https://preview.redd.it/i4rxpfwcdtac1.jpg?width=1296&format=pjpg&auto=webp&s=62c2fa0a8d724bfcaa5a21a2e40b7343396bc16f

[txtai](https://github.com/neuml/txtai) has a unified LLM pipeline that can load Hugging Face models, llama.cpp GGUF files and LLM APIs. The example above downloads a GGUF file from the Hugging Face Hub and runs inference with the model.

See this article for more: [https://neuml.hashnode.dev/integrate-llm-frameworks](https://neuml.hashnode.dev/integrate-llm-frameworks)",NEGATIVE,0.9911091327667236,2,0,https://www.reddit.com/r/MachineLearning/comments/18zz27k/p_llamacpp_gguf_inference_with_a_single_llm/
"[D] off-topic, is Meta Llama 2 license agreement safe to sign for commercial use ?","[D] off-topic, is Meta Llama 2 license agreement safe to sign for commercial use ? in the Meta Llama 2 license agreement ([that can be found here](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)), there is a section of ""**Prohibited Uses**"" that clearly states several use cases that the signer must accept upon himself, but several of them state the word ""**facilitate**"", as far as i can understand, if we use Llama 2 as part of a commercial product, and some end-user will use the product in malicious way (say cause the chat-bot to write the recipe of mustard gas) then this could be considered that the creator of the product is  facilitating the end-user,

&#x200B;

so my questions are:

1. do you think this is a fair interpretation of the agreement ?
2. does that mean the creator is liable to whatever the model spit out ?
3. is there a way to censor the model (short of retraining a new model, or fine-tune on a large scale) ?
4. is there an open source model that already gone through the process, and more safe for commercial use ?

&#x200B;

https://preview.redd.it/3zo3tm4e8esb1.png?width=1197&format=png&auto=webp&s=8aa522183f82ba8f85edb69cbaabd93262efd516

&#x200B;",NEGATIVE,0.997900128364563,3,4,https://www.reddit.com/r/MachineLearning/comments/170jah3/d_offtopic_is_meta_llama_2_license_agreement_safe/
decapoda-research llama models removed from HuggingFace? [D],"decapoda-research llama models removed from HuggingFace? [D] Is anyone else no longer able to access the standard \`decapoda-research\` LLaMA models on HuggingFace? E.g., this [\[link\]](https://huggingface.co/decapoda-research) shows no models publicly available. Has there been any news or announcements about this? ",NEGATIVE,0.9997842907905579,10,2,https://www.reddit.com/r/MachineLearning/comments/17j8a7h/decapodaresearch_llama_models_removed_from/
[D] Is there already a way to use Llama 2 with a very big system prompt?,"[D] Is there already a way to use Llama 2 with a very big system prompt?  

I've seen something like that:  
[https://together.ai/blog/llama-2-7b-32k](https://together.ai/blog/llama-2-7b-32k)  
Is there a way to use llama 2 13b chat or 70b chat with 32k prompt? If not what are the alternatives? Would that: [https://youtu.be/ypzmPwLH\_Q4?feature=shared](https://youtu.be/ypzmPwLH_Q4?feature=shared) be the best thing to do?

I'm trying to create a chat bot that would have a pretty specific exeprtise. For example: I would like to feed in soccer rules and then make the bot answear questions about soccer. The system prompt is amazing, but is very limited.",NEGATIVE,0.9991419315338135,4,5,https://www.reddit.com/r/MachineLearning/comments/164cfuc/d_is_there_already_a_way_to_use_llama_2_with_a/
"""We find that GPT-4o values its own wellbeing above that of a middle-class American. Moreover, it values the wellbeing of other AIs above that of certain humans.""","""We find that GPT-4o values its own wellbeing above that of a middle-class American. Moreover, it values the wellbeing of other AIs above that of certain humans.""",POSITIVE,0.994750440120697,175,119,https://i.redd.it/b69su253kqie1.png
[P] A Guide to Building LLM-Based Applications with Code Llama,"[P] A Guide to Building LLM-Based Applications with Code Llama Have you ever wondered about how to take advantage of the power of large language models (LLMs) and Generative AI at the edge? 

Our latest blog, [*A Guide to Building LLM-Based Applications with Code Llama*](https://www.modzy.com/modzy-blog/a-guide-to-building-llm-powered-applications-with-code-llama)*,* shows you how you can use Code Llama on an edge device to build a customized dashboard application. This tutorial shows how Code Llama can empowering analysts in remote, restricted environments to build applications in environments with minimal connectivity and compute capacity. 

In this tutorial, we‚Äôll walk you through how to run code Llama on an edge device in a remote location to build a customized dashboard application.",POSITIVE,0.964409351348877,0,3,https://www.reddit.com/r/MachineLearning/comments/17au0w8/p_a_guide_to_building_llmbased_applications_with/
"üßêKai-Fu Lee's LLM: A LlaMA Lookalike, China's LLM Overload, and US-China AI Risk Talk Begins","üßêKai-Fu Lee's LLM: A LlaMA Lookalike, China's LLM Overload, and US-China AI Risk Talk Begins",NEGATIVE,0.9859611988067627,5,1,https://recodechinaai.substack.com/p/kai-fu-lees-llm-a-llama-lookalike
[D] demystifying-llama-index-with-zephyr-7b-alpha-beta-a-pydantic-powered-query-engine-guide,"[D] demystifying-llama-index-with-zephyr-7b-alpha-beta-a-pydantic-powered-query-engine-guide Please read, clap and follow:  https://ai.plainenglish.io/demystifying-llama-index-with-zephyr-7b-alpha-beta-a-pydantic-powered-query-engine-guide-fb355170b209",NEGATIVE,0.9973534345626831,0,2,https://www.reddit.com/r/MachineLearning/comments/17lyudi/d/
[D] RAG Evaluation: LlamaIndex is generally better than OpenAI Assistants,"[D] RAG Evaluation: LlamaIndex is generally better than OpenAI Assistants TLDR; OpenAI's Assistants API: Showcases high efficiency with single documents but faces challenges when managing multiple documents. LlamaIndex: Exhibits consistent performance across both single and multiple document formats, outshining OpenAI in more complex scenarios.

[https://www.tonic.ai/blog/rag-evaluation-series-validating-rag-performance-openai-vs-llamaindex](https://www.tonic.ai/blog/rag-evaluation-series-validating-rag-performance-openai-vs-llamaindex)",POSITIVE,0.9550265073776245,11,0,https://www.reddit.com/r/MachineLearning/comments/181l7yr/d_rag_evaluation_llamaindex_is_generally_better/
"[P] Coding LLaMA 2 from scratch in PyTorch, with step by step explanation of KV Cache, Grouped Query Attention, Rotary Positional Embedding, RMS Normalization, SwiGLU and much more!","[P] Coding LLaMA 2 from scratch in PyTorch, with step by step explanation of KV Cache, Grouped Query Attention, Rotary Positional Embedding, RMS Normalization, SwiGLU and much more!",NEGATIVE,0.9926324486732483,37,1,https://www.youtube.com/watch?v=oM4VmoabDAI
Would I be able to run ggml models such as whisper.cpp or llama.cpp on a raspberry pi with a coral ai USB Accelerator?,"Would I be able to run ggml models such as whisper.cpp or llama.cpp on a raspberry pi with a coral ai USB Accelerator? This is a project that I'm working on: making a type of ""Alexa"", ""Hey Google"", or ""Siri"" for my workplace. I'm very new to AI and am looking forward to learning a lot. I thought initially to use different models that interact together to create such a voice assistant. For example, I would use Whisper.cpp to transcribe audio, then send the text to Llama.cpp, and then use a text-to-speech software to reply. I want to do this all on a raspberry pi 3 B2 (it's what I have available). 

However, a pi doesn't have the strength to run something like Llama.cpp, of course, so I've been considering using something like the Coral USB Accelerator ([https://coral.ai/products/accelerator](https://coral.ai/products/accelerator)). As I've been learning more about it, it seems to be very geared towards TensorFlow Lite models. But whisper.cpp and Llama.cpp use ggml models.

**Here are my questions:**

1. Could the coral ai USB Accelerator run ggml models and, if so, how?
2. Is there a better system to creating a local (no 3rd party api) at-home assistant?

Please let me know if I could do something better and what that thing is. I'd appreciate all sorts of advice. Thank you!

**Links**

1. Coral USB Accelerator [https://coral.ai/products/accelerator](https://coral.ai/products/accelerator)
2. Whisper.cpp [https://github.com/ggerganov/whisper.cpp](https://github.com/ggerganov/whisper.cpp)
3. Llama.cpp [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)",NEGATIVE,0.6820738911628723,4,7,https://www.reddit.com/r/artificial/comments/14fm598/would_i_be_able_to_run_ggml_models_such_as/
[D] CodeLlama-xb/CodeLlama-xb-Python vs. CodeLlama-xb-instruct,"[D] CodeLlama-xb/CodeLlama-xb-Python vs. CodeLlama-xb-instruct Hey guys, so I have googled around and read the documentation but I am still confused between what's the difference between CodeLlama-xb/CodeLlama-xb-Python vs. CodeLlama-xb-instruct? I know the xb model is the base model (for several languages) and the Python model specializes in Python, but what's the instruct model and how is it different from the other 2 models?

Would really appreciate your help. Thanks a million!",NEGATIVE,0.955445408821106,1,4,https://www.reddit.com/r/MachineLearning/comments/16etk1f/d_codellamaxbcodellamaxbpython_vs/
[R] RO-LLaMA: Generalist LLM for Radiation Oncology via Noise Augmentation and Consistency Regularization,[R] RO-LLaMA: Generalist LLM for Radiation Oncology via Noise Augmentation and Consistency Regularization,NEGATIVE,0.9725677371025085,4,0,https://arxiv.org/abs/2311.15876
Did you get access to Meta AI's LLAMA? [Discussion],"Did you get access to Meta AI's LLAMA? [Discussion] Many have been granted access to Meta AI's LLAMA, while others are questioning whether access is currently limited to email domains with the '.edu' extension. This poll aims to determine who has been granted access based on the email domain they provided. 

[View Poll](https://www.reddit.com/poll/11hxvib)",NEGATIVE,0.9973998069763184,7,11,https://www.reddit.com/r/MachineLearning/comments/11hxvib/did_you_get_access_to_meta_ais_llama_discussion/
[R] - An Open-sourced Data Contamination Reports for Llama Series Models,"[R] - An Open-sourced Data Contamination Reports for Llama Series Models # Data Contamination in Multi-choice QA Benchmarks

[How much test samples are included in Llama's training data?](https://preview.redd.it/cyub5cioslxb1.png?width=1792&format=png&auto=webp&s=fe68dc73862e475fc2b4cc3d8554b33d5e2da088)

This presented how much test samples in popular Multi-Choise QA benchmarks are included in the training data of Llama models (Common Crawl 2017‚Äì2020).

Three types of data contamination: input-only contamination, input-and-label contamination, and all contamination containing both.

**Input-only contamination** represents contaminations where only input part of test samples was included in the training data. On the contrary, **input-and-label contamination** indicate both input and the answer were included in the training data.

# Impact on Model Performance

[How much data contamination affects model evaluation?](https://preview.redd.it/3j8vjultslxb1.png?width=1280&format=png&auto=webp&s=75ca1d879b862eeaa4db3467069c71007047d521)

The full open sourced data contamination report: [https://arxiv.org/abs/2310.17589](https://arxiv.org/abs/2310.17589)

All data and code: [https://github.com/liyucheng09/Contamination\_Detector](https://github.com/liyucheng09/Contamination_Detector)",NEGATIVE,0.9976831674575806,3,1,https://www.reddit.com/r/MachineLearning/comments/17kviz3/r_an_opensourced_data_contamination_reports_for/
[P] Ways to speed up llama-2 summarization on sagemaker?,"[P] Ways to speed up llama-2 summarization on sagemaker? I'm currently working on a project to give a quick summary of long articles/conversations.

I'm running llama-2-7b-chat-hf with 4bit quantization on a g5.2xlarge instance on sagemaker.

The method I'm using is map\_reduce (option 2)from this webpage [https://python.langchain.com/docs/use\_cases/summarization](https://python.langchain.com/docs/use_cases/summarization))

Of everything I've tried this is the only one that's been able to do decent summaries in a reasonable amount of time. However with really long articles (10,000+ words) it takes \~6 minutes before giving an output.

I tried running this same thing on a g5.12xlarge instance which has 4 A10G gpus but it hasn't reduced the time by any noticeable amount.

Is there anything else I could be doing to speed this up?

&#x200B;

For reference here is the code I'm running in Sagemaker notebook

[https://gist.github.com/phwang4/1ab4d772228b6fff8616c28ac054c229](https://gist.github.com/phwang4/1ab4d772228b6fff8616c28ac054c229)",NEGATIVE,0.9987916350364685,1,3,https://www.reddit.com/r/MachineLearning/comments/16iutyp/p_ways_to_speed_up_llama2_summarization_on/
[Project] Using multiple GPUs for evaluation during fine-tuning of llama-2-7b,"[Project] Using multiple GPUs for evaluation during fine-tuning of llama-2-7b Hi, I am currently working on finetuning the llama-2-7b model on my own custom dataset using QLoRA. Right now, I have access to 4 Nvidia A100 GPUs, with 40GB memory each. I am training for 20000 steps, and realized that the training is going by very quickly (using multiple GPUs), while the evaluation is taking a very long time at each evaluation step (i'm assuming it is only using one GPU). My dataset has around 144k rows, and the evaluation data has around 17k rows (which is significantly less). It is taking around 4 minutes to go through 100 train steps, but 35 minutes to evaluate the model at each eval\_step. Has anyone run into this issue before?

I'm loading my model using device\_map=""auto"" for distributing the model weights across the GPUs:

    model = AutoModelForCausalLM.from_pretrained(
            llm_name,
            quantization_config=bnb_config,
            trust_remote_code=True,
            torch_dtype=torch.float16, # if not use_bf16 else torch.bfloat16,
            device_map = ""auto"" 
        )

Below are my training arguments:

    output_dir = ""./results""
        per_device_train_batch_size = 22 
        per_device_eval_batch_size = 22 
        gradient_accumulation_steps = 1 
        eval_accumulation_steps = 1
        optim = ""paged_adamw_32bit""
        save_steps = 1000 
        logging_steps = 1
        learning_rate = 0.00002 
        max_grad_norm = 0.3
        max_steps = 20000
        warmup_ratio = 0.03
        lr_scheduler_type = ""constant""
    
        training_arguments = TrainingArguments(
            report_to=""wandb"",
            output_dir=output_dir,
            per_device_train_batch_size=per_device_train_batch_size,
            per_device_eval_batch_size=per_device_eval_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            eval_accumulation_steps = eval_accumulation_steps,
            optim=optim,
            save_steps=save_steps,
            logging_steps=logging_steps,
            learning_rate=learning_rate,
            fp16=True,
            max_grad_norm=max_grad_norm,
            max_steps=max_steps,
            warmup_ratio=warmup_ratio,
            group_by_length=True,
            lr_scheduler_type=lr_scheduler_type,
            #do_eval=True,                 
            evaluation_strategy=""steps"",
            eval_steps= 1000           
        )

Finally, here is my SFTTrainer:

    trainer = SFTTrainer(
            model=model,
            train_dataset=fineune_dataset_dict['train'],
            eval_dataset=fineune_dataset_dict['validation'],
            peft_config=peft_config,
            dataset_text_field=""text"",
            max_seq_length=max_seq_length,
            tokenizer=tokenizer,
            args=training_arguments
        )
    
        for name, module in trainer.model.named_modules():
            if ""norm"" in name:
                module = module.to(torch.float32)
    
        trainer.train()

Please let me know how I can improve the speed of the evaluation, given that I have access to multiple GPUs.",NEGATIVE,0.9982243180274963,1,0,https://www.reddit.com/r/MachineLearning/comments/1802um6/project_using_multiple_gpus_for_evaluation_during/
[P] Open Sourcing Llmtuner - An Experimental Framework for Finetuning Large Models Like Whisper and Llama with scikit-learn-inspired interface,"[P] Open Sourcing Llmtuner - An Experimental Framework for Finetuning Large Models Like Whisper and Llama with scikit-learn-inspired interface Hi Folks,

Happy to share an open source side project I've been working on -  [LLmtuner](https://github.com/promptslab/LLMtuner). It's a framework for finetuning large models like Whisper, Llama, Llama-2, etc with best practices like LoRA, QLoRA, through a sleek, scikit-learn-inspired interface.

As someone who works with Large Models a lot, I found myself writing a lot of boilerplate code every time I wanted to finetune a model. Llmtuner aims to simplify the finetuning process down to just 2-3 lines to get training started, similar to scikit-learn.  


[Sample usecase](https://preview.redd.it/3coqpv9a1fyb1.png?width=1502&format=png&auto=webp&s=d9965b8e7f65b5940e73060958514a89a2fe4ea3)

[Supported Models](https://preview.redd.it/78cai3gb1fyb1.png?width=1098&format=png&auto=webp&s=5d898a9ff75a8dda3e0da5f912cfec80de456ad9)

  
üöÄ Features:

* üßô‚Äç‚ôÄÔ∏è Finetune state-of-the-art LLMs like **Whisper, Llama wit**h minimal code
* üî® Built-in utilities for techniques like **LoRA and QLoRA**
* ‚úå **Launch webapp demos** for your finetuned models with one click
* üí• **Fast inference** without separate code
* **üåê Easy model sharing** and deployment coming soon

  
This is still experimental code I've been using for personal projects. I thought others might find it useful too so decided to open-source it.   


* Github : [https://github.com/promptslab/LLMtuner](https://github.com/promptslab/LLMtuner)
* For quick demo : [Colab](https://colab.research.google.com/drive/1ia9KvqEGOxARtJScPBY6ccF8l41-w_l5?usp=sharing)

Contributions and feedback are very welcome! I hope it will be helpful in your research & projects. Have a good weekend, Thanks :)",POSITIVE,0.9991082549095154,8,0,https://www.reddit.com/r/MachineLearning/comments/17nysgz/p_open_sourcing_llmtuner_an_experimental/
[D] Is there an online LLaMA model that supports plugging in embeddings directly?,"[D] Is there an online LLaMA model that supports plugging in embeddings directly? Hi,

I'm doing some work with using multi-modal data with LLaMA, for example, Video-LLaMA, which converts images/videos into embeddings, concatenates it with the text embeddings, and feeds it into LLaMA. It's difficult for me to run some of the models myself because of computational constraints. I'm wondering if there is an online demo that supports inputting embeddings directly (as opposed to text tokens).

To clarify the title, I meant an online demo, not the weights.",NEGATIVE,0.9989842772483826,2,1,https://www.reddit.com/r/MachineLearning/comments/17h7d9u/d_is_there_an_online_llama_model_that_supports/
[D] How does Llama-2 perform in sentiment analysis?,"[D] How does Llama-2 perform in sentiment analysis? Hey guys, if you have explored using Llama-2 in doing sentiment analysis, just wanted to get your experience in how Llama-2 perform in this task?

I have tried using GPT and it‚Äôs pretty accurate.

If Llama-2 isn‚Äôt all that good in sentiment analysis, which other open LLM would you recommend? 

Thank heaps!",POSITIVE,0.9980887770652771,4,3,https://www.reddit.com/r/MachineLearning/comments/16bcq8t/d_how_does_llama2_perform_in_sentiment_analysis/
[Project] Simple FastAPI service to serve LLAMA-2 7B chat model,"[Project] Simple FastAPI service to serve LLAMA-2 7B chat model Hey,

I wrote a simple FastAPI service to serve the LLAMA-2 7B chat model for our internal usage (just to avoid using chatgpt in our prototypes).

I thought it could also be beneficial for you to use it if needed.

Feel free to play with it [https://github.com/mowa-ai/llm-as-a-service](https://github.com/mowa-ai/llm-as-a-service)

Tested on Nvidia L4 (24GB) with \`g2-standard-8\` VM at GCP.

&#x200B;

Any feedback welcome :)",NEGATIVE,0.99836665391922,12,3,https://www.reddit.com/r/MachineLearning/comments/15snxgw/project_simple_fastapi_service_to_serve_llama2_7b/
[P] I made a finetune of CodeLlama to resolve merge conflicts!,[P] I made a finetune of CodeLlama to resolve merge conflicts! I made a finetune of CodeLlama-7b for resolving merge conflicts following up on an [IEEE study](https://arxiv.org/pdf/2109.00084.pdf) from 2022. The demo is [here](https://huggingface.co/spaces/codys12/MergeLlama-7b) if anyone wants to check it out and give some feedback. It would help a ton for future versions improving the dataset and going forward with the 13b and 34b models,NEGATIVE,0.9793450236320496,3,1,https://www.reddit.com/r/MachineLearning/comments/17ap687/p_i_made_a_finetune_of_codellama_to_resolve_merge/
[D] Comparing LLaMA and Alpaca,[D] Comparing LLaMA and Alpaca Are there any examples comparing the output of LLaMA and Alpaca starting from the same prompt? I would be interested in understanding how much the model output has changed after a relatively light fine-tuning.,NEGATIVE,0.9930528998374939,39,5,https://www.reddit.com/r/MachineLearning/comments/12fzpuy/d_comparing_llama_and_alpaca/
[D] LLama model 65B - pay per prompt,"[D] LLama model 65B - pay per prompt Hi,

Is there any way to run llama (or any other) model in such a way, that you only pay per API request?

I wanted to test how the llama model would do in my specific usecase, but when I went to HF Interface Endpoints it says that I would have to pay over 3k USD per month (ofc I do not have that much money to spend on a side-project).

I would like to test this model by paying on per request basis.",NEGATIVE,0.999233603477478,1,10,https://www.reddit.com/r/MachineLearning/comments/11v1eu7/d_llama_model_65b_pay_per_prompt/
[R] Reasoning with Language Model is Planning with World Model - Shibo Hao et al UC San Diego - RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting!,"[R] Reasoning with Language Model is Planning with World Model - Shibo Hao et al UC San Diego - RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting! Paper: [https://arxiv.org/abs/2305.14992](https://arxiv.org/abs/2305.14992) 

Abstract:

>Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal *world model* to predict the world *state* (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, ***R*****‚Äì‚Äì*****easoning via*****‚Äì‚Äì*****P*****‚Äì‚Äì*****lanning*** **(RAP).** RAP repurposes the **LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space.** During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration *vs.* exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. **RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.**

https://preview.redd.it/jaoiil2mc12b1.jpg?width=747&format=pjpg&auto=webp&s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53

https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&format=pjpg&auto=webp&s=7389d75d0ff7d1d8787c7c5f9add4787b02b47be

https://preview.redd.it/ykpqvp2mc12b1.jpg?width=980&format=pjpg&auto=webp&s=834c39fb3e549418b8396725e86ddff3c6584077

https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&format=pjpg&auto=webp&s=9c5192d6c012bfe4390fa67b010580b8e4508daa

https://preview.redd.it/qd8pjo2mc12b1.jpg?width=1400&format=pjpg&auto=webp&s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea",POSITIVE,0.9960634112358093,29,4,https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/
"Llama 2: open source, free for research and commercial use","Llama 2: open source, free for research and commercial use",POSITIVE,0.986764669418335,6,4,https://ai.meta.com/resources/models-and-libraries/llama/
A[R]xiv [D]ives - Llama 2 Deep Dive,"A[R]xiv [D]ives - Llama 2 Deep Dive We‚Äôve been diving deep into foundational papers on Fridays as a group. It‚Äôs been helpful for us to get into the nitty gritty details of these papers, so hope you find it helpful too.

Would love to have anyone join the discussion next week!",POSITIVE,0.9304632544517517,5,0,https://blog.oxen.ai/arxiv-dives-how-llama-2-works/
[D] Does anyone know where the report of the open-source Llama trained on 1T tokens is?,"[D] Does anyone know where the report of the open-source Llama trained on 1T tokens is? Hi. I remember that there was a group that trained an open-source Llama on \~ 1T tokens, and they then released a report sharing the details of the training run--specifically, they had plans to change the dataset / the mixture of datasources.

I've been trying to find it with no luck, does anyone know where it might be?",NEGATIVE,0.9991641044616699,5,6,https://www.reddit.com/r/MachineLearning/comments/13p1esg/d_does_anyone_know_where_the_report_of_the/
[D]Why are special tokens not allowed in the prompt for llama-2?,"[D]Why are special tokens not allowed in the prompt for llama-2? I was going through the code for Llama-2 text generation on the official github where I stumbled across this code in the [generation.py](https://github.com/facebookresearch/llama/blob/main/llama/generation.py) file:

    B_INST, E_INST = ""[INST]"", ""[/INST]""
    SPECIAL_TAGS = [B_INST, E_INST, ""<<SYS>>"", ""<</SYS>>""]
    UNSAFE_ERROR = ""Error: special tags are not allowed as part of the prompt.""
    ...
    ...
    ...
    unsafe_requests = []
    unsafe_requests.append(any([tag in msg[""content""] for tag in SPECIAL_TAGS for msg in dialog]))
    ...
    ...
    ...
    return [
                {
                    ""generation"": {
                        ""role"": ""assistant"",
                        ""content"": self.tokenizer.decode(t) if not unsafe else UNSAFE_ERROR,
                    }
                }
                for t, unsafe in zip(generation_tokens, unsafe_requests)
            ]

Is there a reason why we can't have these tokens in the prompt? 

I am planning to bypass the role based dictionary entries for the prompt and instead building my own prompt generator that'll take the the system prompts and the user prompts and generate a single string to then send to the LLM. Depending on the the user's choice I want the LLM to generate concise or detailed answers(also impose a word limit in the prompt itself), so I am planning to have this as a dropdown a user can choose. based on the system option chosen(concise/detailed answer), I then want to call my prompt generator which will add the instruction tags around the ""system"" and ""user"" prompts to generate 1 string I can then pass to the LLM.

I wanted to know if there was any reason these tags aren't allowed to be in the prompt. Is it only to avoid ""confusion"" on the different roles and following a conventional way to pass the prompts? If not, and there's a reason those tags aren't supposed to be passed inside the prompts, please do let me know,, because inside the same file the [chat\_completion()](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L274) function is doing exactly that; adding the <<SYS>> and <</SYS>> around the system prompts and prepending it to the user prompt.",NEGATIVE,0.9995643496513367,2,1,https://www.reddit.com/r/MachineLearning/comments/167amdt/dwhy_are_special_tokens_not_allowed_in_the_prompt/
"[P] Llama 2, CodeLlama, and GPT-4 performance: A write-up on the LLM developments and research.","[P] Llama 2, CodeLlama, and GPT-4 performance: A write-up on the LLM developments and research.",NEGATIVE,0.9934700727462769,27,0,https://magazine.sebastianraschka.com/p/ahead-of-ai-11-new-foundation-models
LLaMA drama as Meta's mega language model files leak,LLaMA drama as Meta's mega language model files leak,NEGATIVE,0.9997630715370178,37,5,https://www.theregister.com/2023/03/08/meta_llama_ai_leak/
[P] An implementation of LLaMA based on nanoGPT,[P] An implementation of LLaMA based on nanoGPT,NEGATIVE,0.9809179306030273,30,5,https://github.com/Lightning-AI/lit-llama
[P] Run LLaMA LLM chatbots on any cloud with one click,"[P] Run LLaMA LLM chatbots on any cloud with one click We made a \*basic\* chatbot based on LLaMA models; code here: [https://github.com/skypilot-org/skypilot/tree/master/examples/llama-llm-chatbots](https://github.com/skypilot-org/skypilot/tree/master/examples/llama-llm-chatbots) [https://github.com/skypilot-org/sky-llama](https://github.com/skypilot-org/sky-llama)

A detailed post on how to run it on the cloud (Lambda Cloud, AWS, GCP, Azure) with 1 command: [https://blog.skypilot.co/llama-llm-chatbots-on-any-cloud/](https://blog.skypilot.co/llama-llm-chatbots-on-any-cloud/)

Would love to hear your thoughts. Although people are making LLMs run on laptops and other devices ({llama,alpaca}.cpp}, we think that as more open and compute-hungry LLMs emerge, it's increasingly important to finetune them and that's where getting powerful cloud compute in flexible locations comes into play.",NEGATIVE,0.9868103265762329,30,5,https://www.reddit.com/r/MachineLearning/comments/11xvo1i/p_run_llama_llm_chatbots_on_any_cloud_with_one/
[P] Deploy and Run LLMs at the Edge: Use Code Llama to Generate a Dashboard in a Network Restricted Environment,"[P] Deploy and Run LLMs at the Edge: Use Code Llama to Generate a Dashboard in a Network Restricted Environment In this blog, we explore different definitions of ‚Äúthe edge,‚Äù and understand the factors driving AI/ML to the edge. We examine why the trends of LLMs and edge computing are intersecting now, and how teams can take advantage of their combined power today. We also demonstrate how LLMs can be used in an edge environment to generate insights for a real-world use case today. Consider a geologist working in a remote oil field who is responsible for building and analyzing 3D models of oil fields to determine production capacity and the impact on profitability. In this demo, we walk through how Code Llama, [Chassisml.io](https://chassisml.io/v1.5/), and Modzy could be used to build a dashboard that geologists could use to analyze well data in real-time in a remote, network restricted environment, allowing for LLM insights generated at the edge.

Learn more: [https://www.modzy.com/modzy-blog/deploy-and-run-llms-at-the-edge](https://www.modzy.com/modzy-blog/deploy-and-run-llms-at-the-edge)",POSITIVE,0.9850516319274902,1,0,https://www.reddit.com/r/MachineLearning/comments/17788eq/p_deploy_and_run_llms_at_the_edge_use_code_llama/
Deepmind proposes a new methodology to extend LLama's context window,Deepmind proposes a new methodology to extend LLama's context window,POSITIVE,0.9639469981193542,10,3,https://arxiv.org/pdf/2307.03170.pdf
[N] Fine-Tuning LLMs: LoRA or Full-Parameter? An in-depth Analysis with Llama 2,"[N] Fine-Tuning LLMs: LoRA or Full-Parameter? An in-depth Analysis with Llama 2 After our first blog post gained some attention from folks interested in applied fine-tuning, we now have a follow-up post that discusses all sorts of things we learned while working with LoRA.   
We hope that this helps engineers and other folks in the community to improve their fine-tuning.

Here's what you can expect from the post:  
*We compare full-parameter fine-tuning with LoRA and answer questions around the strengths and weaknesses of the two techniques. We train the Llama 2 models on three real-world use cases and demonstrate that using LoRA involves a trade-off between serving efficiency and model quality, which varies according to the specific task at hand. Additionally, we offer insights into how to stabilize training with LoRA through intelligent prompting techniques. We further show that adopting a lower learning rate can enhance the reliability of the resulting model checkpoints.*  


[Link to the blog post](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2)  


If you have questions, I'd be happy to answer them here!",POSITIVE,0.9900145530700684,12,0,https://www.reddit.com/r/MachineLearning/comments/16bsvst/n_finetuning_llms_lora_or_fullparameter_an/
"[R] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models - OpenGVLab, Shanghai AI Laboratory 2023 - Provides an pre-trained Omniquant model zoo for multiple model families, including LLaMa-1&2, LLaMa-2-Chat, OPT!","[R] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models - OpenGVLab, Shanghai AI Laboratory 2023 - Provides an pre-trained Omniquant model zoo for multiple model families, including LLaMa-1&2, LLaMa-2-Chat, OPT! Paper: [https://arxiv.org/abs/2308.13137](https://arxiv.org/abs/2308.13137)

Github: [https://github.com/OpenGVLab/OmniQuant](https://github.com/OpenGVLab/OmniQuant)

HuggingFace Model direct download: [https://huggingface.co/ChenMnZ/OmniQuant/tree/main](https://huggingface.co/ChenMnZ/OmniQuant/tree/main)

Abstract:

>Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights through a learnable equivalent transformation. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the **LLaMA-2 model family with the size of 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours** using 128 samples. Extensive experiments validate OmniQuant's **superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16**. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices.  

https://preview.redd.it/veo2o475ywkb1.jpg?width=865&format=pjpg&auto=webp&s=570ff75302d6d38c19719f10d5bc8462065be9f4

https://preview.redd.it/d5haz475ywkb1.jpg?width=1373&format=pjpg&auto=webp&s=d98c1b4e2b670a78c02438d5841d81f4b86258a8

https://preview.redd.it/h3obqa85ywkb1.jpg?width=980&format=pjpg&auto=webp&s=d9d1afdc6f0fc0b77ea2c0fa6dd21cbe7e0cf2b0

https://preview.redd.it/lrgay375ywkb1.jpg?width=1212&format=pjpg&auto=webp&s=755d45a4dcdfcf55698c4f5a406001645bd09c0e

https://preview.redd.it/l41h2585ywkb1.jpg?width=1509&format=pjpg&auto=webp&s=216b9f5d9970e6e141927bcd75cb221360cdb493

&#x200B;",NEGATIVE,0.7682165503501892,6,1,https://www.reddit.com/r/MachineLearning/comments/163xxvw/r_omniquant_omnidirectionally_calibrated/
"[P] LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Multi-Query Attention","[P] LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Multi-Query Attention",POSITIVE,0.5529180765151978,17,0,https://www.youtube.com/watch?v=Mn_9W1nCFLo
"9 New Gemini Leaks, Code Llama and A Major AI Consciousness Paper","9 New Gemini Leaks, Code Llama and A Major AI Consciousness Paper",NEGATIVE,0.9992803931236267,7,1,https://youtu.be/ad5SZeANuJE?si=dqKGGgaUti-lBeVG
[Discussion] Meta open sources llama-2 and tie up with MSFT,"[Discussion] Meta open sources llama-2 and tie up with MSFT [https://about.fb.com/news/2023/07/llama-2/](https://about.fb.com/news/2023/07/llama-2/)

[https://ai.meta.com/llama/](https://ai.meta.com/llama/)",NEGATIVE,0.9839745163917542,10,2,https://www.reddit.com/r/MachineLearning/comments/1534kd8/discussion_meta_open_sources_llama2_and_tie_up/
"Comparing Vicuna to alternative LLMs like ChatGPT, LLaMA, and Alpaca","Comparing Vicuna to alternative LLMs like ChatGPT, LLaMA, and Alpaca I wrote an in-depth article exploring Vicuna as an alternative to competitor LLMs like ChatGPT, Alpaca, and LLaMA for chat applications. I based it off the research data on the [LMSYS.org](https://LMSYS.org) website and the Github repo for the project.

**Key findings:**

* Vicuna achieves over 90% of ChatGPT's conversational quality based on benchmarks, despite being smaller in size.
* It significantly outperforms other open models like LLaMA and Alpaca.
* Vicuna is freely available for non-commercial use under a research license.
* For startups and developers, Vicuna provides an decent open-source alternative to proprietary conversational AI.
* It shows the potential of transfer learning from foundation models like LLaMA.

Overall, Vicuna represents a promising development in **democratizing access** to leading conversational intelligence through its high performance, permissive licensing, and open availability.

You can [read the full article here.](https://notes.aimodels.fyi/vicuna-ai-llama-alpaca-chatgpt-alternative/) I also publish all these articles in a [weekly email](https://aimodels.substack.com/) if you prefer to get them that way.",NEGATIVE,0.7372312545776367,3,2,https://www.reddit.com/r/artificial/comments/15i6fbf/comparing_vicuna_to_alternative_llms_like_chatgpt/
[P] How to fine tune 8k context length Llama 13B on minimal number of gpus?,"[P] How to fine tune 8k context length Llama 13B on minimal number of gpus? 
I have a llama 13B model I want to fine tune. I am using qlora (brings down to 7gb of gpu memory) and using ntk to bring up context length to 8k (dataset requires at least this much context length).

But on 1024 context length, fine tuning spikes to 42gb of gpu vram used, so evidently it won‚Äôt be feasible to use 8k context length unless I use a ton of gpus. Is there anyway to lower memory so that one or two 3090s are enough for 8k context length fine tuning?",NEGATIVE,0.9991540908813477,8,2,https://www.reddit.com/r/MachineLearning/comments/1550job/p_how_to_fine_tune_8k_context_length_llama_13b_on/
"Leveraging LLaMa, or other LLM embeddings for semantic search [D]","Leveraging LLaMa, or other LLM embeddings for semantic search [D] Hi!

I would love to be able to figure out if embeddings produced by the popular LLM are valuable for tasks such as semantic search?

There are many great libraries like sentence transformers which produce good embeddings due to STS fine tuning, but I would like a joint model to have both generative capabilities and to be able to retrieve great embeddings for search applications - does anyone have any ideas on how to get started on this?",POSITIVE,0.883474588394165,0,5,https://www.reddit.com/r/MachineLearning/comments/13n4y0s/leveraging_llama_or_other_llm_embeddings_for/
Alpaca.cpp | Llama + Alpaca-13b + 64 COARS | ./Release/chat -t 120 -m ggml-alpaca-13b-q4,"Alpaca.cpp | Llama + Alpaca-13b + 64 COARS | ./Release/chat -t 120 -m ggml-alpaca-13b-q4  [Llama + Alpaca-13b + 64 COARS | ./Release/chat -t 120 -m ggml-alpaca-13b-q4 - YouTube](https://www.youtube.com/watch?v=NWROXWA3tms) 

Alpaca.cpp demo [https://github.com/antimatter15/alpaca.cpp](https://github.com/antimatter15/alpaca.cpp)",NEGATIVE,0.99795001745224,26,4,https://www.reddit.com/r/artificial/comments/11x5qy5/alpacacpp_llama_alpaca13b_64_coars_releasechat_t/
[P] I ran Llama 2 on my Mac in < 5 mins,"[P] I ran Llama 2 on my Mac in < 5 mins So Llama 2 sounds awesome, but I really wanted to run it locally on my Macbook Pro instead of on a Linux box with an NVIDIA GPU. So I put the llama.cpp GGML models into the XetHub Llama 2 repo so I can use the power of Llama 2 locally. It now takes me 5 seconds to mount Llama 2 and it loads the GGML model almost instantly. Here‚Äôs how I did it:

1. **Create an account:** Go to xethub.com and Sign In with GitHub
2. **Quick start**:¬†Go to xethub.com/explore/quickstart and follow the Install & Setup steps (xethub.com/explore/install)
3. **pip install pyxet** for Python SDK and CLI
4. **Set up authentication:** Create a Personal Access Token and then run the login command from a Terminal so your \~/.xetconfig is set up with your login token.

Here‚Äôs the code to get¬†Llama 2 up and running on your Mac laptop in a few minutes:

    # 1. Clone and compile llama.cpp 
    cd ~/code
    git clone https://github.com/ggerganov/llama.cpp.git
    cd llama.cpp
    LLAMA_METAL=1 make
    
    # 2. Mount Llama 2 Xet repo
    cd ~/code
    xet mount xet://XetHub/Llama2/main Llama2
    
    # (should take ~4s to complete, 660GB repo)
    
    # 4. Run Llama2 locally on Mac
    ~/code/llama.cpp/main -ngl 1 \
      --model ~/code/Llama2/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_0.bin \
      --prompt ""Write a recipe for mayonnaise.""
    
    # Note: -ngl 1 enables using the Apple Silicon GPU, which GREATLY speeds up the LLM.

The first time I ran this, it took \~4 mins for the model to load, but it should then load within a second. So, the total time from start to finish is < 5 mins for running Llama 2 on your laptop.

Here's what the entire process looks like on my laptop. The model loaded in 0.6 secs (622.73 ms) on this invocation, and the recipe seems pretty decent.

    $ cd ~/code
    
    $ xet mount xet://XetHub/Llama2/main Llama2
    Mounting to ""/Users/rajat/code/Llama2""
    Cloning into temporary directory ""/var/folders/c_/wqqh5ytn6vs2wfzwhbxpckrr0000gn/T/.tmp135CxD""
    Mounting as a background task...
    Setting up mount point...
    Mount at ""/Users/rajat/code/Llama2"" successful. Unmount with 'umount ""/Users/rajat/code/Llama2""'
    Mount complete in 3.923592s
    
    $ ~/code/llama.cpp/main -ngl 1 \
        --model ~/code/Llama2/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_0.bin \
        --prompt ""Write me a recipe for mayonnaise.""
    
    main: build = 899 (41c6741)
    main: seed  = 1691186483
    llama.cpp: loading model from /Users/rajat/code/Llama2/models/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_0.bin
    
    ... 
    ... removed lots of spew from llama.cpp 
    ...
    
    system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |
    sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
    generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0
    
    
     Write me a recipe for mayonnaise. nobody elses, just yours.
    Submitted by: anon (2)
    Okay! Here's a simple recipe for homemade mayonnaise:
    Ingredients:
    
    * 1/2 cup neutral-tasting oil, such as canola or grapeseed (I prefer using olive oil for a more flavorful mayonnaise, but it can make the texture less stable)
    * 3 large egg yolks
    * 2 tablespoons white vinegar or apple cider vinegar (you can also use lemon juice or lime juice, but keep in mind that they will give a slightly different flavor profile)
    * Salt and pepper to taste (I recommend using a pinch of salt and a few grinds of black pepper)
    Instructions:
    1. In a medium-sized bowl, whisk together the egg yolks and oil until well combined. You may need to stop and start the whisk several times to get the hang of it. It's important to keep the egg yolks moving during this step, as they will begin to break down if they are left still for too long.
    2. Once you have a smooth mixture, slowly pour in the vinegar or lemon juice while continuously whisking. This is where things can get a bit tricky, as you want to add just enough vinegar to emulsify the mixture without making it too thick and clumpy. It's important to be patient here and not rush the process, as the mayonnaise will thicken up as it sits (and you don't want it to become too thick). Keep whisking until you have a smooth, creamy consistency.
    3. Taste and adjust the seasoning as needed. I find that mayonnaise can be quite versatile in terms of flavor, so feel free to experiment with different types of vinegar or added ingredients (such as minced garlic, chopped herbs, or grated ginger) to give it a unique twist.
    4. Cover the bowl with plastic wrap and chill in the fridge for at least 30 minutes before serving. This will allow the flavors to meld together and the mayonnaise to set properly. If you're feeling impatient, you can also try storing it in an airtight container in the fridge for up to a day or two.
    And that's it! With these simple steps, you should be able to make your own delicious homemade mayonnaise from scratch. Enjoy! [end of text]
    
    llama_print_timings:        load time =   622.73 ms
    llama_print_timings:      sample time =   451.28 ms /   570 runs   (    0.79 ms per token,  1263.08 tokens per second)
    llama_print_timings: prompt eval time =  6611.03 ms /   268 tokens (   24.67 ms per token,    40.54 tokens per second)
    llama_print_timings:        eval time = 12109.81 ms /   568 runs   (   21.32 ms per token,    46.90 tokens per second)
    llama_print_timings:       total time = 19232.02 ms
    ggml_metal_free: deallocating


Let me know how it works for you!",NEGATIVE,0.9973987340927124,4,1,https://www.reddit.com/r/MachineLearning/comments/15nkirl/p_i_ran_llama_2_on_my_mac_in_5_mins/
